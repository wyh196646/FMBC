{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ruiyan/yuhao/project/FMBC/help/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/ruiyan/yuhao/project/FMBC/help/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/ruiyan/yuhao/project/FMBC/help/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "from vision_transformer import DinoVisionTransformer  # 导入您的ViT模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('CONCH.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_scale\n",
      "text.text_projection\n",
      "text.cls_emb\n",
      "text.positional_embedding\n",
      "text.token_embedding.weight\n",
      "text.transformer.resblocks.0.ln_1.weight\n",
      "text.transformer.resblocks.0.ln_1.bias\n",
      "text.transformer.resblocks.0.attn.in_proj_weight\n",
      "text.transformer.resblocks.0.attn.in_proj_bias\n",
      "text.transformer.resblocks.0.attn.out_proj.weight\n",
      "text.transformer.resblocks.0.attn.out_proj.bias\n",
      "text.transformer.resblocks.0.ln_2.weight\n",
      "text.transformer.resblocks.0.ln_2.bias\n",
      "text.transformer.resblocks.0.mlp.c_fc.weight\n",
      "text.transformer.resblocks.0.mlp.c_fc.bias\n",
      "text.transformer.resblocks.0.mlp.c_proj.weight\n",
      "text.transformer.resblocks.0.mlp.c_proj.bias\n",
      "text.transformer.resblocks.1.ln_1.weight\n",
      "text.transformer.resblocks.1.ln_1.bias\n",
      "text.transformer.resblocks.1.attn.in_proj_weight\n",
      "text.transformer.resblocks.1.attn.in_proj_bias\n",
      "text.transformer.resblocks.1.attn.out_proj.weight\n",
      "text.transformer.resblocks.1.attn.out_proj.bias\n",
      "text.transformer.resblocks.1.ln_2.weight\n",
      "text.transformer.resblocks.1.ln_2.bias\n",
      "text.transformer.resblocks.1.mlp.c_fc.weight\n",
      "text.transformer.resblocks.1.mlp.c_fc.bias\n",
      "text.transformer.resblocks.1.mlp.c_proj.weight\n",
      "text.transformer.resblocks.1.mlp.c_proj.bias\n",
      "text.transformer.resblocks.2.ln_1.weight\n",
      "text.transformer.resblocks.2.ln_1.bias\n",
      "text.transformer.resblocks.2.attn.in_proj_weight\n",
      "text.transformer.resblocks.2.attn.in_proj_bias\n",
      "text.transformer.resblocks.2.attn.out_proj.weight\n",
      "text.transformer.resblocks.2.attn.out_proj.bias\n",
      "text.transformer.resblocks.2.ln_2.weight\n",
      "text.transformer.resblocks.2.ln_2.bias\n",
      "text.transformer.resblocks.2.mlp.c_fc.weight\n",
      "text.transformer.resblocks.2.mlp.c_fc.bias\n",
      "text.transformer.resblocks.2.mlp.c_proj.weight\n",
      "text.transformer.resblocks.2.mlp.c_proj.bias\n",
      "text.transformer.resblocks.3.ln_1.weight\n",
      "text.transformer.resblocks.3.ln_1.bias\n",
      "text.transformer.resblocks.3.attn.in_proj_weight\n",
      "text.transformer.resblocks.3.attn.in_proj_bias\n",
      "text.transformer.resblocks.3.attn.out_proj.weight\n",
      "text.transformer.resblocks.3.attn.out_proj.bias\n",
      "text.transformer.resblocks.3.ln_2.weight\n",
      "text.transformer.resblocks.3.ln_2.bias\n",
      "text.transformer.resblocks.3.mlp.c_fc.weight\n",
      "text.transformer.resblocks.3.mlp.c_fc.bias\n",
      "text.transformer.resblocks.3.mlp.c_proj.weight\n",
      "text.transformer.resblocks.3.mlp.c_proj.bias\n",
      "text.transformer.resblocks.4.ln_1.weight\n",
      "text.transformer.resblocks.4.ln_1.bias\n",
      "text.transformer.resblocks.4.attn.in_proj_weight\n",
      "text.transformer.resblocks.4.attn.in_proj_bias\n",
      "text.transformer.resblocks.4.attn.out_proj.weight\n",
      "text.transformer.resblocks.4.attn.out_proj.bias\n",
      "text.transformer.resblocks.4.ln_2.weight\n",
      "text.transformer.resblocks.4.ln_2.bias\n",
      "text.transformer.resblocks.4.mlp.c_fc.weight\n",
      "text.transformer.resblocks.4.mlp.c_fc.bias\n",
      "text.transformer.resblocks.4.mlp.c_proj.weight\n",
      "text.transformer.resblocks.4.mlp.c_proj.bias\n",
      "text.transformer.resblocks.5.ln_1.weight\n",
      "text.transformer.resblocks.5.ln_1.bias\n",
      "text.transformer.resblocks.5.attn.in_proj_weight\n",
      "text.transformer.resblocks.5.attn.in_proj_bias\n",
      "text.transformer.resblocks.5.attn.out_proj.weight\n",
      "text.transformer.resblocks.5.attn.out_proj.bias\n",
      "text.transformer.resblocks.5.ln_2.weight\n",
      "text.transformer.resblocks.5.ln_2.bias\n",
      "text.transformer.resblocks.5.mlp.c_fc.weight\n",
      "text.transformer.resblocks.5.mlp.c_fc.bias\n",
      "text.transformer.resblocks.5.mlp.c_proj.weight\n",
      "text.transformer.resblocks.5.mlp.c_proj.bias\n",
      "text.transformer.resblocks.6.ln_1.weight\n",
      "text.transformer.resblocks.6.ln_1.bias\n",
      "text.transformer.resblocks.6.attn.in_proj_weight\n",
      "text.transformer.resblocks.6.attn.in_proj_bias\n",
      "text.transformer.resblocks.6.attn.out_proj.weight\n",
      "text.transformer.resblocks.6.attn.out_proj.bias\n",
      "text.transformer.resblocks.6.ln_2.weight\n",
      "text.transformer.resblocks.6.ln_2.bias\n",
      "text.transformer.resblocks.6.mlp.c_fc.weight\n",
      "text.transformer.resblocks.6.mlp.c_fc.bias\n",
      "text.transformer.resblocks.6.mlp.c_proj.weight\n",
      "text.transformer.resblocks.6.mlp.c_proj.bias\n",
      "text.transformer.resblocks.7.ln_1.weight\n",
      "text.transformer.resblocks.7.ln_1.bias\n",
      "text.transformer.resblocks.7.attn.in_proj_weight\n",
      "text.transformer.resblocks.7.attn.in_proj_bias\n",
      "text.transformer.resblocks.7.attn.out_proj.weight\n",
      "text.transformer.resblocks.7.attn.out_proj.bias\n",
      "text.transformer.resblocks.7.ln_2.weight\n",
      "text.transformer.resblocks.7.ln_2.bias\n",
      "text.transformer.resblocks.7.mlp.c_fc.weight\n",
      "text.transformer.resblocks.7.mlp.c_fc.bias\n",
      "text.transformer.resblocks.7.mlp.c_proj.weight\n",
      "text.transformer.resblocks.7.mlp.c_proj.bias\n",
      "text.transformer.resblocks.8.ln_1.weight\n",
      "text.transformer.resblocks.8.ln_1.bias\n",
      "text.transformer.resblocks.8.attn.in_proj_weight\n",
      "text.transformer.resblocks.8.attn.in_proj_bias\n",
      "text.transformer.resblocks.8.attn.out_proj.weight\n",
      "text.transformer.resblocks.8.attn.out_proj.bias\n",
      "text.transformer.resblocks.8.ln_2.weight\n",
      "text.transformer.resblocks.8.ln_2.bias\n",
      "text.transformer.resblocks.8.mlp.c_fc.weight\n",
      "text.transformer.resblocks.8.mlp.c_fc.bias\n",
      "text.transformer.resblocks.8.mlp.c_proj.weight\n",
      "text.transformer.resblocks.8.mlp.c_proj.bias\n",
      "text.transformer.resblocks.9.ln_1.weight\n",
      "text.transformer.resblocks.9.ln_1.bias\n",
      "text.transformer.resblocks.9.attn.in_proj_weight\n",
      "text.transformer.resblocks.9.attn.in_proj_bias\n",
      "text.transformer.resblocks.9.attn.out_proj.weight\n",
      "text.transformer.resblocks.9.attn.out_proj.bias\n",
      "text.transformer.resblocks.9.ln_2.weight\n",
      "text.transformer.resblocks.9.ln_2.bias\n",
      "text.transformer.resblocks.9.mlp.c_fc.weight\n",
      "text.transformer.resblocks.9.mlp.c_fc.bias\n",
      "text.transformer.resblocks.9.mlp.c_proj.weight\n",
      "text.transformer.resblocks.9.mlp.c_proj.bias\n",
      "text.transformer.resblocks.10.ln_1.weight\n",
      "text.transformer.resblocks.10.ln_1.bias\n",
      "text.transformer.resblocks.10.attn.in_proj_weight\n",
      "text.transformer.resblocks.10.attn.in_proj_bias\n",
      "text.transformer.resblocks.10.attn.out_proj.weight\n",
      "text.transformer.resblocks.10.attn.out_proj.bias\n",
      "text.transformer.resblocks.10.ln_2.weight\n",
      "text.transformer.resblocks.10.ln_2.bias\n",
      "text.transformer.resblocks.10.mlp.c_fc.weight\n",
      "text.transformer.resblocks.10.mlp.c_fc.bias\n",
      "text.transformer.resblocks.10.mlp.c_proj.weight\n",
      "text.transformer.resblocks.10.mlp.c_proj.bias\n",
      "text.transformer.resblocks.11.ln_1.weight\n",
      "text.transformer.resblocks.11.ln_1.bias\n",
      "text.transformer.resblocks.11.attn.in_proj_weight\n",
      "text.transformer.resblocks.11.attn.in_proj_bias\n",
      "text.transformer.resblocks.11.attn.out_proj.weight\n",
      "text.transformer.resblocks.11.attn.out_proj.bias\n",
      "text.transformer.resblocks.11.ln_2.weight\n",
      "text.transformer.resblocks.11.ln_2.bias\n",
      "text.transformer.resblocks.11.mlp.c_fc.weight\n",
      "text.transformer.resblocks.11.mlp.c_fc.bias\n",
      "text.transformer.resblocks.11.mlp.c_proj.weight\n",
      "text.transformer.resblocks.11.mlp.c_proj.bias\n",
      "text.ln_final.weight\n",
      "text.ln_final.bias\n",
      "visual.proj_contrast\n",
      "visual.trunk.cls_token\n",
      "visual.trunk.pos_embed\n",
      "visual.trunk.patch_embed.proj.weight\n",
      "visual.trunk.patch_embed.proj.bias\n",
      "visual.trunk.blocks.0.norm1.weight\n",
      "visual.trunk.blocks.0.norm1.bias\n",
      "visual.trunk.blocks.0.attn.qkv.weight\n",
      "visual.trunk.blocks.0.attn.qkv.bias\n",
      "visual.trunk.blocks.0.attn.proj.weight\n",
      "visual.trunk.blocks.0.attn.proj.bias\n",
      "visual.trunk.blocks.0.norm2.weight\n",
      "visual.trunk.blocks.0.norm2.bias\n",
      "visual.trunk.blocks.0.mlp.fc1.weight\n",
      "visual.trunk.blocks.0.mlp.fc1.bias\n",
      "visual.trunk.blocks.0.mlp.fc2.weight\n",
      "visual.trunk.blocks.0.mlp.fc2.bias\n",
      "visual.trunk.blocks.1.norm1.weight\n",
      "visual.trunk.blocks.1.norm1.bias\n",
      "visual.trunk.blocks.1.attn.qkv.weight\n",
      "visual.trunk.blocks.1.attn.qkv.bias\n",
      "visual.trunk.blocks.1.attn.proj.weight\n",
      "visual.trunk.blocks.1.attn.proj.bias\n",
      "visual.trunk.blocks.1.norm2.weight\n",
      "visual.trunk.blocks.1.norm2.bias\n",
      "visual.trunk.blocks.1.mlp.fc1.weight\n",
      "visual.trunk.blocks.1.mlp.fc1.bias\n",
      "visual.trunk.blocks.1.mlp.fc2.weight\n",
      "visual.trunk.blocks.1.mlp.fc2.bias\n",
      "visual.trunk.blocks.2.norm1.weight\n",
      "visual.trunk.blocks.2.norm1.bias\n",
      "visual.trunk.blocks.2.attn.qkv.weight\n",
      "visual.trunk.blocks.2.attn.qkv.bias\n",
      "visual.trunk.blocks.2.attn.proj.weight\n",
      "visual.trunk.blocks.2.attn.proj.bias\n",
      "visual.trunk.blocks.2.norm2.weight\n",
      "visual.trunk.blocks.2.norm2.bias\n",
      "visual.trunk.blocks.2.mlp.fc1.weight\n",
      "visual.trunk.blocks.2.mlp.fc1.bias\n",
      "visual.trunk.blocks.2.mlp.fc2.weight\n",
      "visual.trunk.blocks.2.mlp.fc2.bias\n",
      "visual.trunk.blocks.3.norm1.weight\n",
      "visual.trunk.blocks.3.norm1.bias\n",
      "visual.trunk.blocks.3.attn.qkv.weight\n",
      "visual.trunk.blocks.3.attn.qkv.bias\n",
      "visual.trunk.blocks.3.attn.proj.weight\n",
      "visual.trunk.blocks.3.attn.proj.bias\n",
      "visual.trunk.blocks.3.norm2.weight\n",
      "visual.trunk.blocks.3.norm2.bias\n",
      "visual.trunk.blocks.3.mlp.fc1.weight\n",
      "visual.trunk.blocks.3.mlp.fc1.bias\n",
      "visual.trunk.blocks.3.mlp.fc2.weight\n",
      "visual.trunk.blocks.3.mlp.fc2.bias\n",
      "visual.trunk.blocks.4.norm1.weight\n",
      "visual.trunk.blocks.4.norm1.bias\n",
      "visual.trunk.blocks.4.attn.qkv.weight\n",
      "visual.trunk.blocks.4.attn.qkv.bias\n",
      "visual.trunk.blocks.4.attn.proj.weight\n",
      "visual.trunk.blocks.4.attn.proj.bias\n",
      "visual.trunk.blocks.4.norm2.weight\n",
      "visual.trunk.blocks.4.norm2.bias\n",
      "visual.trunk.blocks.4.mlp.fc1.weight\n",
      "visual.trunk.blocks.4.mlp.fc1.bias\n",
      "visual.trunk.blocks.4.mlp.fc2.weight\n",
      "visual.trunk.blocks.4.mlp.fc2.bias\n",
      "visual.trunk.blocks.5.norm1.weight\n",
      "visual.trunk.blocks.5.norm1.bias\n",
      "visual.trunk.blocks.5.attn.qkv.weight\n",
      "visual.trunk.blocks.5.attn.qkv.bias\n",
      "visual.trunk.blocks.5.attn.proj.weight\n",
      "visual.trunk.blocks.5.attn.proj.bias\n",
      "visual.trunk.blocks.5.norm2.weight\n",
      "visual.trunk.blocks.5.norm2.bias\n",
      "visual.trunk.blocks.5.mlp.fc1.weight\n",
      "visual.trunk.blocks.5.mlp.fc1.bias\n",
      "visual.trunk.blocks.5.mlp.fc2.weight\n",
      "visual.trunk.blocks.5.mlp.fc2.bias\n",
      "visual.trunk.blocks.6.norm1.weight\n",
      "visual.trunk.blocks.6.norm1.bias\n",
      "visual.trunk.blocks.6.attn.qkv.weight\n",
      "visual.trunk.blocks.6.attn.qkv.bias\n",
      "visual.trunk.blocks.6.attn.proj.weight\n",
      "visual.trunk.blocks.6.attn.proj.bias\n",
      "visual.trunk.blocks.6.norm2.weight\n",
      "visual.trunk.blocks.6.norm2.bias\n",
      "visual.trunk.blocks.6.mlp.fc1.weight\n",
      "visual.trunk.blocks.6.mlp.fc1.bias\n",
      "visual.trunk.blocks.6.mlp.fc2.weight\n",
      "visual.trunk.blocks.6.mlp.fc2.bias\n",
      "visual.trunk.blocks.7.norm1.weight\n",
      "visual.trunk.blocks.7.norm1.bias\n",
      "visual.trunk.blocks.7.attn.qkv.weight\n",
      "visual.trunk.blocks.7.attn.qkv.bias\n",
      "visual.trunk.blocks.7.attn.proj.weight\n",
      "visual.trunk.blocks.7.attn.proj.bias\n",
      "visual.trunk.blocks.7.norm2.weight\n",
      "visual.trunk.blocks.7.norm2.bias\n",
      "visual.trunk.blocks.7.mlp.fc1.weight\n",
      "visual.trunk.blocks.7.mlp.fc1.bias\n",
      "visual.trunk.blocks.7.mlp.fc2.weight\n",
      "visual.trunk.blocks.7.mlp.fc2.bias\n",
      "visual.trunk.blocks.8.norm1.weight\n",
      "visual.trunk.blocks.8.norm1.bias\n",
      "visual.trunk.blocks.8.attn.qkv.weight\n",
      "visual.trunk.blocks.8.attn.qkv.bias\n",
      "visual.trunk.blocks.8.attn.proj.weight\n",
      "visual.trunk.blocks.8.attn.proj.bias\n",
      "visual.trunk.blocks.8.norm2.weight\n",
      "visual.trunk.blocks.8.norm2.bias\n",
      "visual.trunk.blocks.8.mlp.fc1.weight\n",
      "visual.trunk.blocks.8.mlp.fc1.bias\n",
      "visual.trunk.blocks.8.mlp.fc2.weight\n",
      "visual.trunk.blocks.8.mlp.fc2.bias\n",
      "visual.trunk.blocks.9.norm1.weight\n",
      "visual.trunk.blocks.9.norm1.bias\n",
      "visual.trunk.blocks.9.attn.qkv.weight\n",
      "visual.trunk.blocks.9.attn.qkv.bias\n",
      "visual.trunk.blocks.9.attn.proj.weight\n",
      "visual.trunk.blocks.9.attn.proj.bias\n",
      "visual.trunk.blocks.9.norm2.weight\n",
      "visual.trunk.blocks.9.norm2.bias\n",
      "visual.trunk.blocks.9.mlp.fc1.weight\n",
      "visual.trunk.blocks.9.mlp.fc1.bias\n",
      "visual.trunk.blocks.9.mlp.fc2.weight\n",
      "visual.trunk.blocks.9.mlp.fc2.bias\n",
      "visual.trunk.blocks.10.norm1.weight\n",
      "visual.trunk.blocks.10.norm1.bias\n",
      "visual.trunk.blocks.10.attn.qkv.weight\n",
      "visual.trunk.blocks.10.attn.qkv.bias\n",
      "visual.trunk.blocks.10.attn.proj.weight\n",
      "visual.trunk.blocks.10.attn.proj.bias\n",
      "visual.trunk.blocks.10.norm2.weight\n",
      "visual.trunk.blocks.10.norm2.bias\n",
      "visual.trunk.blocks.10.mlp.fc1.weight\n",
      "visual.trunk.blocks.10.mlp.fc1.bias\n",
      "visual.trunk.blocks.10.mlp.fc2.weight\n",
      "visual.trunk.blocks.10.mlp.fc2.bias\n",
      "visual.trunk.blocks.11.norm1.weight\n",
      "visual.trunk.blocks.11.norm1.bias\n",
      "visual.trunk.blocks.11.attn.qkv.weight\n",
      "visual.trunk.blocks.11.attn.qkv.bias\n",
      "visual.trunk.blocks.11.attn.proj.weight\n",
      "visual.trunk.blocks.11.attn.proj.bias\n",
      "visual.trunk.blocks.11.norm2.weight\n",
      "visual.trunk.blocks.11.norm2.bias\n",
      "visual.trunk.blocks.11.mlp.fc1.weight\n",
      "visual.trunk.blocks.11.mlp.fc1.bias\n",
      "visual.trunk.blocks.11.mlp.fc2.weight\n",
      "visual.trunk.blocks.11.mlp.fc2.bias\n",
      "visual.trunk.norm.weight\n",
      "visual.trunk.norm.bias\n",
      "visual.attn_pool_contrast.query\n",
      "visual.attn_pool_contrast.attn.q_proj_weight\n",
      "visual.attn_pool_contrast.attn.k_proj_weight\n",
      "visual.attn_pool_contrast.attn.v_proj_weight\n",
      "visual.attn_pool_contrast.attn.in_proj_bias\n",
      "visual.attn_pool_contrast.attn.out_proj.weight\n",
      "visual.attn_pool_contrast.attn.out_proj.bias\n",
      "visual.attn_pool_contrast.ln_q.weight\n",
      "visual.attn_pool_contrast.ln_q.bias\n",
      "visual.attn_pool_contrast.ln_k.weight\n",
      "visual.attn_pool_contrast.ln_k.bias\n",
      "visual.ln_contrast.weight\n",
      "visual.ln_contrast.bias\n",
      "visual.attn_pool_caption.query\n",
      "visual.attn_pool_caption.attn.in_proj_weight\n",
      "visual.attn_pool_caption.attn.in_proj_bias\n",
      "visual.attn_pool_caption.attn.out_proj.weight\n",
      "visual.attn_pool_caption.attn.out_proj.bias\n",
      "visual.attn_pool_caption.ln_q.weight\n",
      "visual.attn_pool_caption.ln_q.bias\n",
      "visual.attn_pool_caption.ln_k.weight\n",
      "visual.attn_pool_caption.ln_k.bias\n",
      "visual.ln_caption.weight\n",
      "visual.ln_caption.bias\n"
     ]
    }
   ],
   "source": [
    "for key in list(state_dict.keys()):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DinoVisionTransformer(embed_dim=1024, depth=24, num_heads=16, patch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the state_dict from the saved file\n",
    "state_dict = torch.load('UNI.bin')\n",
    "\n",
    "# Create a copy of the state_dict with updated key names\n",
    "updated_state_dict = {}\n",
    "\n",
    "for key in state_dict.keys():\n",
    "    if 'blocks' in key:\n",
    "        # Rename keys from 'blocks' to 'blocks.0'\n",
    "        new_key = key.replace('blocks', 'blocks.0')\n",
    "        if new_key in model.state_dict().keys():\n",
    "            updated_state_dict[new_key] = state_dict[key]\n",
    "    else:\n",
    "        if key in model.state_dict().keys():\n",
    "            updated_state_dict[key] = state_dict[key]\n",
    "        else:\n",
    "            print(f\"Key {key} not found in the model's state_dict\")\n",
    "\n",
    "\n",
    "# Save the updated state_dict back to a file\n",
    "torch.save(updated_state_dict, 'UNI_updated.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['mask_token'], unexpected_keys=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict= torch.load('UNI_updated.bin')\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded layers:\n",
      "Layer: cls_token\n",
      "Layer: pos_embed\n",
      "Layer: patch_embed.proj.weight\n",
      "Layer: patch_embed.proj.bias\n",
      "Layer: blocks.0.0.norm1.weight\n",
      "Layer: blocks.0.0.norm1.bias\n",
      "Layer: blocks.0.0.attn.qkv.weight\n",
      "Layer: blocks.0.0.attn.qkv.bias\n",
      "Layer: blocks.0.0.attn.proj.weight\n",
      "Layer: blocks.0.0.attn.proj.bias\n",
      "Layer: blocks.0.0.norm2.weight\n",
      "Layer: blocks.0.0.norm2.bias\n",
      "Layer: blocks.0.0.mlp.fc1.weight\n",
      "Layer: blocks.0.0.mlp.fc1.bias\n",
      "Layer: blocks.0.0.mlp.fc2.weight\n",
      "Layer: blocks.0.0.mlp.fc2.bias\n",
      "Layer: blocks.0.1.norm1.weight\n",
      "Layer: blocks.0.1.norm1.bias\n",
      "Layer: blocks.0.1.attn.qkv.weight\n",
      "Layer: blocks.0.1.attn.qkv.bias\n",
      "Layer: blocks.0.1.attn.proj.weight\n",
      "Layer: blocks.0.1.attn.proj.bias\n",
      "Layer: blocks.0.1.norm2.weight\n",
      "Layer: blocks.0.1.norm2.bias\n",
      "Layer: blocks.0.1.mlp.fc1.weight\n",
      "Layer: blocks.0.1.mlp.fc1.bias\n",
      "Layer: blocks.0.1.mlp.fc2.weight\n",
      "Layer: blocks.0.1.mlp.fc2.bias\n",
      "Layer: blocks.0.2.norm1.weight\n",
      "Layer: blocks.0.2.norm1.bias\n",
      "Layer: blocks.0.2.attn.qkv.weight\n",
      "Layer: blocks.0.2.attn.qkv.bias\n",
      "Layer: blocks.0.2.attn.proj.weight\n",
      "Layer: blocks.0.2.attn.proj.bias\n",
      "Layer: blocks.0.2.norm2.weight\n",
      "Layer: blocks.0.2.norm2.bias\n",
      "Layer: blocks.0.2.mlp.fc1.weight\n",
      "Layer: blocks.0.2.mlp.fc1.bias\n",
      "Layer: blocks.0.2.mlp.fc2.weight\n",
      "Layer: blocks.0.2.mlp.fc2.bias\n",
      "Layer: blocks.0.3.norm1.weight\n",
      "Layer: blocks.0.3.norm1.bias\n",
      "Layer: blocks.0.3.attn.qkv.weight\n",
      "Layer: blocks.0.3.attn.qkv.bias\n",
      "Layer: blocks.0.3.attn.proj.weight\n",
      "Layer: blocks.0.3.attn.proj.bias\n",
      "Layer: blocks.0.3.norm2.weight\n",
      "Layer: blocks.0.3.norm2.bias\n",
      "Layer: blocks.0.3.mlp.fc1.weight\n",
      "Layer: blocks.0.3.mlp.fc1.bias\n",
      "Layer: blocks.0.3.mlp.fc2.weight\n",
      "Layer: blocks.0.3.mlp.fc2.bias\n",
      "Layer: blocks.0.4.norm1.weight\n",
      "Layer: blocks.0.4.norm1.bias\n",
      "Layer: blocks.0.4.attn.qkv.weight\n",
      "Layer: blocks.0.4.attn.qkv.bias\n",
      "Layer: blocks.0.4.attn.proj.weight\n",
      "Layer: blocks.0.4.attn.proj.bias\n",
      "Layer: blocks.0.4.norm2.weight\n",
      "Layer: blocks.0.4.norm2.bias\n",
      "Layer: blocks.0.4.mlp.fc1.weight\n",
      "Layer: blocks.0.4.mlp.fc1.bias\n",
      "Layer: blocks.0.4.mlp.fc2.weight\n",
      "Layer: blocks.0.4.mlp.fc2.bias\n",
      "Layer: blocks.0.5.norm1.weight\n",
      "Layer: blocks.0.5.norm1.bias\n",
      "Layer: blocks.0.5.attn.qkv.weight\n",
      "Layer: blocks.0.5.attn.qkv.bias\n",
      "Layer: blocks.0.5.attn.proj.weight\n",
      "Layer: blocks.0.5.attn.proj.bias\n",
      "Layer: blocks.0.5.norm2.weight\n",
      "Layer: blocks.0.5.norm2.bias\n",
      "Layer: blocks.0.5.mlp.fc1.weight\n",
      "Layer: blocks.0.5.mlp.fc1.bias\n",
      "Layer: blocks.0.5.mlp.fc2.weight\n",
      "Layer: blocks.0.5.mlp.fc2.bias\n",
      "Layer: blocks.0.6.norm1.weight\n",
      "Layer: blocks.0.6.norm1.bias\n",
      "Layer: blocks.0.6.attn.qkv.weight\n",
      "Layer: blocks.0.6.attn.qkv.bias\n",
      "Layer: blocks.0.6.attn.proj.weight\n",
      "Layer: blocks.0.6.attn.proj.bias\n",
      "Layer: blocks.0.6.norm2.weight\n",
      "Layer: blocks.0.6.norm2.bias\n",
      "Layer: blocks.0.6.mlp.fc1.weight\n",
      "Layer: blocks.0.6.mlp.fc1.bias\n",
      "Layer: blocks.0.6.mlp.fc2.weight\n",
      "Layer: blocks.0.6.mlp.fc2.bias\n",
      "Layer: blocks.0.7.norm1.weight\n",
      "Layer: blocks.0.7.norm1.bias\n",
      "Layer: blocks.0.7.attn.qkv.weight\n",
      "Layer: blocks.0.7.attn.qkv.bias\n",
      "Layer: blocks.0.7.attn.proj.weight\n",
      "Layer: blocks.0.7.attn.proj.bias\n",
      "Layer: blocks.0.7.norm2.weight\n",
      "Layer: blocks.0.7.norm2.bias\n",
      "Layer: blocks.0.7.mlp.fc1.weight\n",
      "Layer: blocks.0.7.mlp.fc1.bias\n",
      "Layer: blocks.0.7.mlp.fc2.weight\n",
      "Layer: blocks.0.7.mlp.fc2.bias\n",
      "Layer: blocks.0.8.norm1.weight\n",
      "Layer: blocks.0.8.norm1.bias\n",
      "Layer: blocks.0.8.attn.qkv.weight\n",
      "Layer: blocks.0.8.attn.qkv.bias\n",
      "Layer: blocks.0.8.attn.proj.weight\n",
      "Layer: blocks.0.8.attn.proj.bias\n",
      "Layer: blocks.0.8.norm2.weight\n",
      "Layer: blocks.0.8.norm2.bias\n",
      "Layer: blocks.0.8.mlp.fc1.weight\n",
      "Layer: blocks.0.8.mlp.fc1.bias\n",
      "Layer: blocks.0.8.mlp.fc2.weight\n",
      "Layer: blocks.0.8.mlp.fc2.bias\n",
      "Layer: blocks.0.9.norm1.weight\n",
      "Layer: blocks.0.9.norm1.bias\n",
      "Layer: blocks.0.9.attn.qkv.weight\n",
      "Layer: blocks.0.9.attn.qkv.bias\n",
      "Layer: blocks.0.9.attn.proj.weight\n",
      "Layer: blocks.0.9.attn.proj.bias\n",
      "Layer: blocks.0.9.norm2.weight\n",
      "Layer: blocks.0.9.norm2.bias\n",
      "Layer: blocks.0.9.mlp.fc1.weight\n",
      "Layer: blocks.0.9.mlp.fc1.bias\n",
      "Layer: blocks.0.9.mlp.fc2.weight\n",
      "Layer: blocks.0.9.mlp.fc2.bias\n",
      "Layer: blocks.0.10.norm1.weight\n",
      "Layer: blocks.0.10.norm1.bias\n",
      "Layer: blocks.0.10.attn.qkv.weight\n",
      "Layer: blocks.0.10.attn.qkv.bias\n",
      "Layer: blocks.0.10.attn.proj.weight\n",
      "Layer: blocks.0.10.attn.proj.bias\n",
      "Layer: blocks.0.10.norm2.weight\n",
      "Layer: blocks.0.10.norm2.bias\n",
      "Layer: blocks.0.10.mlp.fc1.weight\n",
      "Layer: blocks.0.10.mlp.fc1.bias\n",
      "Layer: blocks.0.10.mlp.fc2.weight\n",
      "Layer: blocks.0.10.mlp.fc2.bias\n",
      "Layer: blocks.0.11.norm1.weight\n",
      "Layer: blocks.0.11.norm1.bias\n",
      "Layer: blocks.0.11.attn.qkv.weight\n",
      "Layer: blocks.0.11.attn.qkv.bias\n",
      "Layer: blocks.0.11.attn.proj.weight\n",
      "Layer: blocks.0.11.attn.proj.bias\n",
      "Layer: blocks.0.11.norm2.weight\n",
      "Layer: blocks.0.11.norm2.bias\n",
      "Layer: blocks.0.11.mlp.fc1.weight\n",
      "Layer: blocks.0.11.mlp.fc1.bias\n",
      "Layer: blocks.0.11.mlp.fc2.weight\n",
      "Layer: blocks.0.11.mlp.fc2.bias\n",
      "Layer: blocks.0.12.norm1.weight\n",
      "Layer: blocks.0.12.norm1.bias\n",
      "Layer: blocks.0.12.attn.qkv.weight\n",
      "Layer: blocks.0.12.attn.qkv.bias\n",
      "Layer: blocks.0.12.attn.proj.weight\n",
      "Layer: blocks.0.12.attn.proj.bias\n",
      "Layer: blocks.0.12.norm2.weight\n",
      "Layer: blocks.0.12.norm2.bias\n",
      "Layer: blocks.0.12.mlp.fc1.weight\n",
      "Layer: blocks.0.12.mlp.fc1.bias\n",
      "Layer: blocks.0.12.mlp.fc2.weight\n",
      "Layer: blocks.0.12.mlp.fc2.bias\n",
      "Layer: blocks.0.13.norm1.weight\n",
      "Layer: blocks.0.13.norm1.bias\n",
      "Layer: blocks.0.13.attn.qkv.weight\n",
      "Layer: blocks.0.13.attn.qkv.bias\n",
      "Layer: blocks.0.13.attn.proj.weight\n",
      "Layer: blocks.0.13.attn.proj.bias\n",
      "Layer: blocks.0.13.norm2.weight\n",
      "Layer: blocks.0.13.norm2.bias\n",
      "Layer: blocks.0.13.mlp.fc1.weight\n",
      "Layer: blocks.0.13.mlp.fc1.bias\n",
      "Layer: blocks.0.13.mlp.fc2.weight\n",
      "Layer: blocks.0.13.mlp.fc2.bias\n",
      "Layer: blocks.0.14.norm1.weight\n",
      "Layer: blocks.0.14.norm1.bias\n",
      "Layer: blocks.0.14.attn.qkv.weight\n",
      "Layer: blocks.0.14.attn.qkv.bias\n",
      "Layer: blocks.0.14.attn.proj.weight\n",
      "Layer: blocks.0.14.attn.proj.bias\n",
      "Layer: blocks.0.14.norm2.weight\n",
      "Layer: blocks.0.14.norm2.bias\n",
      "Layer: blocks.0.14.mlp.fc1.weight\n",
      "Layer: blocks.0.14.mlp.fc1.bias\n",
      "Layer: blocks.0.14.mlp.fc2.weight\n",
      "Layer: blocks.0.14.mlp.fc2.bias\n",
      "Layer: blocks.0.15.norm1.weight\n",
      "Layer: blocks.0.15.norm1.bias\n",
      "Layer: blocks.0.15.attn.qkv.weight\n",
      "Layer: blocks.0.15.attn.qkv.bias\n",
      "Layer: blocks.0.15.attn.proj.weight\n",
      "Layer: blocks.0.15.attn.proj.bias\n",
      "Layer: blocks.0.15.norm2.weight\n",
      "Layer: blocks.0.15.norm2.bias\n",
      "Layer: blocks.0.15.mlp.fc1.weight\n",
      "Layer: blocks.0.15.mlp.fc1.bias\n",
      "Layer: blocks.0.15.mlp.fc2.weight\n",
      "Layer: blocks.0.15.mlp.fc2.bias\n",
      "Layer: blocks.0.16.norm1.weight\n",
      "Layer: blocks.0.16.norm1.bias\n",
      "Layer: blocks.0.16.attn.qkv.weight\n",
      "Layer: blocks.0.16.attn.qkv.bias\n",
      "Layer: blocks.0.16.attn.proj.weight\n",
      "Layer: blocks.0.16.attn.proj.bias\n",
      "Layer: blocks.0.16.norm2.weight\n",
      "Layer: blocks.0.16.norm2.bias\n",
      "Layer: blocks.0.16.mlp.fc1.weight\n",
      "Layer: blocks.0.16.mlp.fc1.bias\n",
      "Layer: blocks.0.16.mlp.fc2.weight\n",
      "Layer: blocks.0.16.mlp.fc2.bias\n",
      "Layer: blocks.0.17.norm1.weight\n",
      "Layer: blocks.0.17.norm1.bias\n",
      "Layer: blocks.0.17.attn.qkv.weight\n",
      "Layer: blocks.0.17.attn.qkv.bias\n",
      "Layer: blocks.0.17.attn.proj.weight\n",
      "Layer: blocks.0.17.attn.proj.bias\n",
      "Layer: blocks.0.17.norm2.weight\n",
      "Layer: blocks.0.17.norm2.bias\n",
      "Layer: blocks.0.17.mlp.fc1.weight\n",
      "Layer: blocks.0.17.mlp.fc1.bias\n",
      "Layer: blocks.0.17.mlp.fc2.weight\n",
      "Layer: blocks.0.17.mlp.fc2.bias\n",
      "Layer: blocks.0.18.norm1.weight\n",
      "Layer: blocks.0.18.norm1.bias\n",
      "Layer: blocks.0.18.attn.qkv.weight\n",
      "Layer: blocks.0.18.attn.qkv.bias\n",
      "Layer: blocks.0.18.attn.proj.weight\n",
      "Layer: blocks.0.18.attn.proj.bias\n",
      "Layer: blocks.0.18.norm2.weight\n",
      "Layer: blocks.0.18.norm2.bias\n",
      "Layer: blocks.0.18.mlp.fc1.weight\n",
      "Layer: blocks.0.18.mlp.fc1.bias\n",
      "Layer: blocks.0.18.mlp.fc2.weight\n",
      "Layer: blocks.0.18.mlp.fc2.bias\n",
      "Layer: blocks.0.19.norm1.weight\n",
      "Layer: blocks.0.19.norm1.bias\n",
      "Layer: blocks.0.19.attn.qkv.weight\n",
      "Layer: blocks.0.19.attn.qkv.bias\n",
      "Layer: blocks.0.19.attn.proj.weight\n",
      "Layer: blocks.0.19.attn.proj.bias\n",
      "Layer: blocks.0.19.norm2.weight\n",
      "Layer: blocks.0.19.norm2.bias\n",
      "Layer: blocks.0.19.mlp.fc1.weight\n",
      "Layer: blocks.0.19.mlp.fc1.bias\n",
      "Layer: blocks.0.19.mlp.fc2.weight\n",
      "Layer: blocks.0.19.mlp.fc2.bias\n",
      "Layer: blocks.0.20.norm1.weight\n",
      "Layer: blocks.0.20.norm1.bias\n",
      "Layer: blocks.0.20.attn.qkv.weight\n",
      "Layer: blocks.0.20.attn.qkv.bias\n",
      "Layer: blocks.0.20.attn.proj.weight\n",
      "Layer: blocks.0.20.attn.proj.bias\n",
      "Layer: blocks.0.20.norm2.weight\n",
      "Layer: blocks.0.20.norm2.bias\n",
      "Layer: blocks.0.20.mlp.fc1.weight\n",
      "Layer: blocks.0.20.mlp.fc1.bias\n",
      "Layer: blocks.0.20.mlp.fc2.weight\n",
      "Layer: blocks.0.20.mlp.fc2.bias\n",
      "Layer: blocks.0.21.norm1.weight\n",
      "Layer: blocks.0.21.norm1.bias\n",
      "Layer: blocks.0.21.attn.qkv.weight\n",
      "Layer: blocks.0.21.attn.qkv.bias\n",
      "Layer: blocks.0.21.attn.proj.weight\n",
      "Layer: blocks.0.21.attn.proj.bias\n",
      "Layer: blocks.0.21.norm2.weight\n",
      "Layer: blocks.0.21.norm2.bias\n",
      "Layer: blocks.0.21.mlp.fc1.weight\n",
      "Layer: blocks.0.21.mlp.fc1.bias\n",
      "Layer: blocks.0.21.mlp.fc2.weight\n",
      "Layer: blocks.0.21.mlp.fc2.bias\n",
      "Layer: blocks.0.22.norm1.weight\n",
      "Layer: blocks.0.22.norm1.bias\n",
      "Layer: blocks.0.22.attn.qkv.weight\n",
      "Layer: blocks.0.22.attn.qkv.bias\n",
      "Layer: blocks.0.22.attn.proj.weight\n",
      "Layer: blocks.0.22.attn.proj.bias\n",
      "Layer: blocks.0.22.norm2.weight\n",
      "Layer: blocks.0.22.norm2.bias\n",
      "Layer: blocks.0.22.mlp.fc1.weight\n",
      "Layer: blocks.0.22.mlp.fc1.bias\n",
      "Layer: blocks.0.22.mlp.fc2.weight\n",
      "Layer: blocks.0.22.mlp.fc2.bias\n",
      "Layer: blocks.0.23.norm1.weight\n",
      "Layer: blocks.0.23.norm1.bias\n",
      "Layer: blocks.0.23.attn.qkv.weight\n",
      "Layer: blocks.0.23.attn.qkv.bias\n",
      "Layer: blocks.0.23.attn.proj.weight\n",
      "Layer: blocks.0.23.attn.proj.bias\n",
      "Layer: blocks.0.23.norm2.weight\n",
      "Layer: blocks.0.23.norm2.bias\n",
      "Layer: blocks.0.23.mlp.fc1.weight\n",
      "Layer: blocks.0.23.mlp.fc1.bias\n",
      "Layer: blocks.0.23.mlp.fc2.weight\n",
      "Layer: blocks.0.23.mlp.fc2.bias\n",
      "Layer: norm.weight\n",
      "Layer: norm.bias\n",
      "\n",
      "Layers not loaded:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "from vision_transformer import DinoVisionTransformer  # 导入您的ViT模型\n",
    "\n",
    "# 加载模型权重\n",
    "  # 这里是您上传的权重文件\n",
    "state_dict = torch.load('UNI_updated.bin')\n",
    "\n",
    "# 实例化Vision Transformer模型\n",
    "model = model = DinoVisionTransformer(embed_dim=1024, depth=24, num_heads=16, patch_size=16)\n",
    "\n",
    "# 获取模型的当前参数\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# 比较并记录哪些层成功加载，哪些层未加载\n",
    "loaded_keys = []\n",
    "unloaded_keys = []\n",
    "\n",
    "# 遍历加载的 state_dict\n",
    "for key in state_dict.keys():\n",
    "    if key in model_state_dict:\n",
    "        # 如果层名存在于模型中，记录为已加载\n",
    "        loaded_keys.append(key)\n",
    "    else:\n",
    "        # 否则记录为未加载\n",
    "        unloaded_keys.append(key)\n",
    "\n",
    "# 输出加载成功和未加载的层\n",
    "print(\"Successfully loaded layers:\")\n",
    "for layer in loaded_keys:\n",
    "    print(f\"Layer: {layer}\")\n",
    "\n",
    "print(\"\\nLayers not loaded:\")\n",
    "for layer in unloaded_keys:\n",
    "    print(f\"Layer: {layer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "# @Email: wangguisen@donews.com\n",
    "# @Time: 2023/3/22 17:39\n",
    "# @File: ddd.py\n",
    "\n",
    "'''\n",
    "https://github.com/huggingface/safetensors\n",
    "https://huggingface.co/docs/scommand:workbench.action.openLargeOutput?dc86f79d-18d5-4662-8fd9-a6fc1602be61afetensors/index\n",
    "\n",
    "pip install safetensors\n",
    "torch >= 2.0\n",
    "'''\n",
    "import torch\n",
    "from safetensors.torch import load_file, save_file\n",
    "from safetensors import safe_open\n",
    "\n",
    "model_path = 'hibou-b.safetensors'\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(model_path, framework=\"pt\", device='cpu') as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)\n",
    "\n",
    "#print(tensors)\n",
    "\n",
    "# model.load_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['embeddings.cls_token', 'embeddings.mask_token', 'embeddings.patch_embeddings.projection.bias', 'embeddings.patch_embeddings.projection.weight', 'embeddings.position_embeddings', 'embeddings.register_tokens', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.layer_scale1.lambda1', 'encoder.layer.0.layer_scale2.lambda1', 'encoder.layer.0.mlp.weights_in.bias', 'encoder.layer.0.mlp.weights_in.weight', 'encoder.layer.0.mlp.weights_out.bias', 'encoder.layer.0.mlp.weights_out.weight', 'encoder.layer.0.norm1.bias', 'encoder.layer.0.norm1.weight', 'encoder.layer.0.norm2.bias', 'encoder.layer.0.norm2.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.layer_scale1.lambda1', 'encoder.layer.1.layer_scale2.lambda1', 'encoder.layer.1.mlp.weights_in.bias', 'encoder.layer.1.mlp.weights_in.weight', 'encoder.layer.1.mlp.weights_out.bias', 'encoder.layer.1.mlp.weights_out.weight', 'encoder.layer.1.norm1.bias', 'encoder.layer.1.norm1.weight', 'encoder.layer.1.norm2.bias', 'encoder.layer.1.norm2.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.layer_scale1.lambda1', 'encoder.layer.10.layer_scale2.lambda1', 'encoder.layer.10.mlp.weights_in.bias', 'encoder.layer.10.mlp.weights_in.weight', 'encoder.layer.10.mlp.weights_out.bias', 'encoder.layer.10.mlp.weights_out.weight', 'encoder.layer.10.norm1.bias', 'encoder.layer.10.norm1.weight', 'encoder.layer.10.norm2.bias', 'encoder.layer.10.norm2.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.layer_scale1.lambda1', 'encoder.layer.11.layer_scale2.lambda1', 'encoder.layer.11.mlp.weights_in.bias', 'encoder.layer.11.mlp.weights_in.weight', 'encoder.layer.11.mlp.weights_out.bias', 'encoder.layer.11.mlp.weights_out.weight', 'encoder.layer.11.norm1.bias', 'encoder.layer.11.norm1.weight', 'encoder.layer.11.norm2.bias', 'encoder.layer.11.norm2.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.layer_scale1.lambda1', 'encoder.layer.2.layer_scale2.lambda1', 'encoder.layer.2.mlp.weights_in.bias', 'encoder.layer.2.mlp.weights_in.weight', 'encoder.layer.2.mlp.weights_out.bias', 'encoder.layer.2.mlp.weights_out.weight', 'encoder.layer.2.norm1.bias', 'encoder.layer.2.norm1.weight', 'encoder.layer.2.norm2.bias', 'encoder.layer.2.norm2.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.layer_scale1.lambda1', 'encoder.layer.3.layer_scale2.lambda1', 'encoder.layer.3.mlp.weights_in.bias', 'encoder.layer.3.mlp.weights_in.weight', 'encoder.layer.3.mlp.weights_out.bias', 'encoder.layer.3.mlp.weights_out.weight', 'encoder.layer.3.norm1.bias', 'encoder.layer.3.norm1.weight', 'encoder.layer.3.norm2.bias', 'encoder.layer.3.norm2.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.layer_scale1.lambda1', 'encoder.layer.4.layer_scale2.lambda1', 'encoder.layer.4.mlp.weights_in.bias', 'encoder.layer.4.mlp.weights_in.weight', 'encoder.layer.4.mlp.weights_out.bias', 'encoder.layer.4.mlp.weights_out.weight', 'encoder.layer.4.norm1.bias', 'encoder.layer.4.norm1.weight', 'encoder.layer.4.norm2.bias', 'encoder.layer.4.norm2.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.layer_scale1.lambda1', 'encoder.layer.5.layer_scale2.lambda1', 'encoder.layer.5.mlp.weights_in.bias', 'encoder.layer.5.mlp.weights_in.weight', 'encoder.layer.5.mlp.weights_out.bias', 'encoder.layer.5.mlp.weights_out.weight', 'encoder.layer.5.norm1.bias', 'encoder.layer.5.norm1.weight', 'encoder.layer.5.norm2.bias', 'encoder.layer.5.norm2.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.layer_scale1.lambda1', 'encoder.layer.6.layer_scale2.lambda1', 'encoder.layer.6.mlp.weights_in.bias', 'encoder.layer.6.mlp.weights_in.weight', 'encoder.layer.6.mlp.weights_out.bias', 'encoder.layer.6.mlp.weights_out.weight', 'encoder.layer.6.norm1.bias', 'encoder.layer.6.norm1.weight', 'encoder.layer.6.norm2.bias', 'encoder.layer.6.norm2.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.layer_scale1.lambda1', 'encoder.layer.7.layer_scale2.lambda1', 'encoder.layer.7.mlp.weights_in.bias', 'encoder.layer.7.mlp.weights_in.weight', 'encoder.layer.7.mlp.weights_out.bias', 'encoder.layer.7.mlp.weights_out.weight', 'encoder.layer.7.norm1.bias', 'encoder.layer.7.norm1.weight', 'encoder.layer.7.norm2.bias', 'encoder.layer.7.norm2.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.layer_scale1.lambda1', 'encoder.layer.8.layer_scale2.lambda1', 'encoder.layer.8.mlp.weights_in.bias', 'encoder.layer.8.mlp.weights_in.weight', 'encoder.layer.8.mlp.weights_out.bias', 'encoder.layer.8.mlp.weights_out.weight', 'encoder.layer.8.norm1.bias', 'encoder.layer.8.norm1.weight', 'encoder.layer.8.norm2.bias', 'encoder.layer.8.norm2.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.layer_scale1.lambda1', 'encoder.layer.9.layer_scale2.lambda1', 'encoder.layer.9.mlp.weights_in.bias', 'encoder.layer.9.mlp.weights_in.weight', 'encoder.layer.9.mlp.weights_out.bias', 'encoder.layer.9.mlp.weights_out.weight', 'encoder.layer.9.norm1.bias', 'encoder.layer.9.norm1.weight', 'encoder.layer.9.norm2.bias', 'encoder.layer.9.norm2.weight', 'layernorm.bias', 'layernorm.weight'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.cls_token\n",
      "embeddings.mask_token\n",
      "embeddings.patch_embeddings.projection.bias\n",
      "embeddings.patch_embeddings.projection.weight\n",
      "embeddings.position_embeddings\n",
      "embeddings.register_tokens\n",
      "encoder.layer.0.attention.attention.key.bias\n",
      "encoder.layer.0.attention.attention.key.weight\n",
      "encoder.layer.0.attention.attention.query.bias\n",
      "encoder.layer.0.attention.attention.query.weight\n",
      "encoder.layer.0.attention.attention.value.bias\n",
      "encoder.layer.0.attention.attention.value.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.layer_scale1.lambda1\n",
      "encoder.layer.0.layer_scale2.lambda1\n",
      "encoder.layer.0.mlp.weights_in.bias\n",
      "encoder.layer.0.mlp.weights_in.weight\n",
      "encoder.layer.0.mlp.weights_out.bias\n",
      "encoder.layer.0.mlp.weights_out.weight\n",
      "encoder.layer.0.norm1.bias\n",
      "encoder.layer.0.norm1.weight\n",
      "encoder.layer.0.norm2.bias\n",
      "encoder.layer.0.norm2.weight\n",
      "encoder.layer.1.attention.attention.key.bias\n",
      "encoder.layer.1.attention.attention.key.weight\n",
      "encoder.layer.1.attention.attention.query.bias\n",
      "encoder.layer.1.attention.attention.query.weight\n",
      "encoder.layer.1.attention.attention.value.bias\n",
      "encoder.layer.1.attention.attention.value.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.layer_scale1.lambda1\n",
      "encoder.layer.1.layer_scale2.lambda1\n",
      "encoder.layer.1.mlp.weights_in.bias\n",
      "encoder.layer.1.mlp.weights_in.weight\n",
      "encoder.layer.1.mlp.weights_out.bias\n",
      "encoder.layer.1.mlp.weights_out.weight\n",
      "encoder.layer.1.norm1.bias\n",
      "encoder.layer.1.norm1.weight\n",
      "encoder.layer.1.norm2.bias\n",
      "encoder.layer.1.norm2.weight\n",
      "encoder.layer.10.attention.attention.key.bias\n",
      "encoder.layer.10.attention.attention.key.weight\n",
      "encoder.layer.10.attention.attention.query.bias\n",
      "encoder.layer.10.attention.attention.query.weight\n",
      "encoder.layer.10.attention.attention.value.bias\n",
      "encoder.layer.10.attention.attention.value.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.layer_scale1.lambda1\n",
      "encoder.layer.10.layer_scale2.lambda1\n",
      "encoder.layer.10.mlp.weights_in.bias\n",
      "encoder.layer.10.mlp.weights_in.weight\n",
      "encoder.layer.10.mlp.weights_out.bias\n",
      "encoder.layer.10.mlp.weights_out.weight\n",
      "encoder.layer.10.norm1.bias\n",
      "encoder.layer.10.norm1.weight\n",
      "encoder.layer.10.norm2.bias\n",
      "encoder.layer.10.norm2.weight\n",
      "encoder.layer.11.attention.attention.key.bias\n",
      "encoder.layer.11.attention.attention.key.weight\n",
      "encoder.layer.11.attention.attention.query.bias\n",
      "encoder.layer.11.attention.attention.query.weight\n",
      "encoder.layer.11.attention.attention.value.bias\n",
      "encoder.layer.11.attention.attention.value.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.layer_scale1.lambda1\n",
      "encoder.layer.11.layer_scale2.lambda1\n",
      "encoder.layer.11.mlp.weights_in.bias\n",
      "encoder.layer.11.mlp.weights_in.weight\n",
      "encoder.layer.11.mlp.weights_out.bias\n",
      "encoder.layer.11.mlp.weights_out.weight\n",
      "encoder.layer.11.norm1.bias\n",
      "encoder.layer.11.norm1.weight\n",
      "encoder.layer.11.norm2.bias\n",
      "encoder.layer.11.norm2.weight\n",
      "encoder.layer.2.attention.attention.key.bias\n",
      "encoder.layer.2.attention.attention.key.weight\n",
      "encoder.layer.2.attention.attention.query.bias\n",
      "encoder.layer.2.attention.attention.query.weight\n",
      "encoder.layer.2.attention.attention.value.bias\n",
      "encoder.layer.2.attention.attention.value.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.layer_scale1.lambda1\n",
      "encoder.layer.2.layer_scale2.lambda1\n",
      "encoder.layer.2.mlp.weights_in.bias\n",
      "encoder.layer.2.mlp.weights_in.weight\n",
      "encoder.layer.2.mlp.weights_out.bias\n",
      "encoder.layer.2.mlp.weights_out.weight\n",
      "encoder.layer.2.norm1.bias\n",
      "encoder.layer.2.norm1.weight\n",
      "encoder.layer.2.norm2.bias\n",
      "encoder.layer.2.norm2.weight\n",
      "encoder.layer.3.attention.attention.key.bias\n",
      "encoder.layer.3.attention.attention.key.weight\n",
      "encoder.layer.3.attention.attention.query.bias\n",
      "encoder.layer.3.attention.attention.query.weight\n",
      "encoder.layer.3.attention.attention.value.bias\n",
      "encoder.layer.3.attention.attention.value.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.layer_scale1.lambda1\n",
      "encoder.layer.3.layer_scale2.lambda1\n",
      "encoder.layer.3.mlp.weights_in.bias\n",
      "encoder.layer.3.mlp.weights_in.weight\n",
      "encoder.layer.3.mlp.weights_out.bias\n",
      "encoder.layer.3.mlp.weights_out.weight\n",
      "encoder.layer.3.norm1.bias\n",
      "encoder.layer.3.norm1.weight\n",
      "encoder.layer.3.norm2.bias\n",
      "encoder.layer.3.norm2.weight\n",
      "encoder.layer.4.attention.attention.key.bias\n",
      "encoder.layer.4.attention.attention.key.weight\n",
      "encoder.layer.4.attention.attention.query.bias\n",
      "encoder.layer.4.attention.attention.query.weight\n",
      "encoder.layer.4.attention.attention.value.bias\n",
      "encoder.layer.4.attention.attention.value.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.layer_scale1.lambda1\n",
      "encoder.layer.4.layer_scale2.lambda1\n",
      "encoder.layer.4.mlp.weights_in.bias\n",
      "encoder.layer.4.mlp.weights_in.weight\n",
      "encoder.layer.4.mlp.weights_out.bias\n",
      "encoder.layer.4.mlp.weights_out.weight\n",
      "encoder.layer.4.norm1.bias\n",
      "encoder.layer.4.norm1.weight\n",
      "encoder.layer.4.norm2.bias\n",
      "encoder.layer.4.norm2.weight\n",
      "encoder.layer.5.attention.attention.key.bias\n",
      "encoder.layer.5.attention.attention.key.weight\n",
      "encoder.layer.5.attention.attention.query.bias\n",
      "encoder.layer.5.attention.attention.query.weight\n",
      "encoder.layer.5.attention.attention.value.bias\n",
      "encoder.layer.5.attention.attention.value.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.layer_scale1.lambda1\n",
      "encoder.layer.5.layer_scale2.lambda1\n",
      "encoder.layer.5.mlp.weights_in.bias\n",
      "encoder.layer.5.mlp.weights_in.weight\n",
      "encoder.layer.5.mlp.weights_out.bias\n",
      "encoder.layer.5.mlp.weights_out.weight\n",
      "encoder.layer.5.norm1.bias\n",
      "encoder.layer.5.norm1.weight\n",
      "encoder.layer.5.norm2.bias\n",
      "encoder.layer.5.norm2.weight\n",
      "encoder.layer.6.attention.attention.key.bias\n",
      "encoder.layer.6.attention.attention.key.weight\n",
      "encoder.layer.6.attention.attention.query.bias\n",
      "encoder.layer.6.attention.attention.query.weight\n",
      "encoder.layer.6.attention.attention.value.bias\n",
      "encoder.layer.6.attention.attention.value.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.layer_scale1.lambda1\n",
      "encoder.layer.6.layer_scale2.lambda1\n",
      "encoder.layer.6.mlp.weights_in.bias\n",
      "encoder.layer.6.mlp.weights_in.weight\n",
      "encoder.layer.6.mlp.weights_out.bias\n",
      "encoder.layer.6.mlp.weights_out.weight\n",
      "encoder.layer.6.norm1.bias\n",
      "encoder.layer.6.norm1.weight\n",
      "encoder.layer.6.norm2.bias\n",
      "encoder.layer.6.norm2.weight\n",
      "encoder.layer.7.attention.attention.key.bias\n",
      "encoder.layer.7.attention.attention.key.weight\n",
      "encoder.layer.7.attention.attention.query.bias\n",
      "encoder.layer.7.attention.attention.query.weight\n",
      "encoder.layer.7.attention.attention.value.bias\n",
      "encoder.layer.7.attention.attention.value.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.layer_scale1.lambda1\n",
      "encoder.layer.7.layer_scale2.lambda1\n",
      "encoder.layer.7.mlp.weights_in.bias\n",
      "encoder.layer.7.mlp.weights_in.weight\n",
      "encoder.layer.7.mlp.weights_out.bias\n",
      "encoder.layer.7.mlp.weights_out.weight\n",
      "encoder.layer.7.norm1.bias\n",
      "encoder.layer.7.norm1.weight\n",
      "encoder.layer.7.norm2.bias\n",
      "encoder.layer.7.norm2.weight\n",
      "encoder.layer.8.attention.attention.key.bias\n",
      "encoder.layer.8.attention.attention.key.weight\n",
      "encoder.layer.8.attention.attention.query.bias\n",
      "encoder.layer.8.attention.attention.query.weight\n",
      "encoder.layer.8.attention.attention.value.bias\n",
      "encoder.layer.8.attention.attention.value.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.layer_scale1.lambda1\n",
      "encoder.layer.8.layer_scale2.lambda1\n",
      "encoder.layer.8.mlp.weights_in.bias\n",
      "encoder.layer.8.mlp.weights_in.weight\n",
      "encoder.layer.8.mlp.weights_out.bias\n",
      "encoder.layer.8.mlp.weights_out.weight\n",
      "encoder.layer.8.norm1.bias\n",
      "encoder.layer.8.norm1.weight\n",
      "encoder.layer.8.norm2.bias\n",
      "encoder.layer.8.norm2.weight\n",
      "encoder.layer.9.attention.attention.key.bias\n",
      "encoder.layer.9.attention.attention.key.weight\n",
      "encoder.layer.9.attention.attention.query.bias\n",
      "encoder.layer.9.attention.attention.query.weight\n",
      "encoder.layer.9.attention.attention.value.bias\n",
      "encoder.layer.9.attention.attention.value.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.layer_scale1.lambda1\n",
      "encoder.layer.9.layer_scale2.lambda1\n",
      "encoder.layer.9.mlp.weights_in.bias\n",
      "encoder.layer.9.mlp.weights_in.weight\n",
      "encoder.layer.9.mlp.weights_out.bias\n",
      "encoder.layer.9.mlp.weights_out.weight\n",
      "encoder.layer.9.norm1.bias\n",
      "encoder.layer.9.norm1.weight\n",
      "encoder.layer.9.norm2.bias\n",
      "encoder.layer.9.norm2.weight\n",
      "layernorm.bias\n",
      "layernorm.weight\n"
     ]
    }
   ],
   "source": [
    "for key in tensors.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_base = DinoVisionTransformer(embed_dim=768, depth=12, num_heads=12, patch_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token :load successful\n",
      "pos_embed :load successful\n",
      "patch_embed.proj.weight :load successful\n",
      "patch_embed.proj.bias :load successful\n",
      "norm.weight :load successful\n",
      "norm.bias :load successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model=vit_base\n",
    "# Load the state_dict from the saved file\n",
    "state_dict =torch.load('CONCH.bin')\n",
    "\n",
    "\n",
    "# Create a copy of the state_dict with updated key names\n",
    "updated_state_dict = {}\n",
    "\n",
    "for key in state_dict.keys():\n",
    "    #print(key)\n",
    "    if 'visual.trunk.' in key:\n",
    "        temp_key = key.replace('visual.trunk.', '')\n",
    "        if 'blocks.' in temp_key:\n",
    "            # Rename keys from 'blocks' to 'blocks.0'\n",
    "            new_key = temp_key.replace('blocks.', 'blocks.0.')\n",
    "           # print(new_key)\n",
    "            if new_key in model.state_dict().keys():\n",
    "                #print(new_key,':load successful')\n",
    "                updated_state_dict[new_key] = state_dict[key]\n",
    "        else:\n",
    "            #print(temp_key)\n",
    "            if temp_key in model.state_dict().keys():\n",
    "                print(temp_key,':load successful')\n",
    "                updated_state_dict[temp_key] = state_dict[key]\n",
    "            else:\n",
    "                \n",
    "                print(f\"Key {key} not found in the model's state_dict\")\n",
    "\n",
    "\n",
    "# Save the updated state_dict back to a file\n",
    "torch.save(updated_state_dict, 'CONCH_updated.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token\n",
      "pos_embed\n",
      "register_tokens\n",
      "mask_token\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "blocks.0.0.norm1.weight\n",
      "blocks.0.0.norm1.bias\n",
      "blocks.0.0.attn.qkv.weight\n",
      "blocks.0.0.attn.qkv.bias\n",
      "blocks.0.0.attn.proj.weight\n",
      "blocks.0.0.attn.proj.bias\n",
      "blocks.0.0.norm2.weight\n",
      "blocks.0.0.norm2.bias\n",
      "blocks.0.0.mlp.fc1.weight\n",
      "blocks.0.0.mlp.fc1.bias\n",
      "blocks.0.0.mlp.fc2.weight\n",
      "blocks.0.0.mlp.fc2.bias\n",
      "blocks.0.1.norm1.weight\n",
      "blocks.0.1.norm1.bias\n",
      "blocks.0.1.attn.qkv.weight\n",
      "blocks.0.1.attn.qkv.bias\n",
      "blocks.0.1.attn.proj.weight\n",
      "blocks.0.1.attn.proj.bias\n",
      "blocks.0.1.norm2.weight\n",
      "blocks.0.1.norm2.bias\n",
      "blocks.0.1.mlp.fc1.weight\n",
      "blocks.0.1.mlp.fc1.bias\n",
      "blocks.0.1.mlp.fc2.weight\n",
      "blocks.0.1.mlp.fc2.bias\n",
      "blocks.0.2.norm1.weight\n",
      "blocks.0.2.norm1.bias\n",
      "blocks.0.2.attn.qkv.weight\n",
      "blocks.0.2.attn.qkv.bias\n",
      "blocks.0.2.attn.proj.weight\n",
      "blocks.0.2.attn.proj.bias\n",
      "blocks.0.2.norm2.weight\n",
      "blocks.0.2.norm2.bias\n",
      "blocks.0.2.mlp.fc1.weight\n",
      "blocks.0.2.mlp.fc1.bias\n",
      "blocks.0.2.mlp.fc2.weight\n",
      "blocks.0.2.mlp.fc2.bias\n",
      "blocks.0.3.norm1.weight\n",
      "blocks.0.3.norm1.bias\n",
      "blocks.0.3.attn.qkv.weight\n",
      "blocks.0.3.attn.qkv.bias\n",
      "blocks.0.3.attn.proj.weight\n",
      "blocks.0.3.attn.proj.bias\n",
      "blocks.0.3.norm2.weight\n",
      "blocks.0.3.norm2.bias\n",
      "blocks.0.3.mlp.fc1.weight\n",
      "blocks.0.3.mlp.fc1.bias\n",
      "blocks.0.3.mlp.fc2.weight\n",
      "blocks.0.3.mlp.fc2.bias\n",
      "blocks.0.4.norm1.weight\n",
      "blocks.0.4.norm1.bias\n",
      "blocks.0.4.attn.qkv.weight\n",
      "blocks.0.4.attn.qkv.bias\n",
      "blocks.0.4.attn.proj.weight\n",
      "blocks.0.4.attn.proj.bias\n",
      "blocks.0.4.norm2.weight\n",
      "blocks.0.4.norm2.bias\n",
      "blocks.0.4.mlp.fc1.weight\n",
      "blocks.0.4.mlp.fc1.bias\n",
      "blocks.0.4.mlp.fc2.weight\n",
      "blocks.0.4.mlp.fc2.bias\n",
      "blocks.0.5.norm1.weight\n",
      "blocks.0.5.norm1.bias\n",
      "blocks.0.5.attn.qkv.weight\n",
      "blocks.0.5.attn.qkv.bias\n",
      "blocks.0.5.attn.proj.weight\n",
      "blocks.0.5.attn.proj.bias\n",
      "blocks.0.5.norm2.weight\n",
      "blocks.0.5.norm2.bias\n",
      "blocks.0.5.mlp.fc1.weight\n",
      "blocks.0.5.mlp.fc1.bias\n",
      "blocks.0.5.mlp.fc2.weight\n",
      "blocks.0.5.mlp.fc2.bias\n",
      "blocks.0.6.norm1.weight\n",
      "blocks.0.6.norm1.bias\n",
      "blocks.0.6.attn.qkv.weight\n",
      "blocks.0.6.attn.qkv.bias\n",
      "blocks.0.6.attn.proj.weight\n",
      "blocks.0.6.attn.proj.bias\n",
      "blocks.0.6.norm2.weight\n",
      "blocks.0.6.norm2.bias\n",
      "blocks.0.6.mlp.fc1.weight\n",
      "blocks.0.6.mlp.fc1.bias\n",
      "blocks.0.6.mlp.fc2.weight\n",
      "blocks.0.6.mlp.fc2.bias\n",
      "blocks.0.7.norm1.weight\n",
      "blocks.0.7.norm1.bias\n",
      "blocks.0.7.attn.qkv.weight\n",
      "blocks.0.7.attn.qkv.bias\n",
      "blocks.0.7.attn.proj.weight\n",
      "blocks.0.7.attn.proj.bias\n",
      "blocks.0.7.norm2.weight\n",
      "blocks.0.7.norm2.bias\n",
      "blocks.0.7.mlp.fc1.weight\n",
      "blocks.0.7.mlp.fc1.bias\n",
      "blocks.0.7.mlp.fc2.weight\n",
      "blocks.0.7.mlp.fc2.bias\n",
      "blocks.0.8.norm1.weight\n",
      "blocks.0.8.norm1.bias\n",
      "blocks.0.8.attn.qkv.weight\n",
      "blocks.0.8.attn.qkv.bias\n",
      "blocks.0.8.attn.proj.weight\n",
      "blocks.0.8.attn.proj.bias\n",
      "blocks.0.8.norm2.weight\n",
      "blocks.0.8.norm2.bias\n",
      "blocks.0.8.mlp.fc1.weight\n",
      "blocks.0.8.mlp.fc1.bias\n",
      "blocks.0.8.mlp.fc2.weight\n",
      "blocks.0.8.mlp.fc2.bias\n",
      "blocks.0.9.norm1.weight\n",
      "blocks.0.9.norm1.bias\n",
      "blocks.0.9.attn.qkv.weight\n",
      "blocks.0.9.attn.qkv.bias\n",
      "blocks.0.9.attn.proj.weight\n",
      "blocks.0.9.attn.proj.bias\n",
      "blocks.0.9.norm2.weight\n",
      "blocks.0.9.norm2.bias\n",
      "blocks.0.9.mlp.fc1.weight\n",
      "blocks.0.9.mlp.fc1.bias\n",
      "blocks.0.9.mlp.fc2.weight\n",
      "blocks.0.9.mlp.fc2.bias\n",
      "blocks.0.10.norm1.weight\n",
      "blocks.0.10.norm1.bias\n",
      "blocks.0.10.attn.qkv.weight\n",
      "blocks.0.10.attn.qkv.bias\n",
      "blocks.0.10.attn.proj.weight\n",
      "blocks.0.10.attn.proj.bias\n",
      "blocks.0.10.norm2.weight\n",
      "blocks.0.10.norm2.bias\n",
      "blocks.0.10.mlp.fc1.weight\n",
      "blocks.0.10.mlp.fc1.bias\n",
      "blocks.0.10.mlp.fc2.weight\n",
      "blocks.0.10.mlp.fc2.bias\n",
      "blocks.0.11.norm1.weight\n",
      "blocks.0.11.norm1.bias\n",
      "blocks.0.11.attn.qkv.weight\n",
      "blocks.0.11.attn.qkv.bias\n",
      "blocks.0.11.attn.proj.weight\n",
      "blocks.0.11.attn.proj.bias\n",
      "blocks.0.11.norm2.weight\n",
      "blocks.0.11.norm2.bias\n",
      "blocks.0.11.mlp.fc1.weight\n",
      "blocks.0.11.mlp.fc1.bias\n",
      "blocks.0.11.mlp.fc2.weight\n",
      "blocks.0.11.mlp.fc2.bias\n",
      "blocks.0.12.norm1.weight\n",
      "blocks.0.12.norm1.bias\n",
      "blocks.0.12.attn.qkv.weight\n",
      "blocks.0.12.attn.qkv.bias\n",
      "blocks.0.12.attn.proj.weight\n",
      "blocks.0.12.attn.proj.bias\n",
      "blocks.0.12.norm2.weight\n",
      "blocks.0.12.norm2.bias\n",
      "blocks.0.12.mlp.fc1.weight\n",
      "blocks.0.12.mlp.fc1.bias\n",
      "blocks.0.12.mlp.fc2.weight\n",
      "blocks.0.12.mlp.fc2.bias\n",
      "blocks.0.13.norm1.weight\n",
      "blocks.0.13.norm1.bias\n",
      "blocks.0.13.attn.qkv.weight\n",
      "blocks.0.13.attn.qkv.bias\n",
      "blocks.0.13.attn.proj.weight\n",
      "blocks.0.13.attn.proj.bias\n",
      "blocks.0.13.norm2.weight\n",
      "blocks.0.13.norm2.bias\n",
      "blocks.0.13.mlp.fc1.weight\n",
      "blocks.0.13.mlp.fc1.bias\n",
      "blocks.0.13.mlp.fc2.weight\n",
      "blocks.0.13.mlp.fc2.bias\n",
      "blocks.0.14.norm1.weight\n",
      "blocks.0.14.norm1.bias\n",
      "blocks.0.14.attn.qkv.weight\n",
      "blocks.0.14.attn.qkv.bias\n",
      "blocks.0.14.attn.proj.weight\n",
      "blocks.0.14.attn.proj.bias\n",
      "blocks.0.14.norm2.weight\n",
      "blocks.0.14.norm2.bias\n",
      "blocks.0.14.mlp.fc1.weight\n",
      "blocks.0.14.mlp.fc1.bias\n",
      "blocks.0.14.mlp.fc2.weight\n",
      "blocks.0.14.mlp.fc2.bias\n",
      "blocks.0.15.norm1.weight\n",
      "blocks.0.15.norm1.bias\n",
      "blocks.0.15.attn.qkv.weight\n",
      "blocks.0.15.attn.qkv.bias\n",
      "blocks.0.15.attn.proj.weight\n",
      "blocks.0.15.attn.proj.bias\n",
      "blocks.0.15.norm2.weight\n",
      "blocks.0.15.norm2.bias\n",
      "blocks.0.15.mlp.fc1.weight\n",
      "blocks.0.15.mlp.fc1.bias\n",
      "blocks.0.15.mlp.fc2.weight\n",
      "blocks.0.15.mlp.fc2.bias\n",
      "blocks.0.16.norm1.weight\n",
      "blocks.0.16.norm1.bias\n",
      "blocks.0.16.attn.qkv.weight\n",
      "blocks.0.16.attn.qkv.bias\n",
      "blocks.0.16.attn.proj.weight\n",
      "blocks.0.16.attn.proj.bias\n",
      "blocks.0.16.norm2.weight\n",
      "blocks.0.16.norm2.bias\n",
      "blocks.0.16.mlp.fc1.weight\n",
      "blocks.0.16.mlp.fc1.bias\n",
      "blocks.0.16.mlp.fc2.weight\n",
      "blocks.0.16.mlp.fc2.bias\n",
      "blocks.0.17.norm1.weight\n",
      "blocks.0.17.norm1.bias\n",
      "blocks.0.17.attn.qkv.weight\n",
      "blocks.0.17.attn.qkv.bias\n",
      "blocks.0.17.attn.proj.weight\n",
      "blocks.0.17.attn.proj.bias\n",
      "blocks.0.17.norm2.weight\n",
      "blocks.0.17.norm2.bias\n",
      "blocks.0.17.mlp.fc1.weight\n",
      "blocks.0.17.mlp.fc1.bias\n",
      "blocks.0.17.mlp.fc2.weight\n",
      "blocks.0.17.mlp.fc2.bias\n",
      "blocks.0.18.norm1.weight\n",
      "blocks.0.18.norm1.bias\n",
      "blocks.0.18.attn.qkv.weight\n",
      "blocks.0.18.attn.qkv.bias\n",
      "blocks.0.18.attn.proj.weight\n",
      "blocks.0.18.attn.proj.bias\n",
      "blocks.0.18.norm2.weight\n",
      "blocks.0.18.norm2.bias\n",
      "blocks.0.18.mlp.fc1.weight\n",
      "blocks.0.18.mlp.fc1.bias\n",
      "blocks.0.18.mlp.fc2.weight\n",
      "blocks.0.18.mlp.fc2.bias\n",
      "blocks.0.19.norm1.weight\n",
      "blocks.0.19.norm1.bias\n",
      "blocks.0.19.attn.qkv.weight\n",
      "blocks.0.19.attn.qkv.bias\n",
      "blocks.0.19.attn.proj.weight\n",
      "blocks.0.19.attn.proj.bias\n",
      "blocks.0.19.norm2.weight\n",
      "blocks.0.19.norm2.bias\n",
      "blocks.0.19.mlp.fc1.weight\n",
      "blocks.0.19.mlp.fc1.bias\n",
      "blocks.0.19.mlp.fc2.weight\n",
      "blocks.0.19.mlp.fc2.bias\n",
      "blocks.0.20.norm1.weight\n",
      "blocks.0.20.norm1.bias\n",
      "blocks.0.20.attn.qkv.weight\n",
      "blocks.0.20.attn.qkv.bias\n",
      "blocks.0.20.attn.proj.weight\n",
      "blocks.0.20.attn.proj.bias\n",
      "blocks.0.20.norm2.weight\n",
      "blocks.0.20.norm2.bias\n",
      "blocks.0.20.mlp.fc1.weight\n",
      "blocks.0.20.mlp.fc1.bias\n",
      "blocks.0.20.mlp.fc2.weight\n",
      "blocks.0.20.mlp.fc2.bias\n",
      "blocks.0.21.norm1.weight\n",
      "blocks.0.21.norm1.bias\n",
      "blocks.0.21.attn.qkv.weight\n",
      "blocks.0.21.attn.qkv.bias\n",
      "blocks.0.21.attn.proj.weight\n",
      "blocks.0.21.attn.proj.bias\n",
      "blocks.0.21.norm2.weight\n",
      "blocks.0.21.norm2.bias\n",
      "blocks.0.21.mlp.fc1.weight\n",
      "blocks.0.21.mlp.fc1.bias\n",
      "blocks.0.21.mlp.fc2.weight\n",
      "blocks.0.21.mlp.fc2.bias\n",
      "blocks.0.22.norm1.weight\n",
      "blocks.0.22.norm1.bias\n",
      "blocks.0.22.attn.qkv.weight\n",
      "blocks.0.22.attn.qkv.bias\n",
      "blocks.0.22.attn.proj.weight\n",
      "blocks.0.22.attn.proj.bias\n",
      "blocks.0.22.norm2.weight\n",
      "blocks.0.22.norm2.bias\n",
      "blocks.0.22.mlp.fc1.weight\n",
      "blocks.0.22.mlp.fc1.bias\n",
      "blocks.0.22.mlp.fc2.weight\n",
      "blocks.0.22.mlp.fc2.bias\n",
      "blocks.0.23.norm1.weight\n",
      "blocks.0.23.norm1.bias\n",
      "blocks.0.23.attn.qkv.weight\n",
      "blocks.0.23.attn.qkv.bias\n",
      "blocks.0.23.attn.proj.weight\n",
      "blocks.0.23.attn.proj.bias\n",
      "blocks.0.23.norm2.weight\n",
      "blocks.0.23.norm2.bias\n",
      "blocks.0.23.mlp.fc1.weight\n",
      "blocks.0.23.mlp.fc1.bias\n",
      "blocks.0.23.mlp.fc2.weight\n",
      "blocks.0.23.mlp.fc2.bias\n",
      "blocks.0.24.norm1.weight\n",
      "blocks.0.24.norm1.bias\n",
      "blocks.0.24.attn.qkv.weight\n",
      "blocks.0.24.attn.qkv.bias\n",
      "blocks.0.24.attn.proj.weight\n",
      "blocks.0.24.attn.proj.bias\n",
      "blocks.0.24.norm2.weight\n",
      "blocks.0.24.norm2.bias\n",
      "blocks.0.24.mlp.fc1.weight\n",
      "blocks.0.24.mlp.fc1.bias\n",
      "blocks.0.24.mlp.fc2.weight\n",
      "blocks.0.24.mlp.fc2.bias\n",
      "blocks.0.25.norm1.weight\n",
      "blocks.0.25.norm1.bias\n",
      "blocks.0.25.attn.qkv.weight\n",
      "blocks.0.25.attn.qkv.bias\n",
      "blocks.0.25.attn.proj.weight\n",
      "blocks.0.25.attn.proj.bias\n",
      "blocks.0.25.norm2.weight\n",
      "blocks.0.25.norm2.bias\n",
      "blocks.0.25.mlp.fc1.weight\n",
      "blocks.0.25.mlp.fc1.bias\n",
      "blocks.0.25.mlp.fc2.weight\n",
      "blocks.0.25.mlp.fc2.bias\n",
      "blocks.0.26.norm1.weight\n",
      "blocks.0.26.norm1.bias\n",
      "blocks.0.26.attn.qkv.weight\n",
      "blocks.0.26.attn.qkv.bias\n",
      "blocks.0.26.attn.proj.weight\n",
      "blocks.0.26.attn.proj.bias\n",
      "blocks.0.26.norm2.weight\n",
      "blocks.0.26.norm2.bias\n",
      "blocks.0.26.mlp.fc1.weight\n",
      "blocks.0.26.mlp.fc1.bias\n",
      "blocks.0.26.mlp.fc2.weight\n",
      "blocks.0.26.mlp.fc2.bias\n",
      "blocks.0.27.norm1.weight\n",
      "blocks.0.27.norm1.bias\n",
      "blocks.0.27.attn.qkv.weight\n",
      "blocks.0.27.attn.qkv.bias\n",
      "blocks.0.27.attn.proj.weight\n",
      "blocks.0.27.attn.proj.bias\n",
      "blocks.0.27.norm2.weight\n",
      "blocks.0.27.norm2.bias\n",
      "blocks.0.27.mlp.fc1.weight\n",
      "blocks.0.27.mlp.fc1.bias\n",
      "blocks.0.27.mlp.fc2.weight\n",
      "blocks.0.27.mlp.fc2.bias\n",
      "blocks.0.28.norm1.weight\n",
      "blocks.0.28.norm1.bias\n",
      "blocks.0.28.attn.qkv.weight\n",
      "blocks.0.28.attn.qkv.bias\n",
      "blocks.0.28.attn.proj.weight\n",
      "blocks.0.28.attn.proj.bias\n",
      "blocks.0.28.norm2.weight\n",
      "blocks.0.28.norm2.bias\n",
      "blocks.0.28.mlp.fc1.weight\n",
      "blocks.0.28.mlp.fc1.bias\n",
      "blocks.0.28.mlp.fc2.weight\n",
      "blocks.0.28.mlp.fc2.bias\n",
      "blocks.0.29.norm1.weight\n",
      "blocks.0.29.norm1.bias\n",
      "blocks.0.29.attn.qkv.weight\n",
      "blocks.0.29.attn.qkv.bias\n",
      "blocks.0.29.attn.proj.weight\n",
      "blocks.0.29.attn.proj.bias\n",
      "blocks.0.29.norm2.weight\n",
      "blocks.0.29.norm2.bias\n",
      "blocks.0.29.mlp.fc1.weight\n",
      "blocks.0.29.mlp.fc1.bias\n",
      "blocks.0.29.mlp.fc2.weight\n",
      "blocks.0.29.mlp.fc2.bias\n",
      "blocks.0.30.norm1.weight\n",
      "blocks.0.30.norm1.bias\n",
      "blocks.0.30.attn.qkv.weight\n",
      "blocks.0.30.attn.qkv.bias\n",
      "blocks.0.30.attn.proj.weight\n",
      "blocks.0.30.attn.proj.bias\n",
      "blocks.0.30.norm2.weight\n",
      "blocks.0.30.norm2.bias\n",
      "blocks.0.30.mlp.fc1.weight\n",
      "blocks.0.30.mlp.fc1.bias\n",
      "blocks.0.30.mlp.fc2.weight\n",
      "blocks.0.30.mlp.fc2.bias\n",
      "blocks.0.31.norm1.weight\n",
      "blocks.0.31.norm1.bias\n",
      "blocks.0.31.attn.qkv.weight\n",
      "blocks.0.31.attn.qkv.bias\n",
      "blocks.0.31.attn.proj.weight\n",
      "blocks.0.31.attn.proj.bias\n",
      "blocks.0.31.norm2.weight\n",
      "blocks.0.31.norm2.bias\n",
      "blocks.0.31.mlp.fc1.weight\n",
      "blocks.0.31.mlp.fc1.bias\n",
      "blocks.0.31.mlp.fc2.weight\n",
      "blocks.0.31.mlp.fc2.bias\n",
      "norm.weight\n",
      "norm.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model=torch.load('vinchow2.bin',map_location=torch.device('cpu'))\n",
    "#def ViT-H with 4 registers\n",
    "vit_h =  DinoVisionTransformer(embed_dim=1280,depth=32,num_heads=16,num_register_tokens=4,patch_size=14)\n",
    "for key in vit_h.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key reg_token not found in the model's state_dict\n"
     ]
    }
   ],
   "source": [
    "model =vit_h\n",
    "import torch\n",
    "\n",
    "# Load the state_dict from the saved file\n",
    "state_dict = torch.load('vinchow2.bin',map_location=torch.device('cpu'))\n",
    "\n",
    "# Create a copy of the state_dict with updated key names\n",
    "updated_state_dict = {}\n",
    "\n",
    "for key in state_dict.keys():\n",
    "    if 'blocks' in key:\n",
    "        # Rename keys from 'blocks' to 'blocks.0'\n",
    "        new_key = key.replace('blocks', 'blocks.0')\n",
    "        if new_key in model.state_dict().keys():\n",
    "            updated_state_dict[new_key] = state_dict[key]\n",
    "    else:\n",
    "        if key in model.state_dict().keys():\n",
    "            updated_state_dict[key] = state_dict[key]\n",
    "        else:\n",
    "            print(f\"Key {key} not found in the model's state_dict\")\n",
    "\n",
    "\n",
    "# Save the updated state_dict back to a file\n",
    "torch.save(updated_state_dict, 'Vinchow_updated.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_vinchow = torch.load('Vinchow_updated.bin',map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token torch.Size([1, 1, 1280])\n",
      "pos_embed torch.Size([1, 261, 1280])\n",
      "patch_embed.proj.weight torch.Size([1280, 3, 14, 14])\n",
      "patch_embed.proj.bias torch.Size([1280])\n",
      "blocks.0.0.norm1.weight torch.Size([1280])\n",
      "blocks.0.0.norm1.bias torch.Size([1280])\n",
      "blocks.0.0.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.0.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.0.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.0.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.0.norm2.weight torch.Size([1280])\n",
      "blocks.0.0.norm2.bias torch.Size([1280])\n",
      "blocks.0.0.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.0.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.0.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.0.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.1.norm1.weight torch.Size([1280])\n",
      "blocks.0.1.norm1.bias torch.Size([1280])\n",
      "blocks.0.1.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.1.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.1.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.1.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.1.norm2.weight torch.Size([1280])\n",
      "blocks.0.1.norm2.bias torch.Size([1280])\n",
      "blocks.0.1.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.1.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.1.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.1.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.2.norm1.weight torch.Size([1280])\n",
      "blocks.0.2.norm1.bias torch.Size([1280])\n",
      "blocks.0.2.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.2.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.2.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.2.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.2.norm2.weight torch.Size([1280])\n",
      "blocks.0.2.norm2.bias torch.Size([1280])\n",
      "blocks.0.2.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.2.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.2.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.2.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.3.norm1.weight torch.Size([1280])\n",
      "blocks.0.3.norm1.bias torch.Size([1280])\n",
      "blocks.0.3.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.3.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.3.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.3.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.3.norm2.weight torch.Size([1280])\n",
      "blocks.0.3.norm2.bias torch.Size([1280])\n",
      "blocks.0.3.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.3.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.3.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.3.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.4.norm1.weight torch.Size([1280])\n",
      "blocks.0.4.norm1.bias torch.Size([1280])\n",
      "blocks.0.4.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.4.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.4.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.4.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.4.norm2.weight torch.Size([1280])\n",
      "blocks.0.4.norm2.bias torch.Size([1280])\n",
      "blocks.0.4.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.4.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.4.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.4.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.5.norm1.weight torch.Size([1280])\n",
      "blocks.0.5.norm1.bias torch.Size([1280])\n",
      "blocks.0.5.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.5.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.5.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.5.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.5.norm2.weight torch.Size([1280])\n",
      "blocks.0.5.norm2.bias torch.Size([1280])\n",
      "blocks.0.5.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.5.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.5.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.5.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.6.norm1.weight torch.Size([1280])\n",
      "blocks.0.6.norm1.bias torch.Size([1280])\n",
      "blocks.0.6.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.6.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.6.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.6.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.6.norm2.weight torch.Size([1280])\n",
      "blocks.0.6.norm2.bias torch.Size([1280])\n",
      "blocks.0.6.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.6.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.6.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.6.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.7.norm1.weight torch.Size([1280])\n",
      "blocks.0.7.norm1.bias torch.Size([1280])\n",
      "blocks.0.7.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.7.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.7.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.7.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.7.norm2.weight torch.Size([1280])\n",
      "blocks.0.7.norm2.bias torch.Size([1280])\n",
      "blocks.0.7.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.7.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.7.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.7.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.8.norm1.weight torch.Size([1280])\n",
      "blocks.0.8.norm1.bias torch.Size([1280])\n",
      "blocks.0.8.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.8.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.8.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.8.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.8.norm2.weight torch.Size([1280])\n",
      "blocks.0.8.norm2.bias torch.Size([1280])\n",
      "blocks.0.8.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.8.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.8.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.8.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.9.norm1.weight torch.Size([1280])\n",
      "blocks.0.9.norm1.bias torch.Size([1280])\n",
      "blocks.0.9.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.9.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.9.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.9.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.9.norm2.weight torch.Size([1280])\n",
      "blocks.0.9.norm2.bias torch.Size([1280])\n",
      "blocks.0.9.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.9.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.9.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.9.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.10.norm1.weight torch.Size([1280])\n",
      "blocks.0.10.norm1.bias torch.Size([1280])\n",
      "blocks.0.10.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.10.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.10.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.10.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.10.norm2.weight torch.Size([1280])\n",
      "blocks.0.10.norm2.bias torch.Size([1280])\n",
      "blocks.0.10.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.10.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.10.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.10.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.11.norm1.weight torch.Size([1280])\n",
      "blocks.0.11.norm1.bias torch.Size([1280])\n",
      "blocks.0.11.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.11.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.11.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.11.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.11.norm2.weight torch.Size([1280])\n",
      "blocks.0.11.norm2.bias torch.Size([1280])\n",
      "blocks.0.11.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.11.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.11.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.11.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.12.norm1.weight torch.Size([1280])\n",
      "blocks.0.12.norm1.bias torch.Size([1280])\n",
      "blocks.0.12.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.12.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.12.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.12.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.12.norm2.weight torch.Size([1280])\n",
      "blocks.0.12.norm2.bias torch.Size([1280])\n",
      "blocks.0.12.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.12.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.12.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.12.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.13.norm1.weight torch.Size([1280])\n",
      "blocks.0.13.norm1.bias torch.Size([1280])\n",
      "blocks.0.13.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.13.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.13.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.13.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.13.norm2.weight torch.Size([1280])\n",
      "blocks.0.13.norm2.bias torch.Size([1280])\n",
      "blocks.0.13.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.13.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.13.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.13.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.14.norm1.weight torch.Size([1280])\n",
      "blocks.0.14.norm1.bias torch.Size([1280])\n",
      "blocks.0.14.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.14.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.14.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.14.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.14.norm2.weight torch.Size([1280])\n",
      "blocks.0.14.norm2.bias torch.Size([1280])\n",
      "blocks.0.14.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.14.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.14.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.14.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.15.norm1.weight torch.Size([1280])\n",
      "blocks.0.15.norm1.bias torch.Size([1280])\n",
      "blocks.0.15.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.15.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.15.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.15.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.15.norm2.weight torch.Size([1280])\n",
      "blocks.0.15.norm2.bias torch.Size([1280])\n",
      "blocks.0.15.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.15.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.15.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.15.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.16.norm1.weight torch.Size([1280])\n",
      "blocks.0.16.norm1.bias torch.Size([1280])\n",
      "blocks.0.16.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.16.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.16.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.16.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.16.norm2.weight torch.Size([1280])\n",
      "blocks.0.16.norm2.bias torch.Size([1280])\n",
      "blocks.0.16.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.16.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.16.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.16.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.17.norm1.weight torch.Size([1280])\n",
      "blocks.0.17.norm1.bias torch.Size([1280])\n",
      "blocks.0.17.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.17.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.17.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.17.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.17.norm2.weight torch.Size([1280])\n",
      "blocks.0.17.norm2.bias torch.Size([1280])\n",
      "blocks.0.17.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.17.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.17.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.17.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.18.norm1.weight torch.Size([1280])\n",
      "blocks.0.18.norm1.bias torch.Size([1280])\n",
      "blocks.0.18.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.18.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.18.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.18.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.18.norm2.weight torch.Size([1280])\n",
      "blocks.0.18.norm2.bias torch.Size([1280])\n",
      "blocks.0.18.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.18.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.18.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.18.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.19.norm1.weight torch.Size([1280])\n",
      "blocks.0.19.norm1.bias torch.Size([1280])\n",
      "blocks.0.19.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.19.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.19.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.19.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.19.norm2.weight torch.Size([1280])\n",
      "blocks.0.19.norm2.bias torch.Size([1280])\n",
      "blocks.0.19.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.19.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.19.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.19.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.20.norm1.weight torch.Size([1280])\n",
      "blocks.0.20.norm1.bias torch.Size([1280])\n",
      "blocks.0.20.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.20.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.20.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.20.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.20.norm2.weight torch.Size([1280])\n",
      "blocks.0.20.norm2.bias torch.Size([1280])\n",
      "blocks.0.20.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.20.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.20.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.20.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.21.norm1.weight torch.Size([1280])\n",
      "blocks.0.21.norm1.bias torch.Size([1280])\n",
      "blocks.0.21.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.21.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.21.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.21.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.21.norm2.weight torch.Size([1280])\n",
      "blocks.0.21.norm2.bias torch.Size([1280])\n",
      "blocks.0.21.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.21.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.21.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.21.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.22.norm1.weight torch.Size([1280])\n",
      "blocks.0.22.norm1.bias torch.Size([1280])\n",
      "blocks.0.22.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.22.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.22.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.22.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.22.norm2.weight torch.Size([1280])\n",
      "blocks.0.22.norm2.bias torch.Size([1280])\n",
      "blocks.0.22.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.22.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.22.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.22.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.23.norm1.weight torch.Size([1280])\n",
      "blocks.0.23.norm1.bias torch.Size([1280])\n",
      "blocks.0.23.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.23.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.23.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.23.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.23.norm2.weight torch.Size([1280])\n",
      "blocks.0.23.norm2.bias torch.Size([1280])\n",
      "blocks.0.23.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.23.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.23.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.23.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.24.norm1.weight torch.Size([1280])\n",
      "blocks.0.24.norm1.bias torch.Size([1280])\n",
      "blocks.0.24.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.24.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.24.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.24.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.24.norm2.weight torch.Size([1280])\n",
      "blocks.0.24.norm2.bias torch.Size([1280])\n",
      "blocks.0.24.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.24.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.24.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.24.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.25.norm1.weight torch.Size([1280])\n",
      "blocks.0.25.norm1.bias torch.Size([1280])\n",
      "blocks.0.25.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.25.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.25.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.25.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.25.norm2.weight torch.Size([1280])\n",
      "blocks.0.25.norm2.bias torch.Size([1280])\n",
      "blocks.0.25.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.25.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.25.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.25.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.26.norm1.weight torch.Size([1280])\n",
      "blocks.0.26.norm1.bias torch.Size([1280])\n",
      "blocks.0.26.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.26.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.26.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.26.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.26.norm2.weight torch.Size([1280])\n",
      "blocks.0.26.norm2.bias torch.Size([1280])\n",
      "blocks.0.26.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.26.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.26.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.26.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.27.norm1.weight torch.Size([1280])\n",
      "blocks.0.27.norm1.bias torch.Size([1280])\n",
      "blocks.0.27.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.27.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.27.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.27.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.27.norm2.weight torch.Size([1280])\n",
      "blocks.0.27.norm2.bias torch.Size([1280])\n",
      "blocks.0.27.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.27.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.27.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.27.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.28.norm1.weight torch.Size([1280])\n",
      "blocks.0.28.norm1.bias torch.Size([1280])\n",
      "blocks.0.28.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.28.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.28.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.28.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.28.norm2.weight torch.Size([1280])\n",
      "blocks.0.28.norm2.bias torch.Size([1280])\n",
      "blocks.0.28.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.28.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.28.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.28.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.29.norm1.weight torch.Size([1280])\n",
      "blocks.0.29.norm1.bias torch.Size([1280])\n",
      "blocks.0.29.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.29.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.29.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.29.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.29.norm2.weight torch.Size([1280])\n",
      "blocks.0.29.norm2.bias torch.Size([1280])\n",
      "blocks.0.29.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.29.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.29.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.29.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.30.norm1.weight torch.Size([1280])\n",
      "blocks.0.30.norm1.bias torch.Size([1280])\n",
      "blocks.0.30.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.30.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.30.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.30.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.30.norm2.weight torch.Size([1280])\n",
      "blocks.0.30.norm2.bias torch.Size([1280])\n",
      "blocks.0.30.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.30.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.30.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.30.mlp.fc2.bias torch.Size([1280])\n",
      "blocks.0.31.norm1.weight torch.Size([1280])\n",
      "blocks.0.31.norm1.bias torch.Size([1280])\n",
      "blocks.0.31.attn.qkv.weight torch.Size([3840, 1280])\n",
      "blocks.0.31.attn.qkv.bias torch.Size([3840])\n",
      "blocks.0.31.attn.proj.weight torch.Size([1280, 1280])\n",
      "blocks.0.31.attn.proj.bias torch.Size([1280])\n",
      "blocks.0.31.norm2.weight torch.Size([1280])\n",
      "blocks.0.31.norm2.bias torch.Size([1280])\n",
      "blocks.0.31.mlp.fc1.weight torch.Size([6832, 1280])\n",
      "blocks.0.31.mlp.fc1.bias torch.Size([6832])\n",
      "blocks.0.31.mlp.fc2.weight torch.Size([1280, 3416])\n",
      "blocks.0.31.mlp.fc2.bias torch.Size([1280])\n",
      "norm.weight torch.Size([1280])\n",
      "norm.bias torch.Size([1280])\n"
     ]
    }
   ],
   "source": [
    "for key,value in load_vinchow.items():\n",
    "    print(key,value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DinoVisionTransformer:\n\tMissing key(s) in state_dict: \"register_tokens\", \"mask_token\". \n\tsize mismatch for pos_embed: copying a param with shape torch.Size([1, 261, 1280]) from checkpoint, the shape in current model is torch.Size([1, 257, 1280]).\n\tsize mismatch for blocks.0.0.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.0.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.0.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.1.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.1.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.1.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.2.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.2.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.2.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.3.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.3.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.3.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.4.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.4.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.4.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.5.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.5.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.5.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.6.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.6.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.6.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.7.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.7.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.7.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.8.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.8.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.8.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.9.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.9.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.9.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.10.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.10.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.10.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.11.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.11.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.11.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.12.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.12.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.12.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.13.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.13.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.13.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.14.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.14.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.14.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.15.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.15.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.15.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.16.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.16.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.16.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.17.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.17.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.17.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.18.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.18.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.18.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.19.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.19.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.19.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.20.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.20.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.20.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.21.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.21.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.21.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.22.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.22.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.22.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.23.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.23.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.23.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.24.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.24.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.24.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.25.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.25.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.25.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.26.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.26.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.26.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.27.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.27.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.27.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.28.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.28.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.28.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.29.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.29.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.29.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.30.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.30.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.30.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.31.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.31.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.31.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m load_vinchow\n\u001b[1;32m      2\u001b[0m vit_h \u001b[38;5;241m=\u001b[39m  DinoVisionTransformer(embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1280\u001b[39m,depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,num_register_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mvit_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_vinchow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DinoVisionTransformer:\n\tMissing key(s) in state_dict: \"register_tokens\", \"mask_token\". \n\tsize mismatch for pos_embed: copying a param with shape torch.Size([1, 261, 1280]) from checkpoint, the shape in current model is torch.Size([1, 257, 1280]).\n\tsize mismatch for blocks.0.0.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.0.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.0.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.1.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.1.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.1.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.2.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.2.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.2.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.3.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.3.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.3.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.4.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.4.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.4.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.5.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.5.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.5.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.6.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.6.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.6.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.7.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.7.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.7.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.8.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.8.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.8.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.9.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.9.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.9.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.10.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.10.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.10.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.11.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.11.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.11.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.12.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.12.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.12.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.13.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.13.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.13.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.14.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.14.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.14.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.15.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.15.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.15.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.16.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.16.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.16.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.17.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.17.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.17.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.18.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.18.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.18.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.19.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.19.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.19.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.20.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.20.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.20.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.21.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.21.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.21.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.22.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.22.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.22.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.23.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.23.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.23.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.24.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.24.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.24.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.25.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.25.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.25.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.26.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.26.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.26.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.27.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.27.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.27.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.28.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.28.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.28.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.29.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.29.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.29.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.30.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.30.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.30.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120]).\n\tsize mismatch for blocks.0.31.mlp.fc1.weight: copying a param with shape torch.Size([6832, 1280]) from checkpoint, the shape in current model is torch.Size([5120, 1280]).\n\tsize mismatch for blocks.0.31.mlp.fc1.bias: copying a param with shape torch.Size([6832]) from checkpoint, the shape in current model is torch.Size([5120]).\n\tsize mismatch for blocks.0.31.mlp.fc2.weight: copying a param with shape torch.Size([1280, 3416]) from checkpoint, the shape in current model is torch.Size([1280, 5120])."
     ]
    }
   ],
   "source": [
    "load_vinchow\n",
    "vit_h =  DinoVisionTransformer(embed_dim=1280,depth=32,num_heads=16,num_register_tokens=4,patch_size=14)\n",
    "model.load_state_dict(load_vinchow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "model = torch.load('UNI-1.5.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token torch.Size([1, 1, 1536])\n",
      "pos_embed torch.Size([1, 257, 1536])\n",
      "register_tokens torch.Size([1, 8, 1536])\n",
      "mask_token torch.Size([1, 1536])\n",
      "patch_embed.proj.weight torch.Size([1536, 3, 14, 14])\n",
      "patch_embed.proj.bias torch.Size([1536])\n",
      "blocks.0.0.norm1.weight torch.Size([1536])\n",
      "blocks.0.0.norm1.bias torch.Size([1536])\n",
      "blocks.0.0.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.0.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.0.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.0.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.0.ls1.gamma torch.Size([1536])\n",
      "blocks.0.0.norm2.weight torch.Size([1536])\n",
      "blocks.0.0.norm2.bias torch.Size([1536])\n",
      "blocks.0.0.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.0.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.0.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.0.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.0.ls2.gamma torch.Size([1536])\n",
      "blocks.0.1.norm1.weight torch.Size([1536])\n",
      "blocks.0.1.norm1.bias torch.Size([1536])\n",
      "blocks.0.1.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.1.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.1.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.1.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.1.ls1.gamma torch.Size([1536])\n",
      "blocks.0.1.norm2.weight torch.Size([1536])\n",
      "blocks.0.1.norm2.bias torch.Size([1536])\n",
      "blocks.0.1.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.1.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.1.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.1.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.1.ls2.gamma torch.Size([1536])\n",
      "blocks.0.2.norm1.weight torch.Size([1536])\n",
      "blocks.0.2.norm1.bias torch.Size([1536])\n",
      "blocks.0.2.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.2.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.2.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.2.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.2.ls1.gamma torch.Size([1536])\n",
      "blocks.0.2.norm2.weight torch.Size([1536])\n",
      "blocks.0.2.norm2.bias torch.Size([1536])\n",
      "blocks.0.2.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.2.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.2.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.2.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.2.ls2.gamma torch.Size([1536])\n",
      "blocks.0.3.norm1.weight torch.Size([1536])\n",
      "blocks.0.3.norm1.bias torch.Size([1536])\n",
      "blocks.0.3.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.3.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.3.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.3.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.3.ls1.gamma torch.Size([1536])\n",
      "blocks.0.3.norm2.weight torch.Size([1536])\n",
      "blocks.0.3.norm2.bias torch.Size([1536])\n",
      "blocks.0.3.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.3.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.3.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.3.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.3.ls2.gamma torch.Size([1536])\n",
      "blocks.0.4.norm1.weight torch.Size([1536])\n",
      "blocks.0.4.norm1.bias torch.Size([1536])\n",
      "blocks.0.4.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.4.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.4.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.4.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.4.ls1.gamma torch.Size([1536])\n",
      "blocks.0.4.norm2.weight torch.Size([1536])\n",
      "blocks.0.4.norm2.bias torch.Size([1536])\n",
      "blocks.0.4.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.4.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.4.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.4.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.4.ls2.gamma torch.Size([1536])\n",
      "blocks.0.5.norm1.weight torch.Size([1536])\n",
      "blocks.0.5.norm1.bias torch.Size([1536])\n",
      "blocks.0.5.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.5.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.5.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.5.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.5.ls1.gamma torch.Size([1536])\n",
      "blocks.0.5.norm2.weight torch.Size([1536])\n",
      "blocks.0.5.norm2.bias torch.Size([1536])\n",
      "blocks.0.5.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.5.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.5.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.5.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.5.ls2.gamma torch.Size([1536])\n",
      "blocks.0.6.norm1.weight torch.Size([1536])\n",
      "blocks.0.6.norm1.bias torch.Size([1536])\n",
      "blocks.0.6.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.6.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.6.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.6.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.6.ls1.gamma torch.Size([1536])\n",
      "blocks.0.6.norm2.weight torch.Size([1536])\n",
      "blocks.0.6.norm2.bias torch.Size([1536])\n",
      "blocks.0.6.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.6.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.6.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.6.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.6.ls2.gamma torch.Size([1536])\n",
      "blocks.0.7.norm1.weight torch.Size([1536])\n",
      "blocks.0.7.norm1.bias torch.Size([1536])\n",
      "blocks.0.7.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.7.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.7.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.7.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.7.ls1.gamma torch.Size([1536])\n",
      "blocks.0.7.norm2.weight torch.Size([1536])\n",
      "blocks.0.7.norm2.bias torch.Size([1536])\n",
      "blocks.0.7.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.7.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.7.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.7.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.7.ls2.gamma torch.Size([1536])\n",
      "blocks.0.8.norm1.weight torch.Size([1536])\n",
      "blocks.0.8.norm1.bias torch.Size([1536])\n",
      "blocks.0.8.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.8.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.8.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.8.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.8.ls1.gamma torch.Size([1536])\n",
      "blocks.0.8.norm2.weight torch.Size([1536])\n",
      "blocks.0.8.norm2.bias torch.Size([1536])\n",
      "blocks.0.8.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.8.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.8.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.8.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.8.ls2.gamma torch.Size([1536])\n",
      "blocks.0.9.norm1.weight torch.Size([1536])\n",
      "blocks.0.9.norm1.bias torch.Size([1536])\n",
      "blocks.0.9.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.9.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.9.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.9.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.9.ls1.gamma torch.Size([1536])\n",
      "blocks.0.9.norm2.weight torch.Size([1536])\n",
      "blocks.0.9.norm2.bias torch.Size([1536])\n",
      "blocks.0.9.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.9.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.9.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.9.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.9.ls2.gamma torch.Size([1536])\n",
      "blocks.0.10.norm1.weight torch.Size([1536])\n",
      "blocks.0.10.norm1.bias torch.Size([1536])\n",
      "blocks.0.10.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.10.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.10.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.10.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.10.ls1.gamma torch.Size([1536])\n",
      "blocks.0.10.norm2.weight torch.Size([1536])\n",
      "blocks.0.10.norm2.bias torch.Size([1536])\n",
      "blocks.0.10.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.10.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.10.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.10.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.10.ls2.gamma torch.Size([1536])\n",
      "blocks.0.11.norm1.weight torch.Size([1536])\n",
      "blocks.0.11.norm1.bias torch.Size([1536])\n",
      "blocks.0.11.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.11.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.11.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.11.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.11.ls1.gamma torch.Size([1536])\n",
      "blocks.0.11.norm2.weight torch.Size([1536])\n",
      "blocks.0.11.norm2.bias torch.Size([1536])\n",
      "blocks.0.11.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.11.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.11.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.11.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.11.ls2.gamma torch.Size([1536])\n",
      "blocks.0.12.norm1.weight torch.Size([1536])\n",
      "blocks.0.12.norm1.bias torch.Size([1536])\n",
      "blocks.0.12.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.12.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.12.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.12.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.12.ls1.gamma torch.Size([1536])\n",
      "blocks.0.12.norm2.weight torch.Size([1536])\n",
      "blocks.0.12.norm2.bias torch.Size([1536])\n",
      "blocks.0.12.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.12.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.12.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.12.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.12.ls2.gamma torch.Size([1536])\n",
      "blocks.0.13.norm1.weight torch.Size([1536])\n",
      "blocks.0.13.norm1.bias torch.Size([1536])\n",
      "blocks.0.13.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.13.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.13.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.13.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.13.ls1.gamma torch.Size([1536])\n",
      "blocks.0.13.norm2.weight torch.Size([1536])\n",
      "blocks.0.13.norm2.bias torch.Size([1536])\n",
      "blocks.0.13.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.13.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.13.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.13.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.13.ls2.gamma torch.Size([1536])\n",
      "blocks.0.14.norm1.weight torch.Size([1536])\n",
      "blocks.0.14.norm1.bias torch.Size([1536])\n",
      "blocks.0.14.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.14.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.14.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.14.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.14.ls1.gamma torch.Size([1536])\n",
      "blocks.0.14.norm2.weight torch.Size([1536])\n",
      "blocks.0.14.norm2.bias torch.Size([1536])\n",
      "blocks.0.14.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.14.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.14.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.14.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.14.ls2.gamma torch.Size([1536])\n",
      "blocks.0.15.norm1.weight torch.Size([1536])\n",
      "blocks.0.15.norm1.bias torch.Size([1536])\n",
      "blocks.0.15.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.15.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.15.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.15.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.15.ls1.gamma torch.Size([1536])\n",
      "blocks.0.15.norm2.weight torch.Size([1536])\n",
      "blocks.0.15.norm2.bias torch.Size([1536])\n",
      "blocks.0.15.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.15.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.15.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.15.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.15.ls2.gamma torch.Size([1536])\n",
      "blocks.0.16.norm1.weight torch.Size([1536])\n",
      "blocks.0.16.norm1.bias torch.Size([1536])\n",
      "blocks.0.16.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.16.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.16.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.16.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.16.ls1.gamma torch.Size([1536])\n",
      "blocks.0.16.norm2.weight torch.Size([1536])\n",
      "blocks.0.16.norm2.bias torch.Size([1536])\n",
      "blocks.0.16.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.16.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.16.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.16.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.16.ls2.gamma torch.Size([1536])\n",
      "blocks.0.17.norm1.weight torch.Size([1536])\n",
      "blocks.0.17.norm1.bias torch.Size([1536])\n",
      "blocks.0.17.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.17.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.17.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.17.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.17.ls1.gamma torch.Size([1536])\n",
      "blocks.0.17.norm2.weight torch.Size([1536])\n",
      "blocks.0.17.norm2.bias torch.Size([1536])\n",
      "blocks.0.17.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.17.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.17.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.17.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.17.ls2.gamma torch.Size([1536])\n",
      "blocks.0.18.norm1.weight torch.Size([1536])\n",
      "blocks.0.18.norm1.bias torch.Size([1536])\n",
      "blocks.0.18.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.18.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.18.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.18.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.18.ls1.gamma torch.Size([1536])\n",
      "blocks.0.18.norm2.weight torch.Size([1536])\n",
      "blocks.0.18.norm2.bias torch.Size([1536])\n",
      "blocks.0.18.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.18.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.18.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.18.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.18.ls2.gamma torch.Size([1536])\n",
      "blocks.0.19.norm1.weight torch.Size([1536])\n",
      "blocks.0.19.norm1.bias torch.Size([1536])\n",
      "blocks.0.19.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.19.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.19.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.19.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.19.ls1.gamma torch.Size([1536])\n",
      "blocks.0.19.norm2.weight torch.Size([1536])\n",
      "blocks.0.19.norm2.bias torch.Size([1536])\n",
      "blocks.0.19.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.19.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.19.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.19.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.19.ls2.gamma torch.Size([1536])\n",
      "blocks.0.20.norm1.weight torch.Size([1536])\n",
      "blocks.0.20.norm1.bias torch.Size([1536])\n",
      "blocks.0.20.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.20.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.20.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.20.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.20.ls1.gamma torch.Size([1536])\n",
      "blocks.0.20.norm2.weight torch.Size([1536])\n",
      "blocks.0.20.norm2.bias torch.Size([1536])\n",
      "blocks.0.20.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.20.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.20.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.20.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.20.ls2.gamma torch.Size([1536])\n",
      "blocks.0.21.norm1.weight torch.Size([1536])\n",
      "blocks.0.21.norm1.bias torch.Size([1536])\n",
      "blocks.0.21.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.21.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.21.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.21.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.21.ls1.gamma torch.Size([1536])\n",
      "blocks.0.21.norm2.weight torch.Size([1536])\n",
      "blocks.0.21.norm2.bias torch.Size([1536])\n",
      "blocks.0.21.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.21.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.21.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.21.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.21.ls2.gamma torch.Size([1536])\n",
      "blocks.0.22.norm1.weight torch.Size([1536])\n",
      "blocks.0.22.norm1.bias torch.Size([1536])\n",
      "blocks.0.22.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.22.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.22.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.22.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.22.ls1.gamma torch.Size([1536])\n",
      "blocks.0.22.norm2.weight torch.Size([1536])\n",
      "blocks.0.22.norm2.bias torch.Size([1536])\n",
      "blocks.0.22.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.22.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.22.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.22.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.22.ls2.gamma torch.Size([1536])\n",
      "blocks.0.23.norm1.weight torch.Size([1536])\n",
      "blocks.0.23.norm1.bias torch.Size([1536])\n",
      "blocks.0.23.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.23.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.23.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.23.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.23.ls1.gamma torch.Size([1536])\n",
      "blocks.0.23.norm2.weight torch.Size([1536])\n",
      "blocks.0.23.norm2.bias torch.Size([1536])\n",
      "blocks.0.23.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.23.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.23.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.23.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.23.ls2.gamma torch.Size([1536])\n",
      "norm.weight torch.Size([1536])\n",
      "norm.bias torch.Size([1536])\n"
     ]
    }
   ],
   "source": [
    "with open('self.txt','w') as f:\n",
    "    for key in vit_h.state_dict().keys():\n",
    "        print(key,vit_h.state_dict()[key].shape)\n",
    "        f.write(key+str(vit_h.state_dict()[key].shape)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_h =  DinoVisionTransformer(**{\n",
    "            'img_size': 224, \n",
    "            'patch_size': 14, \n",
    "            'depth': 24,\n",
    "            'embed_dim': 1536,\n",
    "            'num_heads': 24,\n",
    "            'init_values': 1e-5, \n",
    "            'embed_dim': 1536,\n",
    "            'mlp_ratio': 2.66667*2,\n",
    "            'ffn_layer' :\"SwiGLUPacked\",\n",
    "            'no_embed_class': True,   \n",
    "            'act_layer': torch.nn.SiLU, \n",
    "            'num_register_tokens': 8 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_token torch.Size([1, 8, 1536])\n",
      "cls_token torch.Size([1, 1, 1536])\n",
      "pos_embed torch.Size([1, 256, 1536])\n",
      "patch_embed.proj.weight torch.Size([1536, 3, 14, 14])\n",
      "patch_embed.proj.bias torch.Size([1536])\n",
      "blocks.0.norm1.weight torch.Size([1536])\n",
      "blocks.0.norm1.bias torch.Size([1536])\n",
      "blocks.0.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.ls1.gamma torch.Size([1536])\n",
      "blocks.0.norm2.weight torch.Size([1536])\n",
      "blocks.0.norm2.bias torch.Size([1536])\n",
      "blocks.0.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.ls2.gamma torch.Size([1536])\n",
      "blocks.1.norm1.weight torch.Size([1536])\n",
      "blocks.1.norm1.bias torch.Size([1536])\n",
      "blocks.1.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.1.attn.qkv.bias torch.Size([4608])\n",
      "blocks.1.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.1.attn.proj.bias torch.Size([1536])\n",
      "blocks.1.ls1.gamma torch.Size([1536])\n",
      "blocks.1.norm2.weight torch.Size([1536])\n",
      "blocks.1.norm2.bias torch.Size([1536])\n",
      "blocks.1.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.1.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.1.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.1.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.1.ls2.gamma torch.Size([1536])\n",
      "blocks.2.norm1.weight torch.Size([1536])\n",
      "blocks.2.norm1.bias torch.Size([1536])\n",
      "blocks.2.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.2.attn.qkv.bias torch.Size([4608])\n",
      "blocks.2.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.2.attn.proj.bias torch.Size([1536])\n",
      "blocks.2.ls1.gamma torch.Size([1536])\n",
      "blocks.2.norm2.weight torch.Size([1536])\n",
      "blocks.2.norm2.bias torch.Size([1536])\n",
      "blocks.2.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.2.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.2.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.2.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.2.ls2.gamma torch.Size([1536])\n",
      "blocks.3.norm1.weight torch.Size([1536])\n",
      "blocks.3.norm1.bias torch.Size([1536])\n",
      "blocks.3.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.3.attn.qkv.bias torch.Size([4608])\n",
      "blocks.3.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.3.attn.proj.bias torch.Size([1536])\n",
      "blocks.3.ls1.gamma torch.Size([1536])\n",
      "blocks.3.norm2.weight torch.Size([1536])\n",
      "blocks.3.norm2.bias torch.Size([1536])\n",
      "blocks.3.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.3.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.3.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.3.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.3.ls2.gamma torch.Size([1536])\n",
      "blocks.4.norm1.weight torch.Size([1536])\n",
      "blocks.4.norm1.bias torch.Size([1536])\n",
      "blocks.4.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.4.attn.qkv.bias torch.Size([4608])\n",
      "blocks.4.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.4.attn.proj.bias torch.Size([1536])\n",
      "blocks.4.ls1.gamma torch.Size([1536])\n",
      "blocks.4.norm2.weight torch.Size([1536])\n",
      "blocks.4.norm2.bias torch.Size([1536])\n",
      "blocks.4.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.4.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.4.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.4.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.4.ls2.gamma torch.Size([1536])\n",
      "blocks.5.norm1.weight torch.Size([1536])\n",
      "blocks.5.norm1.bias torch.Size([1536])\n",
      "blocks.5.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.5.attn.qkv.bias torch.Size([4608])\n",
      "blocks.5.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.5.attn.proj.bias torch.Size([1536])\n",
      "blocks.5.ls1.gamma torch.Size([1536])\n",
      "blocks.5.norm2.weight torch.Size([1536])\n",
      "blocks.5.norm2.bias torch.Size([1536])\n",
      "blocks.5.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.5.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.5.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.5.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.5.ls2.gamma torch.Size([1536])\n",
      "blocks.6.norm1.weight torch.Size([1536])\n",
      "blocks.6.norm1.bias torch.Size([1536])\n",
      "blocks.6.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.6.attn.qkv.bias torch.Size([4608])\n",
      "blocks.6.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.6.attn.proj.bias torch.Size([1536])\n",
      "blocks.6.ls1.gamma torch.Size([1536])\n",
      "blocks.6.norm2.weight torch.Size([1536])\n",
      "blocks.6.norm2.bias torch.Size([1536])\n",
      "blocks.6.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.6.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.6.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.6.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.6.ls2.gamma torch.Size([1536])\n",
      "blocks.7.norm1.weight torch.Size([1536])\n",
      "blocks.7.norm1.bias torch.Size([1536])\n",
      "blocks.7.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.7.attn.qkv.bias torch.Size([4608])\n",
      "blocks.7.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.7.attn.proj.bias torch.Size([1536])\n",
      "blocks.7.ls1.gamma torch.Size([1536])\n",
      "blocks.7.norm2.weight torch.Size([1536])\n",
      "blocks.7.norm2.bias torch.Size([1536])\n",
      "blocks.7.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.7.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.7.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.7.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.7.ls2.gamma torch.Size([1536])\n",
      "blocks.8.norm1.weight torch.Size([1536])\n",
      "blocks.8.norm1.bias torch.Size([1536])\n",
      "blocks.8.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.8.attn.qkv.bias torch.Size([4608])\n",
      "blocks.8.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.8.attn.proj.bias torch.Size([1536])\n",
      "blocks.8.ls1.gamma torch.Size([1536])\n",
      "blocks.8.norm2.weight torch.Size([1536])\n",
      "blocks.8.norm2.bias torch.Size([1536])\n",
      "blocks.8.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.8.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.8.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.8.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.8.ls2.gamma torch.Size([1536])\n",
      "blocks.9.norm1.weight torch.Size([1536])\n",
      "blocks.9.norm1.bias torch.Size([1536])\n",
      "blocks.9.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.9.attn.qkv.bias torch.Size([4608])\n",
      "blocks.9.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.9.attn.proj.bias torch.Size([1536])\n",
      "blocks.9.ls1.gamma torch.Size([1536])\n",
      "blocks.9.norm2.weight torch.Size([1536])\n",
      "blocks.9.norm2.bias torch.Size([1536])\n",
      "blocks.9.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.9.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.9.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.9.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.9.ls2.gamma torch.Size([1536])\n",
      "blocks.10.norm1.weight torch.Size([1536])\n",
      "blocks.10.norm1.bias torch.Size([1536])\n",
      "blocks.10.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.10.attn.qkv.bias torch.Size([4608])\n",
      "blocks.10.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.10.attn.proj.bias torch.Size([1536])\n",
      "blocks.10.ls1.gamma torch.Size([1536])\n",
      "blocks.10.norm2.weight torch.Size([1536])\n",
      "blocks.10.norm2.bias torch.Size([1536])\n",
      "blocks.10.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.10.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.10.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.10.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.10.ls2.gamma torch.Size([1536])\n",
      "blocks.11.norm1.weight torch.Size([1536])\n",
      "blocks.11.norm1.bias torch.Size([1536])\n",
      "blocks.11.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.11.attn.qkv.bias torch.Size([4608])\n",
      "blocks.11.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.11.attn.proj.bias torch.Size([1536])\n",
      "blocks.11.ls1.gamma torch.Size([1536])\n",
      "blocks.11.norm2.weight torch.Size([1536])\n",
      "blocks.11.norm2.bias torch.Size([1536])\n",
      "blocks.11.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.11.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.11.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.11.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.11.ls2.gamma torch.Size([1536])\n",
      "blocks.12.norm1.weight torch.Size([1536])\n",
      "blocks.12.norm1.bias torch.Size([1536])\n",
      "blocks.12.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.12.attn.qkv.bias torch.Size([4608])\n",
      "blocks.12.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.12.attn.proj.bias torch.Size([1536])\n",
      "blocks.12.ls1.gamma torch.Size([1536])\n",
      "blocks.12.norm2.weight torch.Size([1536])\n",
      "blocks.12.norm2.bias torch.Size([1536])\n",
      "blocks.12.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.12.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.12.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.12.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.12.ls2.gamma torch.Size([1536])\n",
      "blocks.13.norm1.weight torch.Size([1536])\n",
      "blocks.13.norm1.bias torch.Size([1536])\n",
      "blocks.13.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.13.attn.qkv.bias torch.Size([4608])\n",
      "blocks.13.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.13.attn.proj.bias torch.Size([1536])\n",
      "blocks.13.ls1.gamma torch.Size([1536])\n",
      "blocks.13.norm2.weight torch.Size([1536])\n",
      "blocks.13.norm2.bias torch.Size([1536])\n",
      "blocks.13.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.13.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.13.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.13.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.13.ls2.gamma torch.Size([1536])\n",
      "blocks.14.norm1.weight torch.Size([1536])\n",
      "blocks.14.norm1.bias torch.Size([1536])\n",
      "blocks.14.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.14.attn.qkv.bias torch.Size([4608])\n",
      "blocks.14.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.14.attn.proj.bias torch.Size([1536])\n",
      "blocks.14.ls1.gamma torch.Size([1536])\n",
      "blocks.14.norm2.weight torch.Size([1536])\n",
      "blocks.14.norm2.bias torch.Size([1536])\n",
      "blocks.14.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.14.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.14.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.14.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.14.ls2.gamma torch.Size([1536])\n",
      "blocks.15.norm1.weight torch.Size([1536])\n",
      "blocks.15.norm1.bias torch.Size([1536])\n",
      "blocks.15.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.15.attn.qkv.bias torch.Size([4608])\n",
      "blocks.15.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.15.attn.proj.bias torch.Size([1536])\n",
      "blocks.15.ls1.gamma torch.Size([1536])\n",
      "blocks.15.norm2.weight torch.Size([1536])\n",
      "blocks.15.norm2.bias torch.Size([1536])\n",
      "blocks.15.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.15.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.15.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.15.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.15.ls2.gamma torch.Size([1536])\n",
      "blocks.16.norm1.weight torch.Size([1536])\n",
      "blocks.16.norm1.bias torch.Size([1536])\n",
      "blocks.16.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.16.attn.qkv.bias torch.Size([4608])\n",
      "blocks.16.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.16.attn.proj.bias torch.Size([1536])\n",
      "blocks.16.ls1.gamma torch.Size([1536])\n",
      "blocks.16.norm2.weight torch.Size([1536])\n",
      "blocks.16.norm2.bias torch.Size([1536])\n",
      "blocks.16.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.16.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.16.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.16.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.16.ls2.gamma torch.Size([1536])\n",
      "blocks.17.norm1.weight torch.Size([1536])\n",
      "blocks.17.norm1.bias torch.Size([1536])\n",
      "blocks.17.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.17.attn.qkv.bias torch.Size([4608])\n",
      "blocks.17.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.17.attn.proj.bias torch.Size([1536])\n",
      "blocks.17.ls1.gamma torch.Size([1536])\n",
      "blocks.17.norm2.weight torch.Size([1536])\n",
      "blocks.17.norm2.bias torch.Size([1536])\n",
      "blocks.17.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.17.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.17.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.17.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.17.ls2.gamma torch.Size([1536])\n",
      "blocks.18.norm1.weight torch.Size([1536])\n",
      "blocks.18.norm1.bias torch.Size([1536])\n",
      "blocks.18.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.18.attn.qkv.bias torch.Size([4608])\n",
      "blocks.18.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.18.attn.proj.bias torch.Size([1536])\n",
      "blocks.18.ls1.gamma torch.Size([1536])\n",
      "blocks.18.norm2.weight torch.Size([1536])\n",
      "blocks.18.norm2.bias torch.Size([1536])\n",
      "blocks.18.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.18.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.18.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.18.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.18.ls2.gamma torch.Size([1536])\n",
      "blocks.19.norm1.weight torch.Size([1536])\n",
      "blocks.19.norm1.bias torch.Size([1536])\n",
      "blocks.19.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.19.attn.qkv.bias torch.Size([4608])\n",
      "blocks.19.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.19.attn.proj.bias torch.Size([1536])\n",
      "blocks.19.ls1.gamma torch.Size([1536])\n",
      "blocks.19.norm2.weight torch.Size([1536])\n",
      "blocks.19.norm2.bias torch.Size([1536])\n",
      "blocks.19.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.19.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.19.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.19.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.19.ls2.gamma torch.Size([1536])\n",
      "blocks.20.norm1.weight torch.Size([1536])\n",
      "blocks.20.norm1.bias torch.Size([1536])\n",
      "blocks.20.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.20.attn.qkv.bias torch.Size([4608])\n",
      "blocks.20.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.20.attn.proj.bias torch.Size([1536])\n",
      "blocks.20.ls1.gamma torch.Size([1536])\n",
      "blocks.20.norm2.weight torch.Size([1536])\n",
      "blocks.20.norm2.bias torch.Size([1536])\n",
      "blocks.20.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.20.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.20.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.20.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.20.ls2.gamma torch.Size([1536])\n",
      "blocks.21.norm1.weight torch.Size([1536])\n",
      "blocks.21.norm1.bias torch.Size([1536])\n",
      "blocks.21.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.21.attn.qkv.bias torch.Size([4608])\n",
      "blocks.21.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.21.attn.proj.bias torch.Size([1536])\n",
      "blocks.21.ls1.gamma torch.Size([1536])\n",
      "blocks.21.norm2.weight torch.Size([1536])\n",
      "blocks.21.norm2.bias torch.Size([1536])\n",
      "blocks.21.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.21.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.21.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.21.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.21.ls2.gamma torch.Size([1536])\n",
      "blocks.22.norm1.weight torch.Size([1536])\n",
      "blocks.22.norm1.bias torch.Size([1536])\n",
      "blocks.22.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.22.attn.qkv.bias torch.Size([4608])\n",
      "blocks.22.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.22.attn.proj.bias torch.Size([1536])\n",
      "blocks.22.ls1.gamma torch.Size([1536])\n",
      "blocks.22.norm2.weight torch.Size([1536])\n",
      "blocks.22.norm2.bias torch.Size([1536])\n",
      "blocks.22.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.22.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.22.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.22.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.22.ls2.gamma torch.Size([1536])\n",
      "blocks.23.norm1.weight torch.Size([1536])\n",
      "blocks.23.norm1.bias torch.Size([1536])\n",
      "blocks.23.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.23.attn.qkv.bias torch.Size([4608])\n",
      "blocks.23.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.23.attn.proj.bias torch.Size([1536])\n",
      "blocks.23.ls1.gamma torch.Size([1536])\n",
      "blocks.23.norm2.weight torch.Size([1536])\n",
      "blocks.23.norm2.bias torch.Size([1536])\n",
      "blocks.23.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.23.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.23.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.23.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.23.ls2.gamma torch.Size([1536])\n",
      "norm.weight torch.Size([1536])\n",
      "norm.bias torch.Size([1536])\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('UNI-1.5.bin')\n",
    "with open('UNI-1.5.txt','w') as f:\n",
    "    for key in model:\n",
    "        print(key,model[key].shape)\n",
    "        f.write(key+str(model[key].shape)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated state_dict saved to 'updated_state_dict.pth'\n"
     ]
    }
   ],
   "source": [
    "model = vit_h\n",
    "import torch\n",
    "\n",
    "# Load the state_dict from the saved file\n",
    "state_dict = torch.load('UNI-1.5.bin')\n",
    "\n",
    "# Create a copy of the state_dict with updated key names\n",
    "updated_state_dict = {}\n",
    "\n",
    "for key in state_dict.keys():\n",
    "    if 'reg_token' in key:\n",
    "        updated_state_dict['register_tokens']= state_dict[key]\n",
    "    elif 'blocks' in key:\n",
    "        # Rename keys from 'blocks' to 'blocks.0'\n",
    "        new_key = key.replace('blocks.', 'blocks.0.')\n",
    "        if new_key in model.state_dict().keys():\n",
    "            updated_state_dict[new_key] = state_dict[key]\n",
    "        else:\n",
    "            print(f\"New key {new_key} not found in the model's state_dict\")\n",
    "    elif key == 'pos_embed':\n",
    "        # Handle pos_embed with missing cls_token dimension\n",
    "        pos_embed = state_dict[key]\n",
    "        if pos_embed.shape[1] + 1 == model.state_dict()[key].shape[1]:\n",
    "            cls_token_embed = torch.zeros(1, 1, pos_embed.shape[2])\n",
    "            torch.nn.init.trunc_normal_(cls_token_embed, std=0.02)\n",
    "            updated_pos_embed = torch.cat([cls_token_embed, pos_embed], dim=1)\n",
    "            updated_state_dict[key] = updated_pos_embed\n",
    "        else:\n",
    "            print(f\"pos_embed shape mismatch: {pos_embed.shape} vs {model.state_dict()[key].shape}\")\n",
    "    elif key == 'cls_token':\n",
    "        # Handle cls_token initialization\n",
    "        cls_token = torch.zeros(model.state_dict()[key].shape)\n",
    "        torch.nn.init.normal_(cls_token, std=1e-6)\n",
    "        updated_state_dict[key] = cls_token\n",
    "    else:\n",
    "        if key in model.state_dict().keys():\n",
    "            updated_state_dict[key] = state_dict[key]\n",
    "        else:\n",
    "            print(f\"Key {key} not found in the model's state_dict\")\n",
    "#add mask token mask_token torch.Size([1, 1536]) and trunc_normal_(module.weight, std=0.02)\n",
    "updated_state_dict['mask_token'] = torch.zeros(1, 1536)\n",
    "torch.nn.init.trunc_normal_(updated_state_dict['mask_token'], std=0.02)\n",
    "\n",
    "\n",
    "# Save the updated state_dict\n",
    "torch.save(updated_state_dict, 'UNI-1.5-updated.pth')\n",
    "print(\"Updated state_dict saved to 'updated_state_dict.pth'\")\n",
    "\n",
    "# Save the updated state_dict back to a file\n",
    "#torch.save(updated_state_dict, 'Vinchow_updated.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = torch.load('/ruiyan/yuhao/project/FMBC/dinov2/dinov2/models/UNI-1.5-updated.pth')\n",
    "vit_h =  DinoVisionTransformer(**{\n",
    "            'img_size': 224, \n",
    "            'patch_size': 14, \n",
    "            'depth': 24,\n",
    "            'embed_dim': 1536,\n",
    "            'num_heads': 24,\n",
    "            'init_values': 1e-5, \n",
    "            'embed_dim': 1536,\n",
    "            'mlp_ratio': 2.66667*2,\n",
    "            'ffn_layer' :\"SwiGLUPacked\",\n",
    "            'act_layer': torch.nn.SiLU, \n",
    "            'num_register_tokens': 8 })\n",
    "vit_h.load_state_dict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "register_tokens torch.Size([1, 8, 1536])\n",
      "cls_token torch.Size([1, 1, 1536])\n",
      "pos_embed torch.Size([1, 257, 1536])\n",
      "patch_embed.proj.weight torch.Size([1536, 3, 14, 14])\n",
      "patch_embed.proj.bias torch.Size([1536])\n",
      "blocks.0.0.norm1.weight torch.Size([1536])\n",
      "blocks.0.0.norm1.bias torch.Size([1536])\n",
      "blocks.0.0.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.0.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.0.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.0.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.0.ls1.gamma torch.Size([1536])\n",
      "blocks.0.0.norm2.weight torch.Size([1536])\n",
      "blocks.0.0.norm2.bias torch.Size([1536])\n",
      "blocks.0.0.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.0.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.0.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.0.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.0.ls2.gamma torch.Size([1536])\n",
      "blocks.0.1.norm1.weight torch.Size([1536])\n",
      "blocks.0.1.norm1.bias torch.Size([1536])\n",
      "blocks.0.1.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.1.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.1.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.1.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.1.ls1.gamma torch.Size([1536])\n",
      "blocks.0.1.norm2.weight torch.Size([1536])\n",
      "blocks.0.1.norm2.bias torch.Size([1536])\n",
      "blocks.0.1.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.1.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.1.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.1.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.1.ls2.gamma torch.Size([1536])\n",
      "blocks.0.2.norm1.weight torch.Size([1536])\n",
      "blocks.0.2.norm1.bias torch.Size([1536])\n",
      "blocks.0.2.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.2.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.2.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.2.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.2.ls1.gamma torch.Size([1536])\n",
      "blocks.0.2.norm2.weight torch.Size([1536])\n",
      "blocks.0.2.norm2.bias torch.Size([1536])\n",
      "blocks.0.2.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.2.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.2.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.2.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.2.ls2.gamma torch.Size([1536])\n",
      "blocks.0.3.norm1.weight torch.Size([1536])\n",
      "blocks.0.3.norm1.bias torch.Size([1536])\n",
      "blocks.0.3.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.3.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.3.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.3.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.3.ls1.gamma torch.Size([1536])\n",
      "blocks.0.3.norm2.weight torch.Size([1536])\n",
      "blocks.0.3.norm2.bias torch.Size([1536])\n",
      "blocks.0.3.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.3.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.3.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.3.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.3.ls2.gamma torch.Size([1536])\n",
      "blocks.0.4.norm1.weight torch.Size([1536])\n",
      "blocks.0.4.norm1.bias torch.Size([1536])\n",
      "blocks.0.4.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.4.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.4.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.4.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.4.ls1.gamma torch.Size([1536])\n",
      "blocks.0.4.norm2.weight torch.Size([1536])\n",
      "blocks.0.4.norm2.bias torch.Size([1536])\n",
      "blocks.0.4.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.4.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.4.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.4.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.4.ls2.gamma torch.Size([1536])\n",
      "blocks.0.5.norm1.weight torch.Size([1536])\n",
      "blocks.0.5.norm1.bias torch.Size([1536])\n",
      "blocks.0.5.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.5.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.5.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.5.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.5.ls1.gamma torch.Size([1536])\n",
      "blocks.0.5.norm2.weight torch.Size([1536])\n",
      "blocks.0.5.norm2.bias torch.Size([1536])\n",
      "blocks.0.5.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.5.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.5.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.5.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.5.ls2.gamma torch.Size([1536])\n",
      "blocks.0.6.norm1.weight torch.Size([1536])\n",
      "blocks.0.6.norm1.bias torch.Size([1536])\n",
      "blocks.0.6.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.6.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.6.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.6.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.6.ls1.gamma torch.Size([1536])\n",
      "blocks.0.6.norm2.weight torch.Size([1536])\n",
      "blocks.0.6.norm2.bias torch.Size([1536])\n",
      "blocks.0.6.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.6.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.6.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.6.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.6.ls2.gamma torch.Size([1536])\n",
      "blocks.0.7.norm1.weight torch.Size([1536])\n",
      "blocks.0.7.norm1.bias torch.Size([1536])\n",
      "blocks.0.7.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.7.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.7.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.7.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.7.ls1.gamma torch.Size([1536])\n",
      "blocks.0.7.norm2.weight torch.Size([1536])\n",
      "blocks.0.7.norm2.bias torch.Size([1536])\n",
      "blocks.0.7.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.7.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.7.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.7.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.7.ls2.gamma torch.Size([1536])\n",
      "blocks.0.8.norm1.weight torch.Size([1536])\n",
      "blocks.0.8.norm1.bias torch.Size([1536])\n",
      "blocks.0.8.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.8.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.8.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.8.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.8.ls1.gamma torch.Size([1536])\n",
      "blocks.0.8.norm2.weight torch.Size([1536])\n",
      "blocks.0.8.norm2.bias torch.Size([1536])\n",
      "blocks.0.8.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.8.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.8.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.8.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.8.ls2.gamma torch.Size([1536])\n",
      "blocks.0.9.norm1.weight torch.Size([1536])\n",
      "blocks.0.9.norm1.bias torch.Size([1536])\n",
      "blocks.0.9.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.9.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.9.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.9.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.9.ls1.gamma torch.Size([1536])\n",
      "blocks.0.9.norm2.weight torch.Size([1536])\n",
      "blocks.0.9.norm2.bias torch.Size([1536])\n",
      "blocks.0.9.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.9.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.9.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.9.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.9.ls2.gamma torch.Size([1536])\n",
      "blocks.0.10.norm1.weight torch.Size([1536])\n",
      "blocks.0.10.norm1.bias torch.Size([1536])\n",
      "blocks.0.10.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.10.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.10.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.10.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.10.ls1.gamma torch.Size([1536])\n",
      "blocks.0.10.norm2.weight torch.Size([1536])\n",
      "blocks.0.10.norm2.bias torch.Size([1536])\n",
      "blocks.0.10.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.10.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.10.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.10.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.10.ls2.gamma torch.Size([1536])\n",
      "blocks.0.11.norm1.weight torch.Size([1536])\n",
      "blocks.0.11.norm1.bias torch.Size([1536])\n",
      "blocks.0.11.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.11.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.11.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.11.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.11.ls1.gamma torch.Size([1536])\n",
      "blocks.0.11.norm2.weight torch.Size([1536])\n",
      "blocks.0.11.norm2.bias torch.Size([1536])\n",
      "blocks.0.11.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.11.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.11.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.11.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.11.ls2.gamma torch.Size([1536])\n",
      "blocks.0.12.norm1.weight torch.Size([1536])\n",
      "blocks.0.12.norm1.bias torch.Size([1536])\n",
      "blocks.0.12.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.12.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.12.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.12.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.12.ls1.gamma torch.Size([1536])\n",
      "blocks.0.12.norm2.weight torch.Size([1536])\n",
      "blocks.0.12.norm2.bias torch.Size([1536])\n",
      "blocks.0.12.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.12.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.12.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.12.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.12.ls2.gamma torch.Size([1536])\n",
      "blocks.0.13.norm1.weight torch.Size([1536])\n",
      "blocks.0.13.norm1.bias torch.Size([1536])\n",
      "blocks.0.13.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.13.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.13.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.13.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.13.ls1.gamma torch.Size([1536])\n",
      "blocks.0.13.norm2.weight torch.Size([1536])\n",
      "blocks.0.13.norm2.bias torch.Size([1536])\n",
      "blocks.0.13.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.13.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.13.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.13.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.13.ls2.gamma torch.Size([1536])\n",
      "blocks.0.14.norm1.weight torch.Size([1536])\n",
      "blocks.0.14.norm1.bias torch.Size([1536])\n",
      "blocks.0.14.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.14.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.14.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.14.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.14.ls1.gamma torch.Size([1536])\n",
      "blocks.0.14.norm2.weight torch.Size([1536])\n",
      "blocks.0.14.norm2.bias torch.Size([1536])\n",
      "blocks.0.14.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.14.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.14.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.14.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.14.ls2.gamma torch.Size([1536])\n",
      "blocks.0.15.norm1.weight torch.Size([1536])\n",
      "blocks.0.15.norm1.bias torch.Size([1536])\n",
      "blocks.0.15.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.15.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.15.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.15.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.15.ls1.gamma torch.Size([1536])\n",
      "blocks.0.15.norm2.weight torch.Size([1536])\n",
      "blocks.0.15.norm2.bias torch.Size([1536])\n",
      "blocks.0.15.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.15.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.15.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.15.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.15.ls2.gamma torch.Size([1536])\n",
      "blocks.0.16.norm1.weight torch.Size([1536])\n",
      "blocks.0.16.norm1.bias torch.Size([1536])\n",
      "blocks.0.16.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.16.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.16.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.16.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.16.ls1.gamma torch.Size([1536])\n",
      "blocks.0.16.norm2.weight torch.Size([1536])\n",
      "blocks.0.16.norm2.bias torch.Size([1536])\n",
      "blocks.0.16.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.16.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.16.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.16.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.16.ls2.gamma torch.Size([1536])\n",
      "blocks.0.17.norm1.weight torch.Size([1536])\n",
      "blocks.0.17.norm1.bias torch.Size([1536])\n",
      "blocks.0.17.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.17.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.17.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.17.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.17.ls1.gamma torch.Size([1536])\n",
      "blocks.0.17.norm2.weight torch.Size([1536])\n",
      "blocks.0.17.norm2.bias torch.Size([1536])\n",
      "blocks.0.17.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.17.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.17.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.17.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.17.ls2.gamma torch.Size([1536])\n",
      "blocks.0.18.norm1.weight torch.Size([1536])\n",
      "blocks.0.18.norm1.bias torch.Size([1536])\n",
      "blocks.0.18.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.18.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.18.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.18.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.18.ls1.gamma torch.Size([1536])\n",
      "blocks.0.18.norm2.weight torch.Size([1536])\n",
      "blocks.0.18.norm2.bias torch.Size([1536])\n",
      "blocks.0.18.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.18.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.18.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.18.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.18.ls2.gamma torch.Size([1536])\n",
      "blocks.0.19.norm1.weight torch.Size([1536])\n",
      "blocks.0.19.norm1.bias torch.Size([1536])\n",
      "blocks.0.19.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.19.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.19.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.19.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.19.ls1.gamma torch.Size([1536])\n",
      "blocks.0.19.norm2.weight torch.Size([1536])\n",
      "blocks.0.19.norm2.bias torch.Size([1536])\n",
      "blocks.0.19.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.19.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.19.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.19.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.19.ls2.gamma torch.Size([1536])\n",
      "blocks.0.20.norm1.weight torch.Size([1536])\n",
      "blocks.0.20.norm1.bias torch.Size([1536])\n",
      "blocks.0.20.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.20.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.20.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.20.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.20.ls1.gamma torch.Size([1536])\n",
      "blocks.0.20.norm2.weight torch.Size([1536])\n",
      "blocks.0.20.norm2.bias torch.Size([1536])\n",
      "blocks.0.20.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.20.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.20.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.20.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.20.ls2.gamma torch.Size([1536])\n",
      "blocks.0.21.norm1.weight torch.Size([1536])\n",
      "blocks.0.21.norm1.bias torch.Size([1536])\n",
      "blocks.0.21.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.21.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.21.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.21.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.21.ls1.gamma torch.Size([1536])\n",
      "blocks.0.21.norm2.weight torch.Size([1536])\n",
      "blocks.0.21.norm2.bias torch.Size([1536])\n",
      "blocks.0.21.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.21.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.21.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.21.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.21.ls2.gamma torch.Size([1536])\n",
      "blocks.0.22.norm1.weight torch.Size([1536])\n",
      "blocks.0.22.norm1.bias torch.Size([1536])\n",
      "blocks.0.22.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.22.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.22.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.22.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.22.ls1.gamma torch.Size([1536])\n",
      "blocks.0.22.norm2.weight torch.Size([1536])\n",
      "blocks.0.22.norm2.bias torch.Size([1536])\n",
      "blocks.0.22.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.22.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.22.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.22.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.22.ls2.gamma torch.Size([1536])\n",
      "blocks.0.23.norm1.weight torch.Size([1536])\n",
      "blocks.0.23.norm1.bias torch.Size([1536])\n",
      "blocks.0.23.attn.qkv.weight torch.Size([4608, 1536])\n",
      "blocks.0.23.attn.qkv.bias torch.Size([4608])\n",
      "blocks.0.23.attn.proj.weight torch.Size([1536, 1536])\n",
      "blocks.0.23.attn.proj.bias torch.Size([1536])\n",
      "blocks.0.23.ls1.gamma torch.Size([1536])\n",
      "blocks.0.23.norm2.weight torch.Size([1536])\n",
      "blocks.0.23.norm2.bias torch.Size([1536])\n",
      "blocks.0.23.mlp.fc1.weight torch.Size([8192, 1536])\n",
      "blocks.0.23.mlp.fc1.bias torch.Size([8192])\n",
      "blocks.0.23.mlp.fc2.weight torch.Size([1536, 4096])\n",
      "blocks.0.23.mlp.fc2.bias torch.Size([1536])\n",
      "blocks.0.23.ls2.gamma torch.Size([1536])\n",
      "norm.weight torch.Size([1536])\n",
      "norm.bias torch.Size([1536])\n",
      "mask_token torch.Size([1, 1536])\n"
     ]
    }
   ],
   "source": [
    "for key in dict.keys():\n",
    "    print(key, dict[key].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decimal('0.2')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from decimal import Decimal\n",
    "a = Decimal('1')\n",
    "b = Decimal('0.8')\n",
    "a-b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19999999999999996"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-0.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
