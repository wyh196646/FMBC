{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_absolute_positional_encoding(seq_len, d_model):\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "# 假设句子长度为8，d_model为32\n",
    "sentence = \"我爱你，中国。\"\n",
    "seq_len = len(sentence)\n",
    "d_model = 32\n",
    "\n",
    "absolute_positional_encoding = get_absolute_positional_encoding(seq_len, d_model)\n",
    "\n",
    "# 展示绝对位置编码的效果\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(absolute_positional_encoding, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"Absolute Positional Encoding\")\n",
    "plt.xlabel(\"d_model dimensions\")\n",
    "plt.ylabel(\"Position in Sentence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sin(): argument 'input' (position 1) must be Tensor, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 使用相对位置编码\u001b[39;00m\n\u001b[1;32m     38\u001b[0m sentence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentence)\n\u001b[0;32m---> 39\u001b[0m relative_positional_encoding \u001b[38;5;241m=\u001b[39m RelativePositionalEncoding(d_model, max_len\u001b[38;5;241m=\u001b[39msentence_length)\n\u001b[1;32m     40\u001b[0m relative_positional_encodings \u001b[38;5;241m=\u001b[39m relative_positional_encoding(sentence_length)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 展示相对位置编码的效果\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mRelativePositionalEncoding.__init__\u001b[0;34m(self, d_model, max_len)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 生成相对位置编码\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_positions_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_relative_positions_matrix(max_len)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_embeddings_table(max_len, d_model)\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mRelativePositionalEncoding.create_embeddings_table\u001b[0;34m(self, max_len, d_model)\u001b[0m\n\u001b[1;32m     20\u001b[0m table \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(max_len, max_len, d_model)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m-\u001b[39mmax_len\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, max_len):\n\u001b[0;32m---> 22\u001b[0m     table[:, pos] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_relative_positional_encoding(pos, d_model)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mRelativePositionalEncoding.get_relative_positional_encoding\u001b[0;34m(self, pos, d_model)\u001b[0m\n\u001b[1;32m     26\u001b[0m pos_encoding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(d_model)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, d_model, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     pos_encoding[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(pos \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m i)\u001b[38;5;241m/\u001b[39md_model)))\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m d_model:\n\u001b[1;32m     30\u001b[0m         pos_encoding[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(pos \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m i)\u001b[38;5;241m/\u001b[39md_model)))\n",
      "\u001b[0;31mTypeError\u001b[0m: sin(): argument 'input' (position 1) must be Tensor, not float"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RelativePositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(RelativePositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # 生成相对位置编码\n",
    "        self.relative_positions_matrix = self.generate_relative_positions_matrix(max_len)\n",
    "        self.embeddings_table = self.create_embeddings_table(max_len, d_model)\n",
    "\n",
    "    def generate_relative_positions_matrix(self, length):\n",
    "        range_vec = torch.arange(length)\n",
    "        distance_mat = range_vec[None, :] - range_vec[:, None]\n",
    "        return distance_mat\n",
    "\n",
    "    def create_embeddings_table(self, max_len, d_model):\n",
    "        table = torch.zeros(max_len, max_len, d_model)\n",
    "        for pos in range(-max_len+1, max_len):\n",
    "            table[:, pos] = self.get_relative_positional_encoding(pos, d_model)\n",
    "        return table\n",
    "\n",
    "    def get_relative_positional_encoding(self, pos, d_model):\n",
    "        pos_encoding = torch.zeros(d_model)\n",
    "        for i in range(0, d_model, 2):\n",
    "            pos_encoding[i] = torch.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                pos_encoding[i + 1] = torch.cos(pos / (10000 ** ((2 * i)/d_model)))\n",
    "        return pos_encoding\n",
    "\n",
    "    def forward(self, length):\n",
    "        positions_matrix = self.relative_positions_matrix[:length, :length]\n",
    "        return F.embedding(positions_matrix, self.embeddings_table)\n",
    "\n",
    "# 使用相对位置编码\n",
    "sentence_length = len(sentence)\n",
    "relative_positional_encoding = RelativePositionalEncoding(d_model, max_len=sentence_length)\n",
    "relative_positional_encodings = relative_positional_encoding(sentence_length)\n",
    "\n",
    "# 展示相对位置编码的效果\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(relative_positional_encodings.detach().numpy(), cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"Relative Positional Encoding\")\n",
    "plt.xlabel(\"d_model dimensions\")\n",
    "plt.ylabel(\"Relative Position\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dir='/home/yuhaowang/data/embedding/TCGA-LUAD'\n",
    "import os\n",
    "test_case=os.listdir(embedding_dir)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_assets_from_h5( h5_path: str) -> tuple:\n",
    "    '''Read the assets from the h5 file'''\n",
    "    assets = {}\n",
    "    attrs = {}\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            assets[key] = f[key][:]\n",
    "            if f[key].attrs is not None:\n",
    "                attrs[key] = dict(f[key].attrs)\n",
    "    return assets, attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res,_=read_assets_from_h5(os.path.join(embedding_dir,test_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster_centers': array([[ 0.06074125, -0.04797647,  0.0314041 , ..., -0.00976407,\n",
       "          0.01675332, -0.00295427],\n",
       "        [ 0.05002979, -0.04412391,  0.02896007, ...,  0.00101928,\n",
       "          0.0109681 , -0.02167385],\n",
       "        [ 0.04838791, -0.04288549,  0.01721936, ...,  0.00459621,\n",
       "         -0.00435249, -0.03027587],\n",
       "        ...,\n",
       "        [ 0.04690852, -0.06893064,  0.07062602, ..., -0.02154604,\n",
       "          0.00104492,  0.0054438 ],\n",
       "        [ 0.0498773 , -0.03476577,  0.03767053, ..., -0.02402386,\n",
       "          0.00083551, -0.01236775],\n",
       "        [ 0.04598414, -0.03954945,  0.03422585, ..., -0.00312588,\n",
       "          0.00804252, -0.03439237]]),\n",
       " 'coords': array([[  824, 57945],\n",
       "        [  824, 58969],\n",
       "        [  824, 59993],\n",
       "        ...,\n",
       "        [86841, 46680],\n",
       "        [86841, 47704],\n",
       "        [87865, 45656]]),\n",
       " 'features': array([[ 0.07768731,  0.01407234,  0.0958391 , ..., -0.00307684,\n",
       "          0.01369488, -0.02950972],\n",
       "        [ 0.01849896,  0.03556122,  0.07713094, ..., -0.01821687,\n",
       "          0.04864519, -0.01606839],\n",
       "        [ 0.02407581,  0.0315759 ,  0.09177183, ...,  0.01547028,\n",
       "          0.02760443, -0.00841782],\n",
       "        ...,\n",
       "        [ 0.08713299, -0.06643428,  0.16560106, ..., -0.01728924,\n",
       "          0.00097763,  0.01950411],\n",
       "        [ 0.05545963, -0.02795944,  0.15658545, ..., -0.00114633,\n",
       "         -0.00348314,  0.01832027],\n",
       "        [ 0.02098264, -0.02248756,  0.11607388, ..., -0.02096792,\n",
       "         -0.00048509,  0.03780413]], dtype=float32),\n",
       " 'img_len': array([4103]),\n",
       " 'labels': array([ 5,  5,  5, ..., 13, 29, 29], dtype=int32),\n",
       " 'pad_mask': array([0])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 384)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['cluster_centers'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(vec1, vec2):\n",
    "\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    distance = np.linalg.norm(vec1 - vec2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 281,  282,  323,  324,  363,  365,  405,  406,  407,  408,  452,\n",
       "        453,  454,  455,  456,  497,  503,  504,  505,  543,  544,  545,\n",
       "        546,  547,  548,  549,  589,  591,  592,  593,  635,  637,  638,\n",
       "        639,  640,  643,  687,  689,  733,  736,  784,  785,  787,  835,\n",
       "        836,  837,  890,  891,  892,  894,  896,  947,  948,  949,  950,\n",
       "        951,  955, 1003, 1004, 1005, 1008, 1065, 1066, 1106, 1128, 1132,\n",
       "       1134, 1178, 1188, 1190, 1192, 1193, 1194, 1196, 1230, 1231, 1233,\n",
       "       1234, 1235, 1236, 1244, 1254, 1255, 1256, 1257, 1261, 1265, 1266,\n",
       "       1293, 1300, 1301, 1302, 1303, 1304, 1314, 1320, 1321, 1322, 1323,\n",
       "       1324, 1325, 1329, 1330, 1333, 1334, 1362, 1365, 1367, 1368, 1369,\n",
       "       1370, 1372, 1382, 1387, 1388, 1390, 1391, 1392, 1393, 1396, 1397,\n",
       "       1398, 1400, 1401, 1428, 1431, 1433, 1435, 1454, 1455, 1456, 1457,\n",
       "       1459, 1460, 1461, 1462, 1463, 1466, 1467, 1468, 1498, 1500, 1505,\n",
       "       1517, 1518, 1524, 1525, 1527, 1530, 1531, 1532, 1533, 1534, 1535,\n",
       "       1567, 1592, 1593, 1594, 1595, 1598, 1599, 1600, 1601, 1602, 1603,\n",
       "       1650, 1662, 1663, 1665, 1667, 1668, 1669, 1670, 1700, 1712, 1719,\n",
       "       1724, 1725, 1726, 1728, 1729, 1731, 1732, 1777, 1782, 1785, 1787,\n",
       "       1789, 1790, 1791, 1792, 1794, 1843, 1849, 1850, 1851, 1853, 1907,\n",
       "       1908, 1910, 1912, 1914, 1971, 1972, 1973, 1978, 2025, 2026, 2033,\n",
       "       2081, 2083, 2084, 2087, 2093, 2098, 2147, 2148, 2151, 2187, 2207,\n",
       "       2208, 2209, 2211, 2215, 2222, 2271, 2272, 2274, 2327, 2405, 2465])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_key=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49695843164585723 0.8347871627300506\n",
      "0.6865236112231249 0.9206541320373542\n",
      "0.15965947397005326 0.3886289260592855\n",
      "0.5463448226732246 0.8969584405896887\n",
      "0.7472923550046136 0.913104967742602\n",
      "0.6079110619859472 1.0392453874847447\n",
      "0.3880827293651171 0.691375986283879\n",
      "0.5375631440688109 1.131094084409436\n",
      "0.577325906208356 0.8281451973132208\n",
      "0.23379568810142462 0.574613758769568\n",
      "0.736016566932542 1.008504722876743\n",
      "0.5667063595865213 0.845059450720478\n",
      "0.2636339688765211 0.5309647924300439\n",
      "0.5081500680495393 0.8150117461292699\n",
      "0.7196179659224526 0.9092553250641179\n",
      "0.5621899612018695 0.9135726482406058\n",
      "0.4297852477956964 0.6850388272655397\n",
      "0.7182926582686625 0.9273319825336368\n",
      "0.3908901581592258 0.6542563182124227\n",
      "0.7322446405390921 0.8889480290693409\n",
      "0.39024666581634665 0.7257141121833522\n",
      "0.7526356817616439 0.9521748493245944\n",
      "0.7049623906709864 0.9066033995841241\n",
      "0.4299710427148452 1.0511538719267186\n",
      "0.6266003892195126 0.8599956459095797\n",
      "0.3896535707301488 0.8084717746519425\n",
      "0.20135835035714061 0.5521648661078751\n",
      "0.5657930373245894 0.8990905647114416\n",
      "0.3850190009629305 0.6817142102262896\n",
      "0.7127271277012027 1.0417940237017234\n",
      "0.6931634205424088 1.111299012548658\n",
      "0.7080664461333263 1.0451699783340949\n",
      "0.6921318506885142 0.86869384029549\n",
      "0.21771467269741457 0.5612225700392928\n",
      "0.6152790620040622 0.9410313886784312\n",
      "0.38095419609554737 0.7583059926556437\n",
      "0.4877996838880756 0.8492418082927322\n",
      "0.676256405897038 0.8721754268449778\n",
      "0.7082368322766303 1.1375567649853608\n",
      "0.62415389094743 0.8253731547458395\n",
      "0.5063099222039085 0.8135442757953134\n",
      "0.9333167401633452 1.179714276692545\n",
      "0.3440465050978 0.6465430656169653\n",
      "0.5327803133460373 0.8210898515408445\n",
      "0.5053325154292998 0.9442237389679291\n",
      "0.7371708233772675 0.9301891476023643\n",
      "0.6606957005788003 0.8383299785097339\n",
      "0.6266634557349403 0.9155738492263165\n",
      "0.36566479726889917 0.6549202967322391\n",
      "0.2665803955432735 0.5909748832431139\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    indexs=np.where(res['labels']==i)[0]\n",
    "\n",
    "    similarity_list=[]\n",
    "    for index in indexs:\n",
    "        # print(index)\n",
    "        # print(res['features'][index].shape)\n",
    "        #print(calculate_similarity(res['cluster_centers'][cluster_key],res['features'][index]))\n",
    "        similarity_list.append(calculate_similarity(res['cluster_centers'][cluster_key],res['features'][index]))\n",
    "    #compute kmenas similarity between res['cluster_centers'][cluster_key] and res['features'][index]\n",
    "    print(min(similarity_list),max(similarity_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.rand([10, 1367, 384])\n",
    "b=torch.rand([10, 2734, 384])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The shape of the 2D attn_mask is torch.Size([2734, 2734]), but should be (10, 10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m memory \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, input_seq_len, d_model)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_tensor, memory)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[66], line 41\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, x, memory)\u001b[0m\n\u001b[1;32m     38\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_seq_len)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt\u001b[38;5;241m=\u001b[39mexpanded_x, memory\u001b[38;5;241m=\u001b[39mmemory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj(decoded)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:602\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    599\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 602\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[1;32m    603\u001b[0m         output,\n\u001b[1;32m    604\u001b[0m         memory,\n\u001b[1;32m    605\u001b[0m         tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[1;32m    606\u001b[0m         memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    607\u001b[0m         tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    608\u001b[0m         memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    609\u001b[0m         tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal,\n\u001b[1;32m    610\u001b[0m         memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal,\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    614\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:1087\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1086\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[0;32m-> 1087\u001b[0m         x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n\u001b[1;32m   1088\u001b[0m     )\n\u001b[1;32m   1089\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(\n\u001b[1;32m   1090\u001b[0m         x\n\u001b[1;32m   1091\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(\n\u001b[1;32m   1092\u001b[0m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m     )\n\u001b[1;32m   1095\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:1107\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1102\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m   1108\u001b[0m         x,\n\u001b[1;32m   1109\u001b[0m         x,\n\u001b[1;32m   1110\u001b[0m         x,\n\u001b[1;32m   1111\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1112\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1113\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1114\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1115\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1343\u001b[0m         query,\n\u001b[1;32m   1344\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1369\u001b[0m         query,\n\u001b[1;32m   1370\u001b[0m         key,\n\u001b[1;32m   1371\u001b[0m         value,\n\u001b[1;32m   1372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[1;32m   1373\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight,\n\u001b[1;32m   1375\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k,\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v,\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1382\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1383\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1384\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1385\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1386\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1387\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1388\u001b[0m     )\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:6131\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6129\u001b[0m     correct_2d_size \u001b[38;5;241m=\u001b[39m (tgt_len, src_len)\n\u001b[1;32m   6130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m correct_2d_size:\n\u001b[0;32m-> 6131\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   6132\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the 2D attn_mask is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_2d_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6133\u001b[0m         )\n\u001b[1;32m   6134\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   6135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The shape of the 2D attn_mask is torch.Size([2734, 2734]), but should be (10, 10)."
     ]
    }
   ],
   "source": [
    "#define a decoder that takes a tensor of (10, 2734, 384) and returns a tensor of shape (10, 1367, 384)\n",
    "decoder="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import (\n",
    "randn,\n",
    ")\n",
    "batch_size = 10\n",
    "n_tokens = 1367\n",
    "token_dim = 384\n",
    "\n",
    "# tokens is currently a dummy tensor.\n",
    "# Later, it will be replaced by the actual tokens\n",
    "tokens = randn(batch_size, n_tokens, token_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import (\n",
    "randn,\n",
    "zeros,\n",
    ")\n",
    "\n",
    "\n",
    "tokens = randn(batch_size, n_tokens, token_dim)\n",
    "\n",
    "indices_to_mask = randn(batch_size, n_tokens)\n",
    "\n",
    "n_masked_tokens = int(0.25*n_tokens)\n",
    "\n",
    "indices_to_mask = indices_to_mask.topk(k=n_masked_tokens,dim=1,)\n",
    "indices_to_mask = indices_to_mask.indices\n",
    "bitmask = zeros(batch_size, n_tokens)\n",
    "\n",
    "bitmask = bitmask.scatter(dim=1,index=indices_to_mask,value=1,)\n",
    "bitmask = bitmask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m Linear,\n\u001b[1;32m     10\u001b[0m Parameter,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m l1_loss,\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m tokens \u001b[38;5;241m=\u001b[39m vit\u001b[38;5;241m.\u001b[39mpatch_embed(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     19\u001b[0m mask_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrandn(token_dim))\n\u001b[1;32m     21\u001b[0m mask_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_token\u001b[38;5;241m.\u001b[39mrepeat(batch_size, n_tokens, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vit' is not defined"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "from einops import (\n",
    "rearrange,\n",
    ")\n",
    "from torch import (\n",
    "randn,\n",
    "zeros,\n",
    ")\n",
    "from torch.nn import (\n",
    "Linear,\n",
    "Parameter,\n",
    ")\n",
    "from torch.nn.functional import (\n",
    "l1_loss,\n",
    ")\n",
    "\n",
    "\n",
    "tokens =torch.rand([10, 1367, 384])\n",
    "\n",
    "mask_token = torch.nn.Parameter(torch.randn(token_dim))\n",
    "\n",
    "mask_tokens = self.mask_token.repeat(batch_size, n_tokens, 1)\n",
    "\n",
    "indices_to_mask = randn(batch_size, n_tokens)\n",
    "\n",
    "n_masked_tokens = int(0.5*n_tokens)\n",
    "\n",
    "indices_to_mask = indices_to_mask.topk(\n",
    "k=n_masked_tokens,\n",
    "dim=1,\n",
    ")\n",
    "\n",
    "indices_to_mask = indices_to_mask.indices\n",
    "bitmask = zeros(batch_size, n_tokens)\n",
    "\n",
    "bitmask = bitmask.scatter(\n",
    "dim=1,\n",
    "index=indices_to_mask,\n",
    "value=1,\n",
    ")\n",
    "bitmask = bitmask.bool()\n",
    "\n",
    "bitmask = bitmask.unsqueeze(2)\n",
    "\n",
    "tokens = (~bitmask)*tokens + bitmask*mask_tokens\n",
    "\n",
    "tokens = tokens+vit.pos_embed[:, 1:]\n",
    "\n",
    "encoded = vit.blocks(tokens)\n",
    "\n",
    "bitmask = bitmask.squeeze(2)\n",
    "\n",
    "masked_tokens_encoded = encoded[bitmask]\n",
    "\n",
    "patch_height = patch_width = vit.patch_embed.patch_size\n",
    "\n",
    "decoder_out_dim = 3*patch_height*patch_width\n",
    "decoder = Linear(\n",
    "in_features=token_dim,\n",
    "out_features=decoder_out_dim,\n",
    ")\n",
    "\n",
    "masked_patches_reconstructed = decoder(masked_tokens_encoded)\n",
    "\n",
    "pattern = (\n",
    "'batch_size n_channels (n_patches_height patch_height) (n_patches_width patch_width) -> '\n",
    "'batch_size (n_patches_height n_patches_width) (n_channels patch_height patch_width)'\n",
    ")\n",
    "\n",
    "patches = einops.rearrange(\n",
    "tensor=input,\n",
    "pattern=pattern,\n",
    "patch_height=patch_height,\n",
    "patch_width=patch_width,\n",
    ")\n",
    "\n",
    "maskes_patches_original = patches[bitmask]\n",
    "\n",
    "# The loss is the L1 difference between\n",
    "# the predicted pixel values and the ground truth,\n",
    "# divided by the number of masked patches\n",
    "loss = l1_loss(\n",
    "input=masked_patches_reconstructed,\n",
    "target=maskes_patches_original,\n",
    ")/n_masked_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
