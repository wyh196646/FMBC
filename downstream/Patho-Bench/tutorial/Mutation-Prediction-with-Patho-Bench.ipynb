{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ End-to-end tutorial for WSI processing and benchmarking\n",
    "\n",
    "Follow along this tutorial to download WSIs, process them with Trident, and run benchmarking studies using Patho-Bench.\n",
    "\n",
    "## What will this tutorial cover?\n",
    "1. Downloading WSIs for CPTAC Clear Cell Renal Cell Carcinoma (CCRCC)\n",
    "2. Processing the WSIs with [Trident](https://github.com/mahmoodlab/trident), our one-stop package for WSI preprocessing\n",
    "3. Running benchmarking studies. Following examples are included:  \n",
    "    a. Linear probe for BAP1 mutation prediction  \n",
    "    b. Cox Proportional Hazards (CoxPH) model for survival prediction    \n",
    "    c. Attention-based multiple instance learning model for BAP1 mutation prediction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚¨áÔ∏è Download CPTAC CCRCC WSIs \n",
    "\n",
    "You can easily download CPTAC CCRCC WSIs from [TCIA Cancer Imaging Archive](https://www.cancerimagingarchive.net/collection/cptac-ccrcc/). If you have access to SSD, download your WSIs there for faster IO. \n",
    "\n",
    "**Tip**: Keep an eye out on this webpage for any new updates to the dataset as newer versions are often released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßë‚Äçüî¨ üß¨ Installing Patho-Bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run:\n",
    "\n",
    "1. `conda create -n pathobench python=3.10`\n",
    "2. `conda activate pathobench`\n",
    "3. `pip install -e .`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Preprocess WSIs: segmentation, patching and patch feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [Trident](https://github.com/mahmoodlab/trident), our package for WSI processing and feature extraction. Trident is already installed as part of Patho-Bench installation. \n",
    "\n",
    "Run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from run_batch_of_slides import main # import Trident launch script. \n",
    "\n",
    "sys.argv = [\n",
    "    \"run_batch_of_slides\",\n",
    "    '--task', 'all', \\\n",
    "    '--job_dir', './cptac_ccrcc', \\\n",
    "    '--wsi_dir', './CPTAC-CCRCC_v1/CCRCC', \\\n",
    "    '--patch_encoder', 'conch_v15', \\\n",
    "    '--mag', '20', \\\n",
    "    '--patch_size', '512', \\\n",
    "    '--skip_errors',\n",
    "]\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the following directory structure as the output. Note, we have placed the `wsis` folder inside the job dir, but you can put it anywhere.\n",
    "```bash\n",
    "|-->/path/to/job_dir\n",
    "    |-->20x_512px_0px_overlap\n",
    "        |-->features_conch_v15 --> these are the patch features\n",
    "        |-->patches\n",
    "        |-->visualizations\n",
    "    |-->contours\n",
    "    |-->contours_geojson\n",
    "    |-->thumbnails\n",
    "    |-->wsis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a linear probe experiment for BAP1 mutation prediction  \n",
    "\n",
    "For predicting BAP1 mutation in CCRCC using Titan slide embeddings, we train a linear regression model (linear probe). Below is a modular function to run a linear probe experiment.  \n",
    "\n",
    "### Argument Descriptions  \n",
    "\n",
    "| Argument | Description | Possible Values | Additional Notes |\n",
    "|----------|-------------|----------------|------------------|\n",
    "| `model_name` | Slide encoder model to use | `titan`, `prism`, `chief`, etc. | - |\n",
    "| `train_source` | Train dataset | Unless using custom splits, must match datasource name from [Hugging Face](https://huggingface.co/datasets/MahmoodLab/Patho-Bench) | - |\n",
    "| `test_source` | Test dataset | Unless using custom splits, must match datasource name from [Hugging Face](https://huggingface.co/datasets/MahmoodLab/Patho-Bench) | Defaults to None. If not None, will test generalizability by training on all of train_source and testing on all of test_source. Note that if test_source is used the task must exist in both datasets. |\n",
    "| `task_name` | Task to be performed | Example: `BAP1_mutation` | See [Hugging Face](https://huggingface.co/datasets/MahmoodLab/Patho-Bench) for available tasks. |\n",
    "| `patch_embeddings_dirs` | Location of patch embeddings | Example if using Trident for patch extraction: `'/path/to/job_dir/20x_512px_0px_overlap/features_conch_v15'` | Used by Patho-Bench to construct slide- or patient-level embeddings. Can be a list if your patch embeddings are split across multiple directories. |\n",
    "| `pooled_embeddings_root` | Storage location for pooled slide features | User-defined path | Pooled embeddings are saved when first run; subsequent runs use stored features instead of re-pooling. |\n",
    "| `splits_root` | Location to download dataset splits | User-defined path (optional) | Splits are automatically downloaded from [Hugging Face](https://huggingface.co/datasets/MahmoodLab/Patho-Bench). If not provided, must provide local paths `path_to_split` and `path_to_task_config` instead. |\n",
    "| `combine_slides_per_patient` | Method for pooling multiple WSIs per patient | `early` or `late` fusion | Use `True` for early fusion (concatenating all patch embeddings and processing as a single pseudo-slide) and `False` for late fusion (processing slides separately and averaging slide embeddings) |\n",
    "| `cost` | Regularization strength for the linear probe | Inverse of regularization strength | - |\n",
    "| `balanced` | Whether to apply balanced loss for the linear probe | `True` or `False` | - |\n",
    "| `saveto` | Directory to save experiment results | User-defined path | Choose an appropriate location. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patho_bench.ExperimentFactory import ExperimentFactory\n",
    "\n",
    "model_name = 'titan'\n",
    "train_source = 'cptac_ccrcc' \n",
    "task_name = 'BAP1_mutation'\n",
    "\n",
    "# For this task, we will automatically download the split and task config from HuggingFace.\n",
    "experiment = ExperimentFactory.linprobe(\n",
    "                    model_name = model_name,\n",
    "                    train_source = train_source,\n",
    "                    task_name = task_name,\n",
    "                    patch_embeddings_dirs = './cptac_ccrcc/20x_512px_0px_overlap/features_conch_v15',\n",
    "                    pooled_embeddings_root = './_tutorial_pooled_features',\n",
    "                    splits_root = './_tutorial_splits', # This is where the downloaded splits are saved\n",
    "                    combine_slides_per_patient = False,\n",
    "                    cost = 1,\n",
    "                    balanced = False,\n",
    "                    saveto = f'./_tutorial_linprobe/{train_source}/{task_name}/{model_name}',\n",
    "                )\n",
    "experiment.train()\n",
    "experiment.test()\n",
    "result = experiment.report_results(metric = 'macro-ovr-auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of automatically downloading the split and task config from Huggingface, you can also provide your own split and task config. To do this, simply provide `path_to_split` and `path_to_task_config` instead of `splits_root`.\n",
    "\n",
    "If you wish to develop custom splits and tasks, please follow the format of the HuggingFace downloaded splits and task configs. At minimum, your task config should contain the following fields.\n",
    "\n",
    "```yaml\n",
    "task_col: BAP1_mutation # Column containing labels for the task\n",
    "extra_cols: [] # Any extra columns needed to perform the task (e.g. survival tasks)\n",
    "\n",
    "metrics: # List of one or more performance metrics to report (this is used for automated result compilation when using Patho-Bench in advanced mode)\n",
    "  - macro-ovr-auc\n",
    "\n",
    "label_dict: # Dictionary of integer labels to string labels\n",
    "  0: wildtype\n",
    "  1: mutant\n",
    "\n",
    "sample_col: case_id # Column containing the unit of analysis. Use 'case_id' for patient-level tasks and 'slide_id' for slide-level tasks.\n",
    "```\n",
    "\n",
    "Here we rerun the previous experiment, but this time we provide local paths to the previously downloaded split and task config files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = ExperimentFactory.linprobe(\n",
    "                    model_name = model_name,\n",
    "                    train_source = train_source,\n",
    "                    task_name = task_name,\n",
    "                    patch_embeddings_dirs = '/media/ssd1/cptac_ccrcc/20x_512px_0px_overlap/features_conch_v15',\n",
    "                    pooled_embeddings_root = './_tutorial_pooled_features',\n",
    "                    path_to_split = f'./_tutorial_splits/{train_source}/{task_name}/k=all.tsv', # Local path to the split\n",
    "                    path_to_task_config = f'./_tutorial_splits/{train_source}/{task_name}/config.yaml', # Local path to the task config\n",
    "                    combine_slides_per_patient = False,\n",
    "                    cost = 1,\n",
    "                    balanced = False,\n",
    "                    saveto = f'./_tutorial_linprobe/{train_source}/{task_name}/{model_name}_fromlocal',\n",
    "                )\n",
    "experiment.train()\n",
    "experiment.test()\n",
    "result = experiment.report_results(metric = 'macro-ovr-auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üê∫ Running a survival prediction experiment for CPTAC CCRCC\n",
    "\n",
    "Let's see how can we train a CoxPH model to predict survival using Titan slide embeddings. Most of the arguments follow from our previous example of linear probe, but some CoxPH-specific hyperparameters are as follows:\n",
    "\n",
    "| Argument | Description | Possible Values | Additional Notes |\n",
    "|----------|-------------|----------------|------------------|\n",
    "| `alpha` | sequence of regularization strengths (L1 penalty) used in the elastic net penalized Cox model, controlling the sparsity of the learned coefficients | float | If you get a c-index of 0.5 exactly, that means CoxPH model has not converged. Try changing alpha |\n",
    "| `l1_ratio` | controls the balance between L1 (lasso) and L2 (ridge) regularization, where `l1_ratio=1` corresponds to pure L1 regularization (lasso), `l1_ratio=0` corresponds to pure L2 regularization (ridge), and values in between apply an elastic net penalty. | 0.0 to 1.0 | We use 0.5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patho_bench.ExperimentFactory import ExperimentFactory\n",
    "\n",
    "model_name = 'titan'\n",
    "train_source = 'cptac_ccrcc' \n",
    "task_name = 'OS'\n",
    "\n",
    "patch_embeddings_dirs = './cptac_ccrcc/20x_512px_0px_overlap/features_conch_v15' \n",
    "\n",
    "experiment = ExperimentFactory.coxnet(\n",
    "                    model_name = model_name,\n",
    "                    train_source = train_source,\n",
    "                    task_name = task_name,\n",
    "                    patch_embeddings_dirs = patch_embeddings_dirs,\n",
    "                    pooled_embeddings_root = './_tutorial_pooled_features',\n",
    "                    splits_root = './_tutorial_splits',\n",
    "                    combine_slides_per_patient = False,\n",
    "                    alpha = 0.07,\n",
    "                    l1_ratio = 0.5,\n",
    "                    saveto = f'./_tutorial_coxnet/{train_source}/{task_name}/{model_name}',\n",
    "                )\n",
    "experiment.train()\n",
    "experiment.test()\n",
    "result = experiment.report_results(metric = 'cindex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üñåÔ∏è Training an ABMIL from scratch\n",
    "\n",
    "In many scenarios, a simple linear probe may not be sufficient and you need a deep learning model. `Patho-Bench` will support you in easily training attention based multiple instance learning (ABMIL) models for this purpose. Let's use our example of BAP1 mutation to train an ABMIL model. Note that running the below cell may take some time, as this task has 50 folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patho_bench.ExperimentFactory import ExperimentFactory\n",
    "\n",
    "model_name = 'abmil'\n",
    "train_source = 'cptac_ccrcc'\n",
    "task_name = 'BAP1_mutation'\n",
    "\n",
    "# Hyperparameters\n",
    "bag_size = 2048\n",
    "base_learning_rate = 0.0003\n",
    "layer_decay = None\n",
    "gradient_accumulation = 1\n",
    "weight_decay = 0.00001\n",
    "num_epochs = 20\n",
    "scheduler_type = 'cosine'\n",
    "optimizer_type = 'AdamW'\n",
    "\n",
    "patch_embeddings_dirs = './cptac_ccrcc/20x_512px_0px_overlap/features_conch_v15' \n",
    "\n",
    "experiment = ExperimentFactory.finetune(\n",
    "                    model_name = model_name,\n",
    "                    train_source = train_source,\n",
    "                    task_name = task_name,\n",
    "                    task_type = 'classification',\n",
    "                    patch_embeddings_dirs = patch_embeddings_dirs,\n",
    "                    combine_slides_per_patient = False,\n",
    "                    splits_root = './_tutorial_splits',\n",
    "                    bag_size = bag_size,\n",
    "                    base_learning_rate = base_learning_rate,\n",
    "                    layer_decay = layer_decay, \n",
    "                    gradient_accumulation = gradient_accumulation,\n",
    "                    weight_decay = weight_decay,\n",
    "                    num_epochs = num_epochs,\n",
    "                    scheduler_type = scheduler_type,\n",
    "                    optimizer_type = optimizer_type,\n",
    "                    balanced = True, \n",
    "                    save_which_checkpoints = 'last-1',\n",
    "                    saveto = f'./_tutorial_finetune/{train_source}/{task_name}/{model_name}')\n",
    "experiment.train()\n",
    "experiment.test()\n",
    "result = experiment.report_results(task_name, 'macro-ovr-auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `ExperimentFactory.finetune()` method can also be used for finetuning pretrained slide encoders instead of training an ABMIL from scratch. You are encouraged to read the code and explore the additional capabilities of Patho-Bench."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathobench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
