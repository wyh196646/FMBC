{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0+cu117'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m torch.distributed.launch --master_port 14528 --nproc_per_node=1 dinov2/train/train.py --config-file=dinov2/configs/train/vitl16_short.yaml --output-dir=./output/ train.dataset_path=TileDataset:split=TRAIN:root=/yuhaowang/data/processed_data/private_chunk_7/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile, Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path='/yuhaowang/data/processed_data/private_chunk_7'\n",
    "image_paths=Path(data_path).rglob('*.png')\n",
    "\n",
    "def validate_img(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert(mode=\"RGB\")\n",
    "    except:\n",
    "        os.remove(path)\n",
    "        print('rm rf ',path)\n",
    "        pass\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291931"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm rf  /yuhaowang/data/processed_data/private_chunk_7/output/S24570.svs/23296x_95104y.png\n",
      "rm rf  /yuhaowang/data/processed_data/private_chunk_7/output/S25932.svs/130048x_37632y.png\n",
      "rm rf  /yuhaowang/data/processed_data/private_chunk_7/output/S26768.svs/199936x_31104y.png\n",
      "rm rf  /yuhaowang/data/processed_data/private_chunk_7/output/S22457.svs/120485x_97052y.png\n",
      "rm rf  /yuhaowang/data/processed_data/private_chunk_7/output/S25618.svs/146212x_11297y.png\n",
      "rm rf  /yuhaowang/data/processed_data/private_chunk_7/output/S25618.svs/41764x_89121y.png\n"
     ]
    }
   ],
   "source": [
    "for img_path in image_paths:\n",
    "    validate_img(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前cuda设备是 0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"当前cuda设备是\", torch.cuda.current_device()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"当前cuda设备是\", torch.cuda.current_device()) # 获取当前cuda设备\n",
    " \n",
    "a=torch.tensor([1, 2, 3, 4, 5], device='cuda') # 在当前cuda设备上创建一个张量\n",
    "print(a)\n",
    " \n",
    "device=torch.device('cuda')                    # 创建一个当前cuda设备\n",
    " \n",
    "b=torch.tensor([1, 2, 3, 4, 5], device=device) # 在当前cuda设备上创建一个张量\n",
    "print(b)\n",
    " \n",
    "c=torch.tensor([1, 2, 3, 4, 5]).to(device)     # 将张量移动到当前cuda设备\n",
    "print(c)\n",
    " \n",
    "d=torch.tensor([1, 2, 3, 4, 5]).to('cuda')     # 将张量移动到当前cuda设备\n",
    "print(d)\n",
    " \n",
    "e=torch.tensor([1, 2, 3, 4, 5]).cuda()         # 将张量移动到当前cuda设备\n",
    "print(e)\n",
    " \n",
    "f=torch.tensor([1, 2, 3, 4, 5]).cuda(device)   # 将张量移动到当前cuda设备\n",
    "print(f)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import Path, get_image_files, verify_images\n",
    "\n",
    "from typing import Any, Optional, Callable, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from dinov2.data.datasets.extended import ExtendedVisionDataset\n",
    "\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/yuhaowang/data/private_chunk_4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(root)\u001b[38;5;241m.\u001b[39mexpanduser()\n\u001b[1;32m      3\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m get_image_files(root)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "root='/yuhaowang/data/private_chunk_4'\n",
    "root = Path(root).expanduser()\n",
    "image_paths = get_image_files(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290868"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Image.open('/yuhaowang/data/private_chunk_4/output/S04168.svs/16352x_27353y.png').convert(mode=\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_image(size=(256, 256)):\n",
    "    return torch.zeros(3, *size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the Apache License, Version 2.0\n",
    "# found in the LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import logging\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the Apache License, Version 2.0\n",
    "# found in the LICENSE file in the root directory of this source tree.\n",
    "\n",
    "from typing import Sequence\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class GaussianBlur(transforms.RandomApply):\n",
    "    \"\"\"\n",
    "    Apply Gaussian Blur to the PIL image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, p: float = 0.5, radius_min: float = 0.1, radius_max: float = 2.0):\n",
    "        # NOTE: torchvision is applying 1 - probability to return the original image\n",
    "        keep_p = 1 - p\n",
    "        transform = transforms.GaussianBlur(kernel_size=9, sigma=(radius_min, radius_max))\n",
    "        super().__init__(transforms=[transform], p=keep_p)\n",
    "\n",
    "\n",
    "class MaybeToTensor(transforms.ToTensor):\n",
    "    \"\"\"\n",
    "    Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor, or keep as is if already a tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL Image, numpy.ndarray or torch.tensor): Image to be converted to tensor.\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        if isinstance(pic, torch.Tensor):\n",
    "            return pic\n",
    "        return super().__call__(pic)\n",
    "\n",
    "\n",
    "# Use timm's names\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def make_normalize_transform(\n",
    "    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n",
    "    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n",
    ") -> transforms.Normalize:\n",
    "    return transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "\n",
    "# This roughly matches torchvision's preset for classification training:\n",
    "#   https://github.com/pytorch/vision/blob/main/references/classification/presets.py#L6-L44\n",
    "def make_classification_train_transform(\n",
    "    *,\n",
    "    crop_size: int = 224,\n",
    "    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "    hflip_prob: float = 0.5,\n",
    "    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n",
    "    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n",
    "):\n",
    "    transforms_list = [transforms.RandomResizedCrop(crop_size, interpolation=interpolation)]\n",
    "    if hflip_prob > 0.0:\n",
    "        transforms_list.append(transforms.RandomHorizontalFlip(hflip_prob))\n",
    "    transforms_list.extend(\n",
    "        [\n",
    "            MaybeToTensor(),\n",
    "            make_normalize_transform(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "    return transforms.Compose(transforms_list)\n",
    "\n",
    "\n",
    "# This matches (roughly) torchvision's preset for classification evaluation:\n",
    "#   https://github.com/pytorch/vision/blob/main/references/classification/presets.py#L47-L69\n",
    "def make_classification_eval_transform(\n",
    "    *,\n",
    "    resize_size: int = 256,\n",
    "    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "    crop_size: int = 224,\n",
    "    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n",
    "    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n",
    ") -> transforms.Compose:\n",
    "    transforms_list = [\n",
    "        transforms.Resize(resize_size, interpolation=interpolation),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        MaybeToTensor(),\n",
    "        make_normalize_transform(mean=mean, std=std),\n",
    "    ]\n",
    "    return transforms.Compose(transforms_list)\n",
    "\n",
    "logger = logging.getLogger(\"dinov2\")\n",
    "\n",
    "\n",
    "class DataAugmentationDINO(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        global_crops_scale,\n",
    "        local_crops_scale,\n",
    "        local_crops_number,\n",
    "        global_crops_size=224,\n",
    "        local_crops_size=96,\n",
    "    ):\n",
    "        self.global_crops_scale = global_crops_scale\n",
    "        self.local_crops_scale = local_crops_scale\n",
    "        self.local_crops_number = local_crops_number\n",
    "        self.global_crops_size = global_crops_size\n",
    "        self.local_crops_size = local_crops_size\n",
    "\n",
    "        logger.info(\"###################################\")\n",
    "        logger.info(\"Using data augmentation parameters:\")\n",
    "        logger.info(f\"global_crops_scale: {global_crops_scale}\")\n",
    "        logger.info(f\"local_crops_scale: {local_crops_scale}\")\n",
    "        logger.info(f\"local_crops_number: {local_crops_number}\")\n",
    "        logger.info(f\"global_crops_size: {global_crops_size}\")\n",
    "        logger.info(f\"local_crops_size: {local_crops_size}\")\n",
    "        logger.info(\"###################################\")\n",
    "\n",
    "        # random resized crop and flip\n",
    "        self.geometric_augmentation_global = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(\n",
    "                    global_crops_size, scale=global_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n",
    "                ),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.geometric_augmentation_local = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(\n",
    "                    local_crops_size, scale=local_crops_scale, interpolation=transforms.InterpolationMode.BICUBIC\n",
    "                ),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # color distorsions / blurring\n",
    "        color_jittering = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomApply(\n",
    "                    [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "                    p=0.8,\n",
    "                ),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        global_transfo1_extra = GaussianBlur(p=1.0)\n",
    "\n",
    "        global_transfo2_extra = transforms.Compose(\n",
    "            [\n",
    "                GaussianBlur(p=0.1),\n",
    "                transforms.RandomSolarize(threshold=128, p=0.2),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        local_transfo_extra = GaussianBlur(p=0.5)\n",
    "\n",
    "        # normalization\n",
    "        self.normalize = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                make_normalize_transform(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.global_transfo1 = transforms.Compose([color_jittering, global_transfo1_extra, self.normalize])\n",
    "        self.global_transfo2 = transforms.Compose([color_jittering, global_transfo2_extra, self.normalize])\n",
    "        self.local_transfo = transforms.Compose([color_jittering, local_transfo_extra, self.normalize])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        output = {}\n",
    "\n",
    "        # global crops:\n",
    "        im1_base = self.geometric_augmentation_global(image)\n",
    "        global_crop_1 = self.global_transfo1(im1_base)\n",
    "\n",
    "        im2_base = self.geometric_augmentation_global(image)\n",
    "        global_crop_2 = self.global_transfo2(im2_base)\n",
    "\n",
    "        output[\"global_crops\"] = [global_crop_1, global_crop_2]\n",
    "\n",
    "        # global crops for teacher:\n",
    "        output[\"global_crops_teacher\"] = [global_crop_1, global_crop_2]\n",
    "\n",
    "        # local crops:\n",
    "        local_crops = [\n",
    "            self.local_transfo(self.geometric_augmentation_local(image)) for _ in range(self.local_crops_number)\n",
    "        ]\n",
    "        output[\"local_crops\"] = local_crops\n",
    "        output[\"offsets\"] = ()\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m target \u001b[38;5;241m=\u001b[39m get_target(\u001b[38;5;241m15000\u001b[39m)\n\u001b[1;32m      6\u001b[0m data_transform \u001b[38;5;241m=\u001b[39m DataAugmentationDINO(\n\u001b[1;32m      7\u001b[0m     global_crops_scale\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.32\u001b[39m,\u001b[38;5;241m1.0\u001b[39m],\n\u001b[1;32m      8\u001b[0m     local_crops_scale\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.32\u001b[39m],\n\u001b[1;32m      9\u001b[0m     local_crops_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m image, target \u001b[38;5;241m=\u001b[39m \u001b[43mdata_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 187\u001b[0m, in \u001b[0;36mDataAugmentationDINO.__call__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# global crops:\u001b[39;00m\n\u001b[1;32m    186\u001b[0m im1_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeometric_augmentation_global(image)\n\u001b[0;32m--> 187\u001b[0m global_crop_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_transfo1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim1_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m im2_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeometric_augmentation_global(image)\n\u001b[1;32m    190\u001b[0m global_crop_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_transfo2(im2_base)\n",
      "File \u001b[0;32m/opt/conda/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/transforms.py:547\u001b[0m, in \u001b[0;36mRandomApply.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m--> 547\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/envs/dinov2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/transforms.py:1287\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1285\u001b[0m         img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1287\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/functional.py:978\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39madjust_hue(img, hue_factor)\n\u001b[0;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/_functional_tensor.py:217\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    215\u001b[0m h \u001b[38;5;241m=\u001b[39m (h \u001b[38;5;241m+\u001b[39m hue_factor) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    216\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((h, s, v), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m--> 217\u001b[0m img_hue_adj \u001b[38;5;241m=\u001b[39m \u001b[43m_hsv2rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m convert_image_dtype(img_hue_adj, orig_dtype)\n",
      "File \u001b[0;32m/opt/conda/envs/dinov2/lib/python3.9/site-packages/torchvision/transforms/_functional_tensor.py:303\u001b[0m, in \u001b[0;36m_hsv2rgb\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_hsv2rgb\u001b[39m(img: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    302\u001b[0m     h, s, v \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39munbind(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m--> 303\u001b[0m     i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloor(\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6.0\u001b[39;49m)\n\u001b[1;32m    304\u001b[0m     f \u001b[38;5;241m=\u001b[39m (h \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m6.0\u001b[39m) \u001b[38;5;241m-\u001b[39m i\n\u001b[1;32m    305\u001b[0m     i \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_target( index: int) -> Any:\n",
    "    return 0\n",
    "\n",
    "image=default_image()\n",
    "target = get_target(15000)\n",
    "data_transform = DataAugmentationDINO(\n",
    "    global_crops_scale=[0.32,1.0],\n",
    "    local_crops_scale=[0.05,0.32],\n",
    "    local_crops_number=8\n",
    ")\n",
    "image, target = data_transform(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# 指定图像大小 (宽, 高)\n",
    "size = \n",
    "# 指定背景颜色，例如白色 (255, 255, 255) 或黑色 (0, 0, 0)\n",
    "background_color =\n",
    "\n",
    "# 创建空白图像\n",
    "\n",
    "\n",
    "# 保存或显示图像\n",
    "blank_image.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yuhao/project/FMBC/dinov2/output/eval/training_399999/teacher_checkpoint.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1133"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "len(os.listdir('/home/yuhaowang/data/TCGA-BRCA/output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1133"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('/data4/embedding/TCGA-BRCA/FMBC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = '/home/yuhaowang/data'\n",
    "processed_dir = '/data4/embedding'\n",
    "all_datasets = os.listdir(data_dir)\n",
    "processed_datasets = os.listdir(processed_dir) if os.path.exists(processed_dir) else []\n",
    "unprocessed_datasets = []\n",
    "for dataset in all_datasets:\n",
    "    if len(os.listdir(os.path.join(data_dir, dataset,'output'))) != len(os.listdir(os.path.join(processed_dir, dataset,'FMBC'))):\n",
    "\n",
    "        unprocessed_datasets.append(dataset)\n",
    "#unprocessd_dataset= [d for d in all_datasets if len(os.listdir(os.path.join(data_dir, d,'output'))) != len(os.listdir(os.path.join(processed_dir, d)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TCGA-BRCA',\n",
       " 'ACROBAT',\n",
       " 'private_chunk_7',\n",
       " 'CMB-BRCA',\n",
       " 'private_chunk_9',\n",
       " 'BRACS',\n",
       " 'TUPAC',\n",
       " 'AIDPATH',\n",
       " 'private_chunk_4',\n",
       " 'Multi-omic',\n",
       " 'BreakHis',\n",
       " 'private_chunk_8',\n",
       " 'BCNB',\n",
       " 'private_chunk_6',\n",
       " 'SLN-Breast',\n",
       " 'HyReCo',\n",
       " 'private_chunk_1',\n",
       " 'private_chunk_5',\n",
       " 'IMPRESS',\n",
       " 'TIGER',\n",
       " 'CAMELYON17',\n",
       " 'private_chunk_3',\n",
       " 'Post-NAT-BRCA',\n",
       " 'HE-vs-MPM',\n",
       " 'GTEX_Breast',\n",
       " 'CPTAC-BREAST-all',\n",
       " 'CAMELYON16',\n",
       " 'private_chunk_2',\n",
       " 'private_chunk_10']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unprocessed_datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
