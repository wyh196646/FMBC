I20250104 12:39:11 95 dinov2 config.py:67] git:
  sha: fda283ef182296a3187e17b7c6482658c6b64ee3, status: has uncommitted changes, branch: main

I20250104 12:39:11 95 dinov2 config.py:68] config_file: dinov2/configs/train/patch.yaml
eval: 
eval_only: False
local_rank: 5
no_resume: False
opts: ['train.dataset_path=TileDataset:split=TRAIN:root=/ruiyan/yuhao/data', 'train.output_dir=/ruiyan/yuhao/project/FMBC/dinov2/output']
output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
I20250104 12:39:11 95 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.006838803314120883
I20250104 12:39:11 95 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
dino:
  loss_weight: 1.0
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 380
  dataset_path: TileDataset:split=TRAIN:root=/ruiyan/yuhao/data
  output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
  saveckp_freq: 20
  seed: 0
  num_workers: 8
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 2000
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.006838803314120883
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20250104 12:39:11 95 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 12:39:12 95 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 12:39:13 95 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 768
I20250104 12:39:13 95 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20250104 12:39:13 95 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20250104 12:39:13 95 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 384
I20250104 12:39:13 95 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20250104 12:39:13 95 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20250104 12:39:13 95 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20250104 12:39:14 95 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20250104 12:39:14 95 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20250104 12:39:14 95 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250104 12:39:14 95 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250104 12:39:14 95 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20250104 12:39:14 95 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_base network.
I20250104 12:39:14 95 dinov2 ssl_meta_arch.py:396] DISTRIBUTED FSDP -- preparing model for distributed training
I20250104 12:39:14 95 dinov2 train.py:307] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
)
I20250104 12:39:14 95 dinov2 param_groups.py:54] chunked fsdp
I20250104 12:39:14 95 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.05083731656658002, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.05083731656658002, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.3.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.4.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.1.5.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.6.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.7.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.2.8.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.9.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.10.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] blocks.3.11.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 12:39:14 95 dinov2 param_groups.py:64] else code branch
I20250104 12:39:14 95 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 12:39:14 95 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 12:39:14 95 dinov2 train.py:106] Schedulers ready.
I20250104 12:39:14 95 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20250104 12:39:14 95 dinov2 augmentations.py:34] ###################################
I20250104 12:39:14 95 dinov2 augmentations.py:35] Using data augmentation parameters:
I20250104 12:39:14 95 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20250104 12:39:14 95 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20250104 12:39:14 95 dinov2 augmentations.py:38] local_crops_number: 8
I20250104 12:39:14 95 dinov2 augmentations.py:39] global_crops_size: 224
I20250104 12:39:14 95 dinov2 augmentations.py:40] local_crops_size: 96
I20250104 12:39:14 95 dinov2 augmentations.py:41] ###################################
I20250104 12:39:14 95 dinov2 loaders.py:87] using dataset: "TileDataset:split=TRAIN:root=/ruiyan/yuhao/data"
I20250104 12:46:54 95 dinov2 loaders.py:92] # of dataset samples: 1,960,591
I20250104 12:46:54 95 dinov2 loaders.py:125] sampler: sharded infinite
I20250104 12:46:54 95 dinov2 loaders.py:209] using PyTorch data loader
I20250104 12:46:54 95 dinov2 loaders.py:224] infinite data loader
I20250104 12:46:54 95 dinov2 train.py:221] Starting training from iteration 0
I20250104 12:48:44 95 dinov2 helpers.py:102] Training  [      0/2500000]  eta: 3198 days, 7:55:12  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: inf (inf)  dino_local_crops_loss: 4.6841 (4.6841)  dino_global_crops_loss: 0.5855 (0.5855)  koleo_loss: inf (inf)  ibot_loss: 1.4946 (1.4946)  time: 110.534286  data: 14.156293  max mem: 61575
I20250104 12:48:57 95 dinov2 helpers.py:102] Training  [     10/2500000]  eta: 323 days, 9:22:35  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.8513 (inf)  dino_local_crops_loss: 4.9623 (4.9458)  dino_global_crops_loss: 0.6203 (0.6182)  koleo_loss: 0.7070 (inf)  ibot_loss: 1.5086 (1.5069)  time: 11.176426  data: 1.287636  max mem: 61695
I20250104 12:49:09 95 dinov2 helpers.py:102] Training  [     20/2500000]  eta: 186 days, 20:20:37  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.9619 (inf)  dino_local_crops_loss: 5.1607 (5.0995)  dino_global_crops_loss: 0.6451 (0.6374)  koleo_loss: 0.6292 (inf)  ibot_loss: 1.5107 (1.5090)  time: 1.253668  data: 0.000650  max mem: 61695
I20250104 12:49:22 95 dinov2 helpers.py:102] Training  [     30/2500000]  eta: 138 days, 2:54:36  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.9729 (inf)  dino_local_crops_loss: 5.3146 (5.1710)  dino_global_crops_loss: 0.6643 (0.6464)  koleo_loss: 0.4844 (inf)  ibot_loss: 1.5056 (1.5051)  time: 1.251933  data: 0.000492  max mem: 61695
I20250104 12:49:34 95 dinov2 helpers.py:102] Training  [     40/2500000]  eta: 113 days, 1:53:38  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.8926 (inf)  dino_local_crops_loss: 5.3179 (5.2055)  dino_global_crops_loss: 0.6647 (0.6507)  koleo_loss: 0.4180 (inf)  ibot_loss: 1.4844 (1.4972)  time: 1.231161  data: 0.000365  max mem: 61695
I20250104 12:49:46 95 dinov2 helpers.py:102] Training  [     50/2500000]  eta: 97 days, 22:34:31  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.8134 (inf)  dino_local_crops_loss: 5.3059 (5.2246)  dino_global_crops_loss: 0.6632 (0.6531)  koleo_loss: 0.3792 (inf)  ibot_loss: 1.4569 (1.4878)  time: 1.232517  data: 0.000259  max mem: 61695
I20250104 12:49:58 95 dinov2 helpers.py:102] Training  [     60/2500000]  eta: 87 days, 13:26:22  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.7628 (inf)  dino_local_crops_loss: 5.3001 (5.2369)  dino_global_crops_loss: 0.6625 (0.6546)  koleo_loss: 0.3532 (inf)  ibot_loss: 1.4355 (1.4772)  time: 1.218200  data: 0.000470  max mem: 61695
I20250104 12:50:10 95 dinov2 helpers.py:102] Training  [     70/2500000]  eta: 80 days, 4:41:31  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.6827 (inf)  dino_local_crops_loss: 5.3001 (5.2464)  dino_global_crops_loss: 0.6624 (0.6557)  koleo_loss: 0.3088 (inf)  ibot_loss: 1.4072 (1.4649)  time: 1.207816  data: 0.000518  max mem: 61695
I20250104 12:50:25 95 dinov2 helpers.py:102] Training  [     80/2500000]  eta: 75 days, 12:52:22  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5742 (inf)  dino_local_crops_loss: 5.3078 (5.2543)  dino_global_crops_loss: 0.6628 (0.6566)  koleo_loss: 0.2331 (inf)  ibot_loss: 1.3645 (1.4508)  time: 1.343269  data: 0.000341  max mem: 61695
I20250104 12:50:38 95 dinov2 helpers.py:102] Training  [     90/2500000]  eta: 71 days, 9:36:21  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4723 (inf)  dino_local_crops_loss: 5.3049 (5.2594)  dino_global_crops_loss: 0.6625 (0.6573)  koleo_loss: 0.1611 (inf)  ibot_loss: 1.3397 (1.4381)  time: 1.388649  data: 0.000412  max mem: 61695
I20250104 12:50:52 95 dinov2 helpers.py:102] Training  [    100/2500000]  eta: 68 days, 5:26:23  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3696 (inf)  dino_local_crops_loss: 5.2941 (5.2624)  dino_global_crops_loss: 0.6616 (0.6576)  koleo_loss: 0.0845 (inf)  ibot_loss: 1.3316 (1.4274)  time: 1.334927  data: 0.027584  max mem: 61695
I20250104 12:51:05 95 dinov2 helpers.py:102] Training  [    110/2500000]  eta: 65 days, 11:16:19  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3295 (inf)  dino_local_crops_loss: 5.2861 (5.2645)  dino_global_crops_loss: 0.6609 (0.6579)  koleo_loss: 0.0510 (inf)  ibot_loss: 1.3284 (1.4179)  time: 1.330208  data: 0.027545  max mem: 61695
I20250104 12:51:20 95 dinov2 helpers.py:102] Training  [    120/2500000]  eta: 63 days, 16:40:32  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3054 (inf)  dino_local_crops_loss: 5.2866 (5.2667)  dino_global_crops_loss: 0.6610 (0.6582)  koleo_loss: 0.0392 (inf)  ibot_loss: 1.3118 (1.4086)  time: 1.410522  data: 0.086860  max mem: 61695
I20250104 12:51:33 95 dinov2 helpers.py:102] Training  [    130/2500000]  eta: 61 days, 17:49:58  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3462 (inf)  dino_local_crops_loss: 5.3002 (5.2789)  dino_global_crops_loss: 0.6629 (0.6598)  koleo_loss: 0.0328 (inf)  ibot_loss: 1.2978 (1.3993)  time: 1.419215  data: 0.139739  max mem: 61695
I20250104 12:51:47 95 dinov2 helpers.py:102] Training  [    140/2500000]  eta: 60 days, 5:07:19  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5636 (inf)  dino_local_crops_loss: 5.5653 (5.3009)  dino_global_crops_loss: 0.6943 (0.6624)  koleo_loss: 0.0280 (inf)  ibot_loss: 1.2724 (1.3897)  time: 1.353241  data: 0.134749  max mem: 61695
I20250104 12:52:00 95 dinov2 helpers.py:102] Training  [    150/2500000]  eta: 58 days, 18:35:32  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5810 (inf)  dino_local_crops_loss: 5.6022 (5.3213)  dino_global_crops_loss: 0.6990 (0.6649)  koleo_loss: 0.0282 (inf)  ibot_loss: 1.2488 (1.3800)  time: 1.359505  data: 0.135709  max mem: 61695
I20250104 12:52:15 95 dinov2 helpers.py:102] Training  [    160/2500000]  eta: 57 days, 17:11:53  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5798 (inf)  dino_local_crops_loss: 5.6156 (5.3400)  dino_global_crops_loss: 0.7011 (0.6672)  koleo_loss: 0.0294 (inf)  ibot_loss: 1.2347 (1.3703)  time: 1.386543  data: 0.148826  max mem: 61695
I20250104 12:52:28 95 dinov2 helpers.py:102] Training  [    170/2500000]  eta: 56 days, 13:46:09  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5790 (inf)  dino_local_crops_loss: 5.6256 (5.3568)  dino_global_crops_loss: 0.7022 (0.6693)  koleo_loss: 0.0298 (inf)  ibot_loss: 1.2143 (1.3608)  time: 1.381113  data: 0.142538  max mem: 61695
I20250104 12:52:41 95 dinov2 helpers.py:102] Training  [    180/2500000]  eta: 55 days, 14:18:56  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5624 (inf)  dino_local_crops_loss: 5.6293 (5.3720)  dino_global_crops_loss: 0.7027 (0.6711)  koleo_loss: 0.0305 (inf)  ibot_loss: 1.1983 (1.3515)  time: 1.331808  data: 0.101330  max mem: 61695
I20250104 12:52:55 95 dinov2 helpers.py:102] Training  [    190/2500000]  eta: 54 days, 16:36:46  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5558 (inf)  dino_local_crops_loss: 5.6328 (5.3858)  dino_global_crops_loss: 0.7030 (0.6728)  koleo_loss: 0.0308 (inf)  ibot_loss: 1.1865 (1.3427)  time: 1.334434  data: 0.115596  max mem: 61695
I20250104 12:53:09 95 dinov2 helpers.py:102] Training  [    200/2500000]  eta: 54 days, 0:13:04  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5474 (inf)  dino_local_crops_loss: 5.6372 (5.3983)  dino_global_crops_loss: 0.7035 (0.6744)  koleo_loss: 0.0308 (inf)  ibot_loss: 1.1768 (1.3341)  time: 1.370312  data: 0.161986  max mem: 61695
I20250104 12:53:22 95 dinov2 helpers.py:102] Training  [    210/2500000]  eta: 53 days, 5:35:09  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5393 (inf)  dino_local_crops_loss: 5.6391 (5.4098)  dino_global_crops_loss: 0.7036 (0.6757)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1627 (1.3259)  time: 1.358292  data: 0.146987  max mem: 61695
I20250104 12:53:35 95 dinov2 helpers.py:102] Training  [    220/2500000]  eta: 52 days, 12:04:11  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5331 (inf)  dino_local_crops_loss: 5.6407 (5.4203)  dino_global_crops_loss: 0.7037 (0.6770)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1576 (1.3182)  time: 1.291632  data: 0.059220  max mem: 61695
I20250104 12:53:48 95 dinov2 helpers.py:102] Training  [    230/2500000]  eta: 51 days, 21:39:27  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5298 (inf)  dino_local_crops_loss: 5.6422 (5.4300)  dino_global_crops_loss: 0.7039 (0.6782)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1513 (1.3108)  time: 1.308965  data: 0.012411  max mem: 61695
I20250104 12:54:02 95 dinov2 helpers.py:102] Training  [    240/2500000]  eta: 51 days, 11:30:13  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5221 (inf)  dino_local_crops_loss: 5.6430 (5.4388)  dino_global_crops_loss: 0.7040 (0.6792)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1437 (1.3037)  time: 1.388523  data: 0.000508  max mem: 61695
I20250104 12:54:15 95 dinov2 helpers.py:102] Training  [    250/2500000]  eta: 50 days, 21:49:22  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5153 (inf)  dino_local_crops_loss: 5.6420 (5.4468)  dino_global_crops_loss: 0.7037 (0.6802)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.1360 (1.2970)  time: 1.363309  data: 0.000476  max mem: 61695
I20250104 12:54:28 95 dinov2 helpers.py:102] Training  [    260/2500000]  eta: 50 days, 9:22:25  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5027 (inf)  dino_local_crops_loss: 5.6365 (5.4539)  dino_global_crops_loss: 0.7031 (0.6811)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.1340 (1.2907)  time: 1.288392  data: 0.000435  max mem: 61695
I20250104 12:54:41 95 dinov2 helpers.py:102] Training  [    270/2500000]  eta: 49 days, 22:19:38  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4861 (inf)  dino_local_crops_loss: 5.6252 (5.4599)  dino_global_crops_loss: 0.7022 (0.6818)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.1288 (1.2846)  time: 1.301294  data: 0.000520  max mem: 61695
I20250104 12:54:56 95 dinov2 helpers.py:102] Training  [    280/2500000]  eta: 49 days, 15:05:53  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4649 (inf)  dino_local_crops_loss: 5.6021 (5.4644)  dino_global_crops_loss: 0.7001 (0.6824)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1250 (1.2789)  time: 1.372086  data: 0.000648  max mem: 61695
I20250104 12:55:09 95 dinov2 helpers.py:102] Training  [    290/2500000]  eta: 49 days, 6:05:42  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4324 (inf)  dino_local_crops_loss: 5.5600 (5.4665)  dino_global_crops_loss: 0.6959 (0.6828)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1226 (1.2735)  time: 1.385845  data: 0.000692  max mem: 61695
I20250104 12:55:23 95 dinov2 helpers.py:102] Training  [    300/2500000]  eta: 48 days, 23:59:04  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3170 (inf)  dino_local_crops_loss: 5.4743 (5.4644)  dino_global_crops_loss: 0.6866 (0.6826)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1174 (1.2683)  time: 1.387999  data: 0.000668  max mem: 61695
I20250104 12:55:37 95 dinov2 helpers.py:102] Training  [    310/2500000]  eta: 48 days, 16:29:31  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.1142 (inf)  dino_local_crops_loss: 5.2990 (5.4551)  dino_global_crops_loss: 0.6672 (0.6817)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1166 (1.2634)  time: 1.397985  data: 0.075190  max mem: 61695
I20250104 12:55:52 95 dinov2 helpers.py:102] Training  [    320/2500000]  eta: 48 days, 12:32:01  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 6.8622 (inf)  dino_local_crops_loss: 4.9936 (5.4367)  dino_global_crops_loss: 0.6334 (0.6797)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1125 (1.2587)  time: 1.429149  data: 0.222726  max mem: 61695
I20250104 12:56:06 95 dinov2 helpers.py:102] Training  [    330/2500000]  eta: 48 days, 6:25:01  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 6.5121 (inf)  dino_local_crops_loss: 4.7092 (5.4125)  dino_global_crops_loss: 0.6015 (0.6771)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1126 (1.2543)  time: 1.442924  data: 0.234739  max mem: 61695
I20250104 12:56:20 95 dinov2 helpers.py:102] Training  [    340/2500000]  eta: 48 days, 0:20:17  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 6.3390 (inf)  dino_local_crops_loss: 4.6126 (5.3885)  dino_global_crops_loss: 0.5894 (0.6745)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1121 (1.2501)  time: 1.377910  data: 0.166650  max mem: 61695
I20250104 12:56:33 95 dinov2 helpers.py:102] Training  [    350/2500000]  eta: 47 days, 17:52:58  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 6.3727 (inf)  dino_local_crops_loss: 4.6399 (5.3706)  dino_global_crops_loss: 0.5936 (0.6726)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1100 (1.2461)  time: 1.351766  data: 0.140637  max mem: 61695
I20250104 12:56:49 95 dinov2 helpers.py:102] Training  [    360/2500000]  eta: 47 days, 16:52:48  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 6.7405 (inf)  dino_local_crops_loss: 4.8965 (5.3631)  dino_global_crops_loss: 0.6216 (0.6718)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1080 (1.2422)  time: 1.465953  data: 0.250671  max mem: 61695
I20250104 12:57:03 95 dinov2 helpers.py:102] Training  [    370/2500000]  eta: 47 days, 12:19:16  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.0848 (inf)  dino_local_crops_loss: 5.2576 (5.3636)  dino_global_crops_loss: 0.6618 (0.6719)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1076 (1.2386)  time: 1.501965  data: 0.288872  max mem: 61695
I20250104 12:57:17 95 dinov2 helpers.py:102] Training  [    380/2500000]  eta: 47 days, 7:02:35  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2864 (inf)  dino_local_crops_loss: 5.4641 (5.3680)  dino_global_crops_loss: 0.6833 (0.6724)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1061 (1.2351)  time: 1.379225  data: 0.173944  max mem: 61695
I20250104 12:57:30 95 dinov2 helpers.py:102] Training  [    390/2500000]  eta: 47 days, 2:19:48  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4062 (inf)  dino_local_crops_loss: 5.5733 (5.3742)  dino_global_crops_loss: 0.6946 (0.6730)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.1051 (1.2317)  time: 1.361245  data: 0.156716  max mem: 61695
I20250104 12:57:46 95 dinov2 helpers.py:102] Training  [    400/2500000]  eta: 47 days, 1:15:29  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4665 (inf)  dino_local_crops_loss: 5.6324 (5.3811)  dino_global_crops_loss: 0.6999 (0.6737)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1020 (1.2285)  time: 1.467922  data: 0.259863  max mem: 61695
I20250104 12:57:59 95 dinov2 helpers.py:102] Training  [    410/2500000]  eta: 46 days, 20:36:03  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5009 (inf)  dino_local_crops_loss: 5.6674 (5.3883)  dino_global_crops_loss: 0.7031 (0.6745)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1007 (1.2254)  time: 1.458634  data: 0.248360  max mem: 61695
I20250104 12:58:13 95 dinov2 helpers.py:102] Training  [    420/2500000]  eta: 46 days, 16:29:39  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5292 (inf)  dino_local_crops_loss: 5.6870 (5.3957)  dino_global_crops_loss: 0.7047 (0.6752)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1004 (1.2224)  time: 1.360971  data: 0.152662  max mem: 61695
I20250104 12:58:27 95 dinov2 helpers.py:102] Training  [    430/2500000]  eta: 46 days, 12:13:07  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5416 (inf)  dino_local_crops_loss: 5.7028 (5.4030)  dino_global_crops_loss: 0.7052 (0.6759)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1018 (1.2196)  time: 1.359812  data: 0.161330  max mem: 61695
I20250104 12:58:41 95 dinov2 helpers.py:102] Training  [    440/2500000]  eta: 46 days, 9:55:51  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5547 (inf)  dino_local_crops_loss: 5.7180 (5.4103)  dino_global_crops_loss: 0.7057 (0.6766)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1002 (1.2169)  time: 1.405635  data: 0.213392  max mem: 61695
I20250104 12:58:55 95 dinov2 helpers.py:102] Training  [    450/2500000]  eta: 46 days, 6:21:47  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5643 (inf)  dino_local_crops_loss: 5.7273 (5.4173)  dino_global_crops_loss: 0.7063 (0.6773)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0980 (1.2143)  time: 1.417740  data: 0.222461  max mem: 61695
I20250104 12:59:08 95 dinov2 helpers.py:102] Training  [    460/2500000]  eta: 46 days, 2:32:04  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5691 (inf)  dino_local_crops_loss: 5.7319 (5.4242)  dino_global_crops_loss: 0.7062 (0.6779)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0979 (1.2118)  time: 1.359093  data: 0.155635  max mem: 61695
I20250104 12:59:22 95 dinov2 helpers.py:102] Training  [    470/2500000]  eta: 45 days, 23:34:44  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5736 (inf)  dino_local_crops_loss: 5.7357 (5.4309)  dino_global_crops_loss: 0.7061 (0.6785)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0977 (1.2093)  time: 1.369408  data: 0.172725  max mem: 61695
I20250104 12:59:37 95 dinov2 helpers.py:102] Training  [    480/2500000]  eta: 45 days, 22:22:01  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5745 (inf)  dino_local_crops_loss: 5.7409 (5.4374)  dino_global_crops_loss: 0.7059 (0.6791)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0953 (1.2070)  time: 1.449654  data: 0.259635  max mem: 61695
I20250104 12:59:51 95 dinov2 helpers.py:102] Training  [    490/2500000]  eta: 45 days, 18:47:24  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5751 (inf)  dino_local_crops_loss: 5.7419 (5.4436)  dino_global_crops_loss: 0.7056 (0.6796)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0954 (1.2047)  time: 1.420439  data: 0.218430  max mem: 61695
I20250104 13:00:04 95 dinov2 helpers.py:102] Training  [    500/2500000]  eta: 45 days, 15:39:52  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5734 (inf)  dino_local_crops_loss: 5.7399 (5.4496)  dino_global_crops_loss: 0.7050 (0.6801)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0958 (1.2025)  time: 1.346210  data: 0.141304  max mem: 61695
I20250104 13:00:18 95 dinov2 helpers.py:102] Training  [    510/2500000]  eta: 45 days, 12:27:54  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5729 (inf)  dino_local_crops_loss: 5.7394 (5.4552)  dino_global_crops_loss: 0.7046 (0.6806)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0958 (1.2005)  time: 1.350134  data: 0.139188  max mem: 61695
I20250104 13:00:33 95 dinov2 helpers.py:102] Training  [    520/2500000]  eta: 45 days, 11:23:39  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5719 (inf)  dino_local_crops_loss: 5.7385 (5.4607)  dino_global_crops_loss: 0.7043 (0.6810)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0958 (1.1984)  time: 1.418183  data: 0.209877  max mem: 61695
I20250104 13:00:46 95 dinov2 helpers.py:102] Training  [    530/2500000]  eta: 45 days, 8:26:08  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5719 (inf)  dino_local_crops_loss: 5.7389 (5.4659)  dino_global_crops_loss: 0.7038 (0.6815)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0951 (1.1965)  time: 1.419716  data: 0.223299  max mem: 61695
I20250104 13:01:00 95 dinov2 helpers.py:102] Training  [    540/2500000]  eta: 45 days, 6:00:42  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5701 (inf)  dino_local_crops_loss: 5.7389 (5.4710)  dino_global_crops_loss: 0.7032 (0.6819)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0952 (1.1947)  time: 1.362569  data: 0.157691  max mem: 61695
I20250104 13:01:14 95 dinov2 helpers.py:102] Training  [    550/2500000]  eta: 45 days, 3:33:08  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5683 (inf)  dino_local_crops_loss: 5.7359 (5.4758)  dino_global_crops_loss: 0.7031 (0.6823)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0985 (1.1929)  time: 1.374254  data: 0.167253  max mem: 61695
I20250104 13:01:28 95 dinov2 helpers.py:102] Training  [    560/2500000]  eta: 45 days, 2:05:18  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5683 (inf)  dino_local_crops_loss: 5.7363 (5.4805)  dino_global_crops_loss: 0.7030 (0.6826)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0991 (1.1912)  time: 1.406034  data: 0.206125  max mem: 61695
I20250104 13:01:42 95 dinov2 helpers.py:102] Training  [    570/2500000]  eta: 44 days, 23:56:41  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5672 (inf)  dino_local_crops_loss: 5.7373 (5.4850)  dino_global_crops_loss: 0.7029 (0.6830)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0951 (1.1895)  time: 1.412670  data: 0.212440  max mem: 61695
I20250104 13:01:55 95 dinov2 helpers.py:102] Training  [    580/2500000]  eta: 44 days, 21:34:29  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5636 (inf)  dino_local_crops_loss: 5.7367 (5.4893)  dino_global_crops_loss: 0.7029 (0.6833)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0931 (1.1878)  time: 1.370070  data: 0.169271  max mem: 61695
I20250104 13:02:09 95 dinov2 helpers.py:102] Training  [    590/2500000]  eta: 44 days, 18:53:20  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5652 (inf)  dino_local_crops_loss: 5.7345 (5.4935)  dino_global_crops_loss: 0.7029 (0.6837)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0947 (1.1863)  time: 1.340652  data: 0.138866  max mem: 61695
I20250104 13:02:24 95 dinov2 helpers.py:102] Training  [    600/2500000]  eta: 44 days, 18:49:46  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5674 (inf)  dino_local_crops_loss: 5.7371 (5.4976)  dino_global_crops_loss: 0.7029 (0.6840)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0947 (1.1847)  time: 1.433618  data: 0.231590  max mem: 61695
I20250104 13:02:38 95 dinov2 helpers.py:102] Training  [    610/2500000]  eta: 44 days, 17:28:03  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5659 (inf)  dino_local_crops_loss: 5.7378 (5.5015)  dino_global_crops_loss: 0.7025 (0.6843)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0930 (1.1833)  time: 1.486044  data: 0.274216  max mem: 61695
I20250104 13:02:51 95 dinov2 helpers.py:102] Training  [    620/2500000]  eta: 44 days, 14:35:55  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5646 (inf)  dino_local_crops_loss: 5.7358 (5.5052)  dino_global_crops_loss: 0.7024 (0.6846)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0962 (1.1819)  time: 1.359297  data: 0.143982  max mem: 61695
I20250104 13:03:05 95 dinov2 helpers.py:102] Training  [    630/2500000]  eta: 44 days, 12:16:17  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5634 (inf)  dino_local_crops_loss: 5.7358 (5.5089)  dino_global_crops_loss: 0.7022 (0.6848)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0958 (1.1805)  time: 1.310425  data: 0.105359  max mem: 61695
I20250104 13:03:19 95 dinov2 helpers.py:102] Training  [    640/2500000]  eta: 44 days, 11:05:05  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5639 (inf)  dino_local_crops_loss: 5.7382 (5.5125)  dino_global_crops_loss: 0.7024 (0.6851)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0937 (1.1791)  time: 1.380220  data: 0.177208  max mem: 61695
I20250104 13:03:33 95 dinov2 helpers.py:102] Training  [    650/2500000]  eta: 44 days, 9:19:40  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5663 (inf)  dino_local_crops_loss: 5.7372 (5.5159)  dino_global_crops_loss: 0.7029 (0.6854)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0953 (1.1778)  time: 1.401093  data: 0.196615  max mem: 61695
I20250104 13:03:47 95 dinov2 helpers.py:102] Training  [    660/2500000]  eta: 44 days, 8:19:02  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5663 (inf)  dino_local_crops_loss: 5.7385 (5.5193)  dino_global_crops_loss: 0.7029 (0.6857)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0932 (1.1766)  time: 1.405657  data: 0.193819  max mem: 61695
I20250104 13:04:01 95 dinov2 helpers.py:102] Training  [    670/2500000]  eta: 44 days, 6:47:38  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5632 (inf)  dino_local_crops_loss: 5.7375 (5.5225)  dino_global_crops_loss: 0.7024 (0.6859)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0916 (1.1753)  time: 1.412435  data: 0.199124  max mem: 61695
I20250104 13:04:16 95 dinov2 helpers.py:102] Training  [    680/2500000]  eta: 44 days, 6:12:36  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5637 (inf)  dino_local_crops_loss: 5.7382 (5.5257)  dino_global_crops_loss: 0.7023 (0.6861)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0919 (1.1741)  time: 1.430097  data: 0.221150  max mem: 61695
I20250104 13:04:29 95 dinov2 helpers.py:102] Training  [    690/2500000]  eta: 44 days, 3:54:20  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5647 (inf)  dino_local_crops_loss: 5.7378 (5.5287)  dino_global_crops_loss: 0.7024 (0.6864)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0948 (1.1730)  time: 1.387513  data: 0.178500  max mem: 61695
I20250104 13:04:42 95 dinov2 helpers.py:102] Training  [    700/2500000]  eta: 44 days, 1:40:00  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5667 (inf)  dino_local_crops_loss: 5.7381 (5.5318)  dino_global_crops_loss: 0.7026 (0.6866)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0938 (1.1718)  time: 1.301048  data: 0.095954  max mem: 61695
I20250104 13:04:54 95 dinov2 helpers.py:102] Training  [    710/2500000]  eta: 43 days, 23:07:04  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5656 (inf)  dino_local_crops_loss: 5.7395 (5.5347)  dino_global_crops_loss: 0.7025 (0.6868)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0928 (1.1707)  time: 1.281955  data: 0.078266  max mem: 61695
I20250104 13:05:09 95 dinov2 helpers.py:102] Training  [    720/2500000]  eta: 43 days, 22:53:00  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5656 (inf)  dino_local_crops_loss: 5.7398 (5.5375)  dino_global_crops_loss: 0.7026 (0.6871)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0914 (1.1696)  time: 1.379375  data: 0.170245  max mem: 61695
I20250104 13:05:22 95 dinov2 helpers.py:102] Training  [    730/2500000]  eta: 43 days, 20:48:36  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5662 (inf)  dino_local_crops_loss: 5.7400 (5.5403)  dino_global_crops_loss: 0.7030 (0.6873)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0917 (1.1686)  time: 1.398750  data: 0.185276  max mem: 61695
I20250104 13:05:35 95 dinov2 helpers.py:102] Training  [    740/2500000]  eta: 43 days, 18:43:53  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5662 (inf)  dino_local_crops_loss: 5.7371 (5.5429)  dino_global_crops_loss: 0.7030 (0.6875)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0957 (1.1676)  time: 1.298339  data: 0.088209  max mem: 61695
I20250104 13:05:48 95 dinov2 helpers.py:102] Training  [    750/2500000]  eta: 43 days, 16:43:51  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5668 (inf)  dino_local_crops_loss: 5.7364 (5.5456)  dino_global_crops_loss: 0.7026 (0.6877)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0969 (1.1666)  time: 1.296310  data: 0.092570  max mem: 61695
I20250104 13:06:02 95 dinov2 helpers.py:102] Training  [    760/2500000]  eta: 43 days, 15:28:16  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5684 (inf)  dino_local_crops_loss: 5.7383 (5.5481)  dino_global_crops_loss: 0.7029 (0.6879)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0953 (1.1657)  time: 1.335272  data: 0.127643  max mem: 61695
I20250104 13:06:15 95 dinov2 helpers.py:102] Training  [    770/2500000]  eta: 43 days, 13:37:29  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5698 (inf)  dino_local_crops_loss: 5.7424 (5.5507)  dino_global_crops_loss: 0.7032 (0.6881)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0937 (1.1648)  time: 1.338611  data: 0.129000  max mem: 61695
I20250104 13:06:28 95 dinov2 helpers.py:102] Training  [    780/2500000]  eta: 43 days, 11:36:35  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5708 (inf)  dino_local_crops_loss: 5.7428 (5.5531)  dino_global_crops_loss: 0.7034 (0.6883)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0936 (1.1638)  time: 1.292086  data: 0.097178  max mem: 61695
I20250104 13:06:41 95 dinov2 helpers.py:102] Training  [    790/2500000]  eta: 43 days, 9:56:37  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5731 (inf)  dino_local_crops_loss: 5.7428 (5.5555)  dino_global_crops_loss: 0.7030 (0.6885)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0950 (1.1630)  time: 1.296932  data: 0.095203  max mem: 61695
I20250104 13:06:55 95 dinov2 helpers.py:102] Training  [    800/2500000]  eta: 43 days, 8:42:05  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5725 (inf)  dino_local_crops_loss: 5.7428 (5.5579)  dino_global_crops_loss: 0.7033 (0.6887)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0942 (1.1621)  time: 1.335958  data: 0.090729  max mem: 61695
I20250104 13:07:08 95 dinov2 helpers.py:102] Training  [    810/2500000]  eta: 43 days, 7:22:56  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5705 (inf)  dino_local_crops_loss: 5.7423 (5.5601)  dino_global_crops_loss: 0.7032 (0.6888)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0930 (1.1612)  time: 1.351741  data: 0.043032  max mem: 61695
I20250104 13:07:23 95 dinov2 helpers.py:102] Training  [    820/2500000]  eta: 43 days, 7:10:20  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5692 (inf)  dino_local_crops_loss: 5.7414 (5.5624)  dino_global_crops_loss: 0.7027 (0.6890)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0911 (1.1604)  time: 1.409171  data: 0.009030  max mem: 61695
I20250104 13:07:37 95 dinov2 helpers.py:102] Training  [    830/2500000]  eta: 43 days, 6:27:01  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5699 (inf)  dino_local_crops_loss: 5.7418 (5.5645)  dino_global_crops_loss: 0.7033 (0.6892)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0943 (1.1596)  time: 1.441921  data: 0.009262  max mem: 61695
I20250104 13:07:51 95 dinov2 helpers.py:102] Training  [    840/2500000]  eta: 43 days, 5:41:56  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5704 (inf)  dino_local_crops_loss: 5.7418 (5.5666)  dino_global_crops_loss: 0.7033 (0.6894)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0941 (1.1588)  time: 1.408165  data: 0.000640  max mem: 61695
I20250104 13:08:08 95 dinov2 helpers.py:102] Training  [    850/2500000]  eta: 43 days, 7:17:05  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5714 (inf)  dino_local_crops_loss: 5.7425 (5.5687)  dino_global_crops_loss: 0.7031 (0.6895)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0938 (1.1580)  time: 1.547519  data: 0.095387  max mem: 61695
I20250104 13:08:20 95 dinov2 helpers.py:102] Training  [    860/2500000]  eta: 43 days, 5:26:00  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5714 (inf)  dino_local_crops_loss: 5.7434 (5.5708)  dino_global_crops_loss: 0.7037 (0.6897)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0934 (1.1573)  time: 1.478821  data: 0.095339  max mem: 61695
I20250104 13:08:34 95 dinov2 helpers.py:102] Training  [    870/2500000]  eta: 43 days, 4:00:12  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5712 (inf)  dino_local_crops_loss: 5.7438 (5.5728)  dino_global_crops_loss: 0.7037 (0.6898)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0923 (1.1565)  time: 1.291747  data: 0.000530  max mem: 61695
I20250104 13:08:47 95 dinov2 helpers.py:102] Training  [    880/2500000]  eta: 43 days, 2:26:10  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5731 (inf)  dino_local_crops_loss: 5.7442 (5.5747)  dino_global_crops_loss: 0.7035 (0.6900)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0939 (1.1559)  time: 1.304761  data: 0.000496  max mem: 61695
I20250104 13:09:00 95 dinov2 helpers.py:102] Training  [    890/2500000]  eta: 43 days, 1:02:02  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5756 (inf)  dino_local_crops_loss: 5.7448 (5.5766)  dino_global_crops_loss: 0.7038 (0.6901)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0926 (1.1551)  time: 1.302325  data: 0.000332  max mem: 61695
I20250104 13:09:13 95 dinov2 helpers.py:102] Training  [    900/2500000]  eta: 43 days, 0:03:36  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5746 (inf)  dino_local_crops_loss: 5.7448 (5.5785)  dino_global_crops_loss: 0.7036 (0.6903)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0925 (1.1544)  time: 1.336459  data: 0.000280  max mem: 61695
I20250104 13:09:26 95 dinov2 helpers.py:102] Training  [    910/2500000]  eta: 42 days, 22:29:11  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5730 (inf)  dino_local_crops_loss: 5.7450 (5.5804)  dino_global_crops_loss: 0.7035 (0.6904)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0910 (1.1537)  time: 1.321482  data: 0.001166  max mem: 61695
I20250104 13:09:39 95 dinov2 helpers.py:102] Training  [    920/2500000]  eta: 42 days, 20:55:00  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5729 (inf)  dino_local_crops_loss: 5.7445 (5.5821)  dino_global_crops_loss: 0.7036 (0.6906)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0909 (1.1531)  time: 1.278732  data: 0.001311  max mem: 61695
I20250104 13:09:52 95 dinov2 helpers.py:102] Training  [    930/2500000]  eta: 42 days, 19:39:26  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5764 (inf)  dino_local_crops_loss: 5.7454 (5.5839)  dino_global_crops_loss: 0.7036 (0.6907)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0952 (1.1525)  time: 1.295281  data: 0.000613  max mem: 61695
I20250104 13:10:06 95 dinov2 helpers.py:102] Training  [    940/2500000]  eta: 42 days, 18:57:11  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5762 (inf)  dino_local_crops_loss: 5.7478 (5.5856)  dino_global_crops_loss: 0.7034 (0.6909)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0939 (1.1518)  time: 1.349657  data: 0.000586  max mem: 61695
I20250104 13:10:19 95 dinov2 helpers.py:102] Training  [    950/2500000]  eta: 42 days, 17:38:04  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5762 (inf)  dino_local_crops_loss: 5.7475 (5.5873)  dino_global_crops_loss: 0.7034 (0.6910)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0941 (1.1512)  time: 1.342393  data: 0.000521  max mem: 61695
I20250104 13:10:32 95 dinov2 helpers.py:102] Training  [    960/2500000]  eta: 42 days, 16:11:17  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5762 (inf)  dino_local_crops_loss: 5.7474 (5.5890)  dino_global_crops_loss: 0.7036 (0.6911)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0939 (1.1506)  time: 1.288540  data: 0.000509  max mem: 61695
I20250104 13:10:45 95 dinov2 helpers.py:102] Training  [    970/2500000]  eta: 42 days, 15:03:23  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5762 (inf)  dino_local_crops_loss: 5.7470 (5.5906)  dino_global_crops_loss: 0.7038 (0.6913)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0945 (1.1501)  time: 1.297733  data: 0.000396  max mem: 61695
I20250104 13:10:58 95 dinov2 helpers.py:102] Training  [    980/2500000]  eta: 42 days, 14:14:26  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5782 (inf)  dino_local_crops_loss: 5.7461 (5.5923)  dino_global_crops_loss: 0.7041 (0.6914)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0945 (1.1495)  time: 1.338377  data: 0.000381  max mem: 61695
I20250104 13:11:12 95 dinov2 helpers.py:102] Training  [    990/2500000]  eta: 42 days, 13:07:36  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5779 (inf)  dino_local_crops_loss: 5.7481 (5.5938)  dino_global_crops_loss: 0.7042 (0.6915)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0945 (1.1490)  time: 1.336625  data: 0.000518  max mem: 61695
I20250104 13:11:24 95 dinov2 helpers.py:102] Training  [   1000/2500000]  eta: 42 days, 11:39:58  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5766 (inf)  dino_local_crops_loss: 5.7473 (5.5954)  dino_global_crops_loss: 0.7038 (0.6916)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0957 (1.1484)  time: 1.287576  data: 0.000537  max mem: 61695
I20250104 13:11:37 95 dinov2 helpers.py:102] Training  [   1010/2500000]  eta: 42 days, 10:42:38  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5780 (inf)  dino_local_crops_loss: 5.7467 (5.5969)  dino_global_crops_loss: 0.7037 (0.6918)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0957 (1.1479)  time: 1.295641  data: 0.000453  max mem: 61695
I20250104 13:11:52 95 dinov2 helpers.py:102] Training  [   1020/2500000]  eta: 42 days, 10:25:04  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5787 (inf)  dino_local_crops_loss: 5.7483 (5.5984)  dino_global_crops_loss: 0.7036 (0.6919)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0944 (1.1474)  time: 1.377683  data: 0.001066  max mem: 61695
I20250104 13:12:05 95 dinov2 helpers.py:102] Training  [   1030/2500000]  eta: 42 days, 9:19:09  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5781 (inf)  dino_local_crops_loss: 5.7499 (5.5998)  dino_global_crops_loss: 0.7038 (0.6920)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0938 (1.1468)  time: 1.364828  data: 0.024128  max mem: 61695
I20250104 13:12:18 95 dinov2 helpers.py:102] Training  [   1040/2500000]  eta: 42 days, 8:14:30  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5763 (inf)  dino_local_crops_loss: 5.7491 (5.6012)  dino_global_crops_loss: 0.7039 (0.6921)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0946 (1.1463)  time: 1.304560  data: 0.070987  max mem: 61695
I20250104 13:12:31 95 dinov2 helpers.py:102] Training  [   1050/2500000]  eta: 42 days, 7:09:17  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5766 (inf)  dino_local_crops_loss: 5.7478 (5.6026)  dino_global_crops_loss: 0.7040 (0.6922)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0940 (1.1458)  time: 1.302293  data: 0.094805  max mem: 61695
I20250104 13:12:45 95 dinov2 helpers.py:102] Training  [   1060/2500000]  eta: 42 days, 6:56:36  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5783 (inf)  dino_local_crops_loss: 5.7473 (5.6040)  dino_global_crops_loss: 0.7041 (0.6923)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0934 (1.1453)  time: 1.365412  data: 0.154970  max mem: 61695
I20250104 13:12:58 95 dinov2 helpers.py:102] Training  [   1070/2500000]  eta: 42 days, 6:08:17  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5784 (inf)  dino_local_crops_loss: 5.7473 (5.6054)  dino_global_crops_loss: 0.7037 (0.6924)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0934 (1.1449)  time: 1.384636  data: 0.173165  max mem: 61695
I20250104 13:13:12 95 dinov2 helpers.py:102] Training  [   1080/2500000]  eta: 42 days, 5:09:23  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5780 (inf)  dino_local_crops_loss: 5.7469 (5.6067)  dino_global_crops_loss: 0.7036 (0.6925)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0959 (1.1444)  time: 1.323616  data: 0.110848  max mem: 61695
I20250104 13:13:24 95 dinov2 helpers.py:102] Training  [   1090/2500000]  eta: 42 days, 4:03:49  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5823 (inf)  dino_local_crops_loss: 5.7478 (5.6080)  dino_global_crops_loss: 0.7041 (0.6927)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0978 (1.1440)  time: 1.298610  data: 0.091304  max mem: 61695
I20250104 13:13:39 95 dinov2 helpers.py:102] Training  [   1100/2500000]  eta: 42 days, 3:53:45  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5817 (inf)  dino_local_crops_loss: 5.7492 (5.6093)  dino_global_crops_loss: 0.7044 (0.6928)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0946 (1.1435)  time: 1.360251  data: 0.160228  max mem: 61695
I20250104 13:13:52 95 dinov2 helpers.py:102] Training  [   1110/2500000]  eta: 42 days, 3:05:42  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5792 (inf)  dino_local_crops_loss: 5.7481 (5.6105)  dino_global_crops_loss: 0.7041 (0.6929)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0942 (1.1431)  time: 1.381139  data: 0.178930  max mem: 61695
I20250104 13:14:06 95 dinov2 helpers.py:102] Training  [   1120/2500000]  eta: 42 days, 2:35:02  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5821 (inf)  dino_local_crops_loss: 5.7507 (5.6118)  dino_global_crops_loss: 0.7042 (0.6930)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0946 (1.1427)  time: 1.352486  data: 0.148202  max mem: 61695
I20250104 13:14:21 95 dinov2 helpers.py:102] Training  [   1130/2500000]  eta: 42 days, 2:48:39  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5810 (inf)  dino_local_crops_loss: 5.7516 (5.6130)  dino_global_crops_loss: 0.7042 (0.6931)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0948 (1.1423)  time: 1.434134  data: 0.224494  max mem: 61695
I20250104 13:14:36 95 dinov2 helpers.py:102] Training  [   1140/2500000]  eta: 42 days, 2:57:38  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5808 (inf)  dino_local_crops_loss: 5.7505 (5.6142)  dino_global_crops_loss: 0.7041 (0.6932)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0949 (1.1419)  time: 1.487517  data: 0.283210  max mem: 61695
I20250104 13:14:49 95 dinov2 helpers.py:102] Training  [   1150/2500000]  eta: 42 days, 2:08:01  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5793 (inf)  dino_local_crops_loss: 5.7487 (5.6154)  dino_global_crops_loss: 0.7037 (0.6932)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0950 (1.1415)  time: 1.400742  data: 0.195148  max mem: 61695
I20250104 13:15:02 95 dinov2 helpers.py:102] Training  [   1160/2500000]  eta: 42 days, 1:25:33  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5793 (inf)  dino_local_crops_loss: 5.7493 (5.6165)  dino_global_crops_loss: 0.7042 (0.6933)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0950 (1.1411)  time: 1.328785  data: 0.107251  max mem: 61695
I20250104 13:15:15 95 dinov2 helpers.py:102] Training  [   1170/2500000]  eta: 42 days, 0:40:30  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5816 (inf)  dino_local_crops_loss: 5.7507 (5.6177)  dino_global_crops_loss: 0.7044 (0.6934)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0934 (1.1407)  time: 1.332895  data: 0.112029  max mem: 61695
I20250104 13:15:31 95 dinov2 helpers.py:102] Training  [   1180/2500000]  eta: 42 days, 1:14:52  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5828 (inf)  dino_local_crops_loss: 5.7518 (5.6188)  dino_global_crops_loss: 0.7044 (0.6935)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0929 (1.1403)  time: 1.439783  data: 0.223753  max mem: 61695
I20250104 13:15:44 95 dinov2 helpers.py:102] Training  [   1190/2500000]  eta: 42 days, 0:24:10  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5807 (inf)  dino_local_crops_loss: 5.7498 (5.6199)  dino_global_crops_loss: 0.7039 (0.6936)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0959 (1.1399)  time: 1.430529  data: 0.219366  max mem: 61695
I20250104 13:15:58 95 dinov2 helpers.py:102] Training  [   1200/2500000]  eta: 41 days, 23:49:14  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5814 (inf)  dino_local_crops_loss: 5.7500 (5.6210)  dino_global_crops_loss: 0.7040 (0.6937)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0953 (1.1395)  time: 1.331225  data: 0.119322  max mem: 61695
I20250104 13:16:11 95 dinov2 helpers.py:102] Training  [   1210/2500000]  eta: 41 days, 23:04:45  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5814 (inf)  dino_local_crops_loss: 5.7514 (5.6221)  dino_global_crops_loss: 0.7044 (0.6938)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0944 (1.1392)  time: 1.338026  data: 0.120095  max mem: 61695
I20250104 13:16:25 95 dinov2 helpers.py:102] Training  [   1220/2500000]  eta: 41 days, 22:56:11  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5803 (inf)  dino_local_crops_loss: 5.7512 (5.6232)  dino_global_crops_loss: 0.7046 (0.6939)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0944 (1.1388)  time: 1.374907  data: 0.166234  max mem: 61695
I20250104 13:16:38 95 dinov2 helpers.py:102] Training  [   1230/2500000]  eta: 41 days, 22:16:29  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5802 (inf)  dino_local_crops_loss: 5.7527 (5.6242)  dino_global_crops_loss: 0.7043 (0.6940)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0924 (1.1384)  time: 1.380268  data: 0.177750  max mem: 61695
I20250104 13:16:52 95 dinov2 helpers.py:102] Training  [   1240/2500000]  eta: 41 days, 21:34:34  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5798 (inf)  dino_local_crops_loss: 5.7523 (5.6252)  dino_global_crops_loss: 0.7042 (0.6940)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0915 (1.1381)  time: 1.329817  data: 0.119215  max mem: 61695
I20250104 13:17:05 95 dinov2 helpers.py:102] Training  [   1250/2500000]  eta: 41 days, 20:52:29  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5859 (inf)  dino_local_crops_loss: 5.7496 (5.6262)  dino_global_crops_loss: 0.7039 (0.6941)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0972 (1.1378)  time: 1.324312  data: 0.118064  max mem: 61695
I20250104 13:17:19 95 dinov2 helpers.py:102] Training  [   1260/2500000]  eta: 41 days, 20:36:55  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0007 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5860 (inf)  dino_local_crops_loss: 5.7521 (5.6272)  dino_global_crops_loss: 0.7043 (0.6942)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0972 (1.1375)  time: 1.362188  data: 0.166266  max mem: 61695
I20250104 13:17:32 95 dinov2 helpers.py:102] Training  [   1270/2500000]  eta: 41 days, 20:05:30  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0007 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5849 (inf)  dino_local_crops_loss: 5.7521 (5.6282)  dino_global_crops_loss: 0.7045 (0.6943)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0953 (1.1371)  time: 1.376792  data: 0.164062  max mem: 61695
I20250104 13:17:37 95 dinov2 train.py:278] NaN detected
I20250104 13:21:37 95 dinov2 config.py:67] git:
  sha: fda283ef182296a3187e17b7c6482658c6b64ee3, status: has uncommitted changes, branch: main

I20250104 13:21:37 95 dinov2 config.py:68] config_file: dinov2/configs/train/patch.yaml
eval: 
eval_only: False
local_rank: 5
no_resume: False
opts: ['train.dataset_path=TileDataset:split=TRAIN:root=/ruiyan/yuhao/data', 'train.output_dir=/ruiyan/yuhao/project/FMBC/dinov2/output']
output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
I20250104 13:21:37 95 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.006928203230275509
I20250104 13:21:37 95 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
dino:
  loss_weight: 1.0
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 380
  dataset_path: TileDataset:split=TRAIN:root=/ruiyan/yuhao/data
  output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
  saveckp_freq: 20
  seed: 0
  num_workers: 8
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 2000
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.006928203230275509
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20250104 13:21:37 95 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 13:21:38 95 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 768
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 384
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20250104 13:21:39 95 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_base network.
I20250104 13:21:40 95 dinov2 ssl_meta_arch.py:396] DISTRIBUTED FSDP -- preparing model for distributed training
I20250104 13:21:40 95 dinov2 train.py:307] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
)
I20250104 13:21:40 95 dinov2 param_groups.py:54] chunked fsdp
I20250104 13:21:40 95 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.05083731656658002, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.05083731656658002, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.3.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.4.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.1.5.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.6.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.7.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.2.8.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.9.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.10.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] blocks.3.11.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 13:21:40 95 dinov2 param_groups.py:64] else code branch
I20250104 13:21:40 95 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:21:40 95 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 13:21:40 95 dinov2 train.py:106] Schedulers ready.
I20250104 13:21:40 95 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20250104 13:21:40 95 dinov2 augmentations.py:34] ###################################
I20250104 13:21:40 95 dinov2 augmentations.py:35] Using data augmentation parameters:
I20250104 13:21:40 95 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20250104 13:21:40 95 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20250104 13:21:40 95 dinov2 augmentations.py:38] local_crops_number: 8
I20250104 13:21:40 95 dinov2 augmentations.py:39] global_crops_size: 224
I20250104 13:21:40 95 dinov2 augmentations.py:40] local_crops_size: 96
I20250104 13:21:40 95 dinov2 augmentations.py:41] ###################################
I20250104 13:21:40 95 dinov2 loaders.py:87] using dataset: "TileDataset:split=TRAIN:root=/ruiyan/yuhao/data"
I20250104 13:24:21 95 dinov2 config.py:67] git:
  sha: fda283ef182296a3187e17b7c6482658c6b64ee3, status: has uncommitted changes, branch: main

I20250104 13:24:21 95 dinov2 config.py:68] config_file: dinov2/configs/train/patch.yaml
eval: 
eval_only: False
local_rank: 5
no_resume: False
opts: ['train.dataset_path=TileDataset:split=TRAIN:root=/ruiyan/yuhao/data', 'train.output_dir=/ruiyan/yuhao/project/FMBC/dinov2/output']
output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
I20250104 13:24:21 95 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.006928203230275509
I20250104 13:24:21 95 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
dino:
  loss_weight: 1.0
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 380
  dataset_path: TileDataset:split=TRAIN:root=/ruiyan/yuhao/data
  output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
  saveckp_freq: 20
  seed: 0
  num_workers: 8
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 2000
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.006928203230275509
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20250104 13:24:21 95 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 13:24:22 95 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 768
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 384
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_base network.
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:396] DISTRIBUTED FSDP -- preparing model for distributed training
I20250104 13:24:24 95 dinov2 train.py:307] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
)
I20250104 13:24:24 95 dinov2 param_groups.py:54] chunked fsdp
I20250104 13:24:24 95 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.05083731656658002, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.05083731656658002, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.3.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.4.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.1.5.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.6.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.7.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.2.8.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.9.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.10.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] blocks.3.11.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 13:24:24 95 dinov2 param_groups.py:64] else code branch
I20250104 13:24:24 95 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:24:24 95 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 13:24:25 95 dinov2 train.py:106] Schedulers ready.
I20250104 13:24:25 95 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20250104 13:24:25 95 dinov2 augmentations.py:34] ###################################
I20250104 13:24:25 95 dinov2 augmentations.py:35] Using data augmentation parameters:
I20250104 13:24:25 95 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20250104 13:24:25 95 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20250104 13:24:25 95 dinov2 augmentations.py:38] local_crops_number: 8
I20250104 13:24:25 95 dinov2 augmentations.py:39] global_crops_size: 224
I20250104 13:24:25 95 dinov2 augmentations.py:40] local_crops_size: 96
I20250104 13:24:25 95 dinov2 augmentations.py:41] ###################################
I20250104 13:24:25 95 dinov2 loaders.py:87] using dataset: "TileDataset:split=TRAIN:root=/ruiyan/yuhao/data"
I20250104 13:33:01 95 dinov2 loaders.py:92] # of dataset samples: 92,020,608
I20250104 13:33:01 95 dinov2 loaders.py:125] sampler: sharded infinite
I20250104 13:33:01 95 dinov2 loaders.py:209] using PyTorch data loader
I20250104 13:33:01 95 dinov2 loaders.py:224] infinite data loader
I20250104 13:33:01 95 dinov2 train.py:221] Starting training from iteration 0
I20250104 13:35:43 95 dinov2 helpers.py:102] Training  [      0/2500000]  eta: 4687 days, 21:02:44  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4789 (7.4789)  dino_local_crops_loss: 4.6839 (4.6839)  dino_global_crops_loss: 0.5855 (0.5855)  koleo_loss: 0.7135 (0.7135)  ibot_loss: 1.4959 (1.4959)  time: 162.013031  data: 109.572365  max mem: 61575
I20250104 13:35:56 95 dinov2 helpers.py:102] Training  [     10/2500000]  eta: 460 days, 4:50:28  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.8011 (7.7703)  dino_local_crops_loss: 4.9622 (4.9458)  dino_global_crops_loss: 0.6203 (0.6182)  koleo_loss: 0.7116 (0.6996)  ibot_loss: 1.5083 (1.5067)  time: 15.904635  data: 9.961410  max mem: 61695
I20250104 13:36:14 95 dinov2 helpers.py:102] Training  [     20/2500000]  eta: 265 days, 17:58:43  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.9771 (7.8926)  dino_local_crops_loss: 5.1624 (5.1015)  dino_global_crops_loss: 0.6453 (0.6377)  koleo_loss: 0.6413 (0.6430)  ibot_loss: 1.5132 (1.5104)  time: 1.542930  data: 0.044295  max mem: 61695
I20250104 13:36:29 95 dinov2 helpers.py:102] Training  [     30/2500000]  eta: 194 days, 2:22:59  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 8.0052 (7.9154)  dino_local_crops_loss: 5.3185 (5.1734)  dino_global_crops_loss: 0.6648 (0.6467)  koleo_loss: 0.5033 (0.5842)  ibot_loss: 1.5140 (1.5112)  time: 1.650087  data: 0.044295  max mem: 61695
I20250104 13:36:46 95 dinov2 helpers.py:102] Training  [     40/2500000]  eta: 158 days, 18:22:47  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.9152 (7.9065)  dino_local_crops_loss: 5.3200 (5.2076)  dino_global_crops_loss: 0.6650 (0.6509)  koleo_loss: 0.4167 (0.5385)  ibot_loss: 1.5089 (1.5095)  time: 1.604840  data: 0.000321  max mem: 61695
I20250104 13:37:05 95 dinov2 helpers.py:102] Training  [     50/2500000]  eta: 138 days, 15:56:28  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.8197 (7.8883)  dino_local_crops_loss: 5.3065 (5.2262)  dino_global_crops_loss: 0.6633 (0.6533)  koleo_loss: 0.3584 (0.5027)  ibot_loss: 1.4980 (1.5061)  time: 1.822809  data: 0.000377  max mem: 61695
I20250104 13:37:20 95 dinov2 helpers.py:102] Training  [     60/2500000]  eta: 123 days, 0:47:35  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.7903 (7.8656)  dino_local_crops_loss: 5.2987 (5.2378)  dino_global_crops_loss: 0.6624 (0.6547)  koleo_loss: 0.3428 (0.4716)  ibot_loss: 1.4864 (1.5014)  time: 1.720557  data: 0.000409  max mem: 61695
I20250104 13:37:36 95 dinov2 helpers.py:102] Training  [     70/2500000]  eta: 111 days, 21:47:37  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.6966 (7.8348)  dino_local_crops_loss: 5.2965 (5.2462)  dino_global_crops_loss: 0.6622 (0.6558)  koleo_loss: 0.2700 (0.4376)  ibot_loss: 1.4690 (1.4953)  time: 1.509718  data: 0.000389  max mem: 61695
I20250104 13:37:51 95 dinov2 helpers.py:102] Training  [     80/2500000]  eta: 103 days, 10:59:34  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5911 (7.7985)  dino_local_crops_loss: 5.3002 (5.2539)  dino_global_crops_loss: 0.6632 (0.6570)  koleo_loss: 0.1774 (0.3994)  ibot_loss: 1.4477 (1.4884)  time: 1.512299  data: 0.000381  max mem: 61695
I20250104 13:38:09 95 dinov2 helpers.py:102] Training  [     90/2500000]  eta: 97 days, 20:56:46  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4859 (7.7612)  dino_local_crops_loss: 5.3063 (5.2594)  dino_global_crops_loss: 0.6646 (0.6577)  koleo_loss: 0.0759 (0.3621)  ibot_loss: 1.4334 (1.4820)  time: 1.660688  data: 0.000473  max mem: 61695
I20250104 13:38:24 95 dinov2 helpers.py:102] Training  [    100/2500000]  eta: 92 days, 15:19:04  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4343 (7.7273)  dino_local_crops_loss: 5.2952 (5.2625)  dino_global_crops_loss: 0.6624 (0.6581)  koleo_loss: 0.0458 (0.3302)  ibot_loss: 1.4282 (1.4764)  time: 1.687337  data: 0.000541  max mem: 61695
I20250104 13:38:40 95 dinov2 helpers.py:102] Training  [    110/2500000]  eta: 88 days, 8:42:12  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4052 (7.6973)  dino_local_crops_loss: 5.2875 (5.2648)  dino_global_crops_loss: 0.6614 (0.6584)  koleo_loss: 0.0343 (0.3031)  ibot_loss: 1.4192 (1.4710)  time: 1.558557  data: 0.000428  max mem: 61695
I20250104 13:38:57 95 dinov2 helpers.py:102] Training  [    120/2500000]  eta: 85 days, 5:05:35  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3851 (7.6709)  dino_local_crops_loss: 5.2874 (5.2667)  dino_global_crops_loss: 0.6613 (0.6587)  koleo_loss: 0.0253 (0.2800)  ibot_loss: 1.4104 (1.4656)  time: 1.649093  data: 0.000334  max mem: 61695
I20250104 13:39:14 95 dinov2 helpers.py:102] Training  [    130/2500000]  eta: 82 days, 11:44:23  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3761 (7.6481)  dino_local_crops_loss: 5.2895 (5.2686)  dino_global_crops_loss: 0.6615 (0.6589)  koleo_loss: 0.0209 (0.2602)  ibot_loss: 1.4033 (1.4605)  time: 1.724472  data: 0.000322  max mem: 61695
I20250104 13:39:29 95 dinov2 helpers.py:102] Training  [    140/2500000]  eta: 79 days, 17:42:13  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3677 (7.6278)  dino_local_crops_loss: 5.2926 (5.2704)  dino_global_crops_loss: 0.6620 (0.6591)  koleo_loss: 0.0184 (0.2430)  ibot_loss: 1.3936 (1.4553)  time: 1.611302  data: 0.000458  max mem: 61695
I20250104 13:39:45 95 dinov2 helpers.py:102] Training  [    150/2500000]  eta: 77 days, 8:30:08  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3556 (7.6094)  dino_local_crops_loss: 5.2953 (5.2721)  dino_global_crops_loss: 0.6625 (0.6594)  koleo_loss: 0.0165 (0.2279)  ibot_loss: 1.3817 (1.4500)  time: 1.511227  data: 0.000485  max mem: 61695
I20250104 13:40:04 95 dinov2 helpers.py:102] Training  [    160/2500000]  eta: 76 days, 2:19:14  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3410 (7.5923)  dino_local_crops_loss: 5.2971 (5.2737)  dino_global_crops_loss: 0.6630 (0.6596)  koleo_loss: 0.0139 (0.2145)  ibot_loss: 1.3666 (1.4445)  time: 1.743057  data: 0.000406  max mem: 61695
I20250104 13:40:24 95 dinov2 helpers.py:102] Training  [    170/2500000]  eta: 74 days, 21:32:43  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3300 (7.5766)  dino_local_crops_loss: 5.2982 (5.2751)  dino_global_crops_loss: 0.6633 (0.6598)  koleo_loss: 0.0118 (0.2026)  ibot_loss: 1.3559 (1.4390)  time: 1.947777  data: 0.000386  max mem: 61695
I20250104 13:40:38 95 dinov2 helpers.py:102] Training  [    180/2500000]  eta: 73 days, 2:07:56  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3163 (7.5615)  dino_local_crops_loss: 5.2987 (5.2765)  dino_global_crops_loss: 0.6634 (0.6600)  koleo_loss: 0.0095 (0.1919)  ibot_loss: 1.3444 (1.4331)  time: 1.689434  data: 0.000428  max mem: 61695
I20250104 13:40:53 95 dinov2 helpers.py:102] Training  [    190/2500000]  eta: 71 days, 11:07:29  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2998 (7.5477)  dino_local_crops_loss: 5.2988 (5.2776)  dino_global_crops_loss: 0.6633 (0.6602)  koleo_loss: 0.0073 (0.1822)  ibot_loss: 1.3285 (1.4277)  time: 1.455306  data: 0.000564  max mem: 61695
I20250104 13:41:09 95 dinov2 helpers.py:102] Training  [    200/2500000]  eta: 70 days, 6:54:29  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2921 (7.5349)  dino_local_crops_loss: 5.2988 (5.2787)  dino_global_crops_loss: 0.6633 (0.6604)  koleo_loss: 0.0054 (0.1734)  ibot_loss: 1.3244 (1.4225)  time: 1.553390  data: 0.000507  max mem: 61695
I20250104 13:41:26 95 dinov2 helpers.py:102] Training  [    210/2500000]  eta: 69 days, 7:25:59  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2824 (7.5226)  dino_local_crops_loss: 5.2986 (5.2796)  dino_global_crops_loss: 0.6632 (0.6605)  koleo_loss: 0.0039 (0.1653)  ibot_loss: 1.3171 (1.4171)  time: 1.684811  data: 0.000330  max mem: 61695
I20250104 13:41:41 95 dinov2 helpers.py:102] Training  [    220/2500000]  eta: 68 days, 1:37:39  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2745 (7.5113)  dino_local_crops_loss: 5.2983 (5.2805)  dino_global_crops_loss: 0.6632 (0.6606)  koleo_loss: 0.0025 (0.1579)  ibot_loss: 1.3099 (1.4123)  time: 1.581680  data: 0.000301  max mem: 61695
I20250104 13:41:56 95 dinov2 helpers.py:102] Training  [    230/2500000]  eta: 67 days, 1:35:13  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2680 (7.5005)  dino_local_crops_loss: 5.2981 (5.2812)  dino_global_crops_loss: 0.6631 (0.6607)  koleo_loss: 0.0003 (0.1510)  ibot_loss: 1.3068 (1.4075)  time: 1.500118  data: 0.000281  max mem: 61695
I20250104 13:42:12 95 dinov2 helpers.py:102] Training  [    240/2500000]  eta: 66 days, 2:54:59  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2555 (7.4903)  dino_local_crops_loss: 5.2979 (5.2819)  dino_global_crops_loss: 0.6631 (0.6608)  koleo_loss: -0.0014 (0.1447)  ibot_loss: 1.2973 (1.4029)  time: 1.542248  data: 0.000358  max mem: 61695
I20250104 13:42:28 95 dinov2 helpers.py:102] Training  [    250/2500000]  eta: 65 days, 8:30:20  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2554 (7.4809)  dino_local_crops_loss: 5.2976 (5.2825)  dino_global_crops_loss: 0.6630 (0.6609)  koleo_loss: -0.0034 (0.1387)  ibot_loss: 1.2968 (1.3987)  time: 1.575752  data: 0.000365  max mem: 61695
I20250104 13:42:44 95 dinov2 helpers.py:102] Training  [    260/2500000]  eta: 64 days, 14:25:35  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2535 (7.4722)  dino_local_crops_loss: 5.2974 (5.2831)  dino_global_crops_loss: 0.6629 (0.6610)  koleo_loss: -0.0050 (0.1332)  ibot_loss: 1.2982 (1.3949)  time: 1.599829  data: 0.000305  max mem: 61695
I20250104 13:42:59 95 dinov2 helpers.py:102] Training  [    270/2500000]  eta: 63 days, 20:47:25  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2467 (7.4637)  dino_local_crops_loss: 5.2971 (5.2836)  dino_global_crops_loss: 0.6629 (0.6611)  koleo_loss: -0.0070 (0.1280)  ibot_loss: 1.2943 (1.3910)  time: 1.562166  data: 0.000454  max mem: 61695
I20250104 13:43:17 95 dinov2 helpers.py:102] Training  [    280/2500000]  eta: 63 days, 10:40:25  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2404 (7.4557)  dino_local_crops_loss: 5.2968 (5.2841)  dino_global_crops_loss: 0.6629 (0.6611)  koleo_loss: -0.0086 (0.1231)  ibot_loss: 1.2890 (1.3874)  time: 1.671542  data: 0.000563  max mem: 61695
I20250104 13:43:33 95 dinov2 helpers.py:102] Training  [    290/2500000]  eta: 62 days, 20:17:20  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2384 (7.4482)  dino_local_crops_loss: 5.2965 (5.2845)  dino_global_crops_loss: 0.6629 (0.6612)  koleo_loss: -0.0097 (0.1185)  ibot_loss: 1.2880 (1.3840)  time: 1.694302  data: 0.000460  max mem: 61695
I20250104 13:43:52 95 dinov2 helpers.py:102] Training  [    300/2500000]  eta: 62 days, 12:54:25  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2347 (7.4410)  dino_local_crops_loss: 5.2962 (5.2849)  dino_global_crops_loss: 0.6627 (0.6612)  koleo_loss: -0.0111 (0.1142)  ibot_loss: 1.2872 (1.3807)  time: 1.721372  data: 0.000510  max mem: 61695
I20250104 13:44:06 95 dinov2 helpers.py:102] Training  [    310/2500000]  eta: 61 days, 20:50:00  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2267 (7.4341)  dino_local_crops_loss: 5.2958 (5.2852)  dino_global_crops_loss: 0.6627 (0.6613)  koleo_loss: -0.0123 (0.1101)  ibot_loss: 1.2819 (1.3775)  time: 1.647174  data: 0.000626  max mem: 61695
I20250104 13:44:21 95 dinov2 helpers.py:102] Training  [    320/2500000]  eta: 61 days, 6:19:11  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2296 (7.4279)  dino_local_crops_loss: 5.2951 (5.2855)  dino_global_crops_loss: 0.6627 (0.6613)  koleo_loss: -0.0133 (0.1063)  ibot_loss: 1.2847 (1.3748)  time: 1.454825  data: 0.000428  max mem: 61695
I20250104 13:44:35 95 dinov2 helpers.py:102] Training  [    330/2500000]  eta: 60 days, 16:42:58  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2309 (7.4218)  dino_local_crops_loss: 5.2943 (5.2858)  dino_global_crops_loss: 0.6626 (0.6614)  koleo_loss: -0.0143 (0.1026)  ibot_loss: 1.2897 (1.3721)  time: 1.468533  data: 0.000472  max mem: 61695
I20250104 13:44:54 95 dinov2 helpers.py:102] Training  [    340/2500000]  eta: 60 days, 12:11:01  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2224 (7.4159)  dino_local_crops_loss: 5.2932 (5.2860)  dino_global_crops_loss: 0.6625 (0.6614)  koleo_loss: -0.0151 (0.0991)  ibot_loss: 1.2813 (1.3694)  time: 1.672479  data: 0.000439  max mem: 61695
I20250104 13:45:11 95 dinov2 helpers.py:102] Training  [    350/2500000]  eta: 60 days, 3:22:05  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2225 (7.4105)  dino_local_crops_loss: 5.2917 (5.2861)  dino_global_crops_loss: 0.6624 (0.6614)  koleo_loss: -0.0159 (0.0959)  ibot_loss: 1.2832 (1.3671)  time: 1.760865  data: 0.000295  max mem: 61695
I20250104 13:45:25 95 dinov2 helpers.py:102] Training  [    360/2500000]  eta: 59 days, 15:19:23  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2277 (7.4055)  dino_local_crops_loss: 5.2905 (5.2862)  dino_global_crops_loss: 0.6623 (0.6614)  koleo_loss: -0.0167 (0.0927)  ibot_loss: 1.2923 (1.3651)  time: 1.549444  data: 0.000419  max mem: 61695
I20250104 13:45:42 95 dinov2 helpers.py:102] Training  [    370/2500000]  eta: 59 days, 8:09:20  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2249 (7.4005)  dino_local_crops_loss: 5.2897 (5.2863)  dino_global_crops_loss: 0.6622 (0.6615)  koleo_loss: -0.0173 (0.0898)  ibot_loss: 1.2903 (1.3630)  time: 1.565771  data: 0.000541  max mem: 61695
I20250104 13:45:57 95 dinov2 helpers.py:102] Training  [    380/2500000]  eta: 58 days, 23:12:17  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2202 (7.3958)  dino_local_crops_loss: 5.2893 (5.2864)  dino_global_crops_loss: 0.6621 (0.6615)  koleo_loss: -0.0178 (0.0869)  ibot_loss: 1.2863 (1.3610)  time: 1.619490  data: 0.000491  max mem: 61695
I20250104 13:46:12 95 dinov2 helpers.py:102] Training  [    390/2500000]  eta: 58 days, 13:28:42  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2232 (7.3914)  dino_local_crops_loss: 5.2890 (5.2864)  dino_global_crops_loss: 0.6621 (0.6615)  koleo_loss: -0.0181 (0.0842)  ibot_loss: 1.2895 (1.3592)  time: 1.525524  data: 0.000516  max mem: 61695
I20250104 13:46:27 95 dinov2 helpers.py:102] Training  [    400/2500000]  eta: 58 days, 3:52:22  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2217 (7.3871)  dino_local_crops_loss: 5.2885 (5.2865)  dino_global_crops_loss: 0.6620 (0.6615)  koleo_loss: -0.0183 (0.0817)  ibot_loss: 1.2901 (1.3574)  time: 1.480293  data: 0.000529  max mem: 61695
I20250104 13:46:44 95 dinov2 helpers.py:102] Training  [    410/2500000]  eta: 57 days, 22:45:40  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2215 (7.3831)  dino_local_crops_loss: 5.2869 (5.2865)  dino_global_crops_loss: 0.6619 (0.6615)  koleo_loss: -0.0192 (0.0792)  ibot_loss: 1.2901 (1.3559)  time: 1.588954  data: 0.002058  max mem: 61695
I20250104 13:47:00 95 dinov2 helpers.py:102] Training  [    420/2500000]  eta: 57 days, 15:07:50  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2187 (7.3792)  dino_local_crops_loss: 5.2857 (5.2864)  dino_global_crops_loss: 0.6617 (0.6615)  koleo_loss: -0.0197 (0.0769)  ibot_loss: 1.2914 (1.3544)  time: 1.624411  data: 0.002049  max mem: 61695
I20250104 13:47:14 95 dinov2 helpers.py:102] Training  [    430/2500000]  eta: 57 days, 6:09:34  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2191 (7.3755)  dino_local_crops_loss: 5.2843 (5.2864)  dino_global_crops_loss: 0.6616 (0.6615)  koleo_loss: -0.0200 (0.0746)  ibot_loss: 1.2933 (1.3530)  time: 1.488096  data: 0.000362  max mem: 61695
I20250104 13:47:30 95 dinov2 helpers.py:102] Training  [    440/2500000]  eta: 57 days, 0:17:03  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2191 (7.3718)  dino_local_crops_loss: 5.2824 (5.2862)  dino_global_crops_loss: 0.6614 (0.6615)  koleo_loss: -0.0204 (0.0725)  ibot_loss: 1.2933 (1.3516)  time: 1.520906  data: 0.000378  max mem: 61695
I20250104 13:47:47 95 dinov2 helpers.py:102] Training  [    450/2500000]  eta: 56 days, 20:05:11  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2085 (7.3682)  dino_local_crops_loss: 5.2788 (5.2861)  dino_global_crops_loss: 0.6610 (0.6615)  koleo_loss: -0.0206 (0.0704)  ibot_loss: 1.2885 (1.3502)  time: 1.652335  data: 0.000370  max mem: 61695
