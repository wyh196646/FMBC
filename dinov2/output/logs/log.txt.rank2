I20250104 12:39:11 92 dinov2 config.py:67] git:
  sha: fda283ef182296a3187e17b7c6482658c6b64ee3, status: has uncommitted changes, branch: main

I20250104 12:39:11 92 dinov2 config.py:68] config_file: dinov2/configs/train/patch.yaml
eval: 
eval_only: False
local_rank: 2
no_resume: False
opts: ['train.dataset_path=TileDataset:split=TRAIN:root=/ruiyan/yuhao/data', 'train.output_dir=/ruiyan/yuhao/project/FMBC/dinov2/output']
output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
I20250104 12:39:11 92 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.006838803314120883
I20250104 12:39:11 92 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
dino:
  loss_weight: 1.0
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 380
  dataset_path: TileDataset:split=TRAIN:root=/ruiyan/yuhao/data
  output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
  saveckp_freq: 20
  seed: 0
  num_workers: 8
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 2000
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.006838803314120883
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20250104 12:39:11 92 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 12:39:12 92 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 768
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 384
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_base network.
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:396] DISTRIBUTED FSDP -- preparing model for distributed training
I20250104 12:39:14 92 dinov2 train.py:307] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
)
I20250104 12:39:14 92 dinov2 param_groups.py:54] chunked fsdp
I20250104 12:39:14 92 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.05083731656658002, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.05083731656658002, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.3.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.4.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.1.5.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.6.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.7.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.2.8.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.9.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.10.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] blocks.3.11.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 12:39:14 92 dinov2 param_groups.py:64] else code branch
I20250104 12:39:14 92 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 12:39:14 92 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 12:39:14 92 dinov2 train.py:106] Schedulers ready.
I20250104 12:39:14 92 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20250104 12:39:14 92 dinov2 augmentations.py:34] ###################################
I20250104 12:39:14 92 dinov2 augmentations.py:35] Using data augmentation parameters:
I20250104 12:39:14 92 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20250104 12:39:14 92 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20250104 12:39:14 92 dinov2 augmentations.py:38] local_crops_number: 8
I20250104 12:39:14 92 dinov2 augmentations.py:39] global_crops_size: 224
I20250104 12:39:14 92 dinov2 augmentations.py:40] local_crops_size: 96
I20250104 12:39:14 92 dinov2 augmentations.py:41] ###################################
I20250104 12:39:14 92 dinov2 loaders.py:87] using dataset: "TileDataset:split=TRAIN:root=/ruiyan/yuhao/data"
I20250104 12:47:48 92 dinov2 loaders.py:92] # of dataset samples: 1,960,591
I20250104 12:47:48 92 dinov2 loaders.py:125] sampler: sharded infinite
I20250104 12:47:48 92 dinov2 loaders.py:209] using PyTorch data loader
I20250104 12:47:48 92 dinov2 loaders.py:224] infinite data loader
I20250104 12:47:48 92 dinov2 train.py:221] Starting training from iteration 0
I20250104 12:48:44 92 dinov2 helpers.py:102] Training  [      0/2500000]  eta: 1637 days, 4:56:01  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: inf (inf)  dino_local_crops_loss: 4.6841 (4.6841)  dino_global_crops_loss: 0.5855 (0.5855)  koleo_loss: inf (inf)  ibot_loss: 1.4946 (1.4946)  time: 56.581825  data: 15.829247  max mem: 61574
I20250104 12:48:57 92 dinov2 helpers.py:102] Training  [     10/2500000]  eta: 181 days, 11:17:19  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.8513 (inf)  dino_local_crops_loss: 4.9623 (4.9458)  dino_global_crops_loss: 0.6203 (0.6182)  koleo_loss: 0.7070 (inf)  ibot_loss: 1.5086 (1.5069)  time: 6.271642  data: 1.439328  max mem: 61695
I20250104 12:49:09 92 dinov2 helpers.py:102] Training  [     20/2500000]  eta: 112 days, 12:12:03  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.9619 (inf)  dino_local_crops_loss: 5.1607 (5.0995)  dino_global_crops_loss: 0.6451 (0.6374)  koleo_loss: 0.6292 (inf)  ibot_loss: 1.5107 (1.5090)  time: 1.253645  data: 0.000289  max mem: 61695
I20250104 12:49:22 92 dinov2 helpers.py:102] Training  [     30/2500000]  eta: 87 days, 18:17:37  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.9729 (inf)  dino_local_crops_loss: 5.3146 (5.1710)  dino_global_crops_loss: 0.6643 (0.6464)  koleo_loss: 0.4844 (inf)  ibot_loss: 1.5056 (1.5051)  time: 1.251901  data: 0.000251  max mem: 61695
I20250104 12:49:34 92 dinov2 helpers.py:102] Training  [     40/2500000]  eta: 75 days, 0:03:06  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.8926 (inf)  dino_local_crops_loss: 5.3179 (5.2055)  dino_global_crops_loss: 0.6647 (0.6507)  koleo_loss: 0.4180 (inf)  ibot_loss: 1.4844 (1.4972)  time: 1.231102  data: 0.000334  max mem: 61695
I20250104 12:49:46 92 dinov2 helpers.py:102] Training  [     50/2500000]  eta: 67 days, 7:55:09  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.8134 (inf)  dino_local_crops_loss: 5.3059 (5.2246)  dino_global_crops_loss: 0.6632 (0.6531)  koleo_loss: 0.3792 (inf)  ibot_loss: 1.4569 (1.4878)  time: 1.232470  data: 0.000386  max mem: 61695
I20250104 12:49:58 92 dinov2 helpers.py:102] Training  [     60/2500000]  eta: 61 days, 23:13:09  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.7628 (inf)  dino_local_crops_loss: 5.3001 (5.2369)  dino_global_crops_loss: 0.6625 (0.6546)  koleo_loss: 0.3532 (inf)  ibot_loss: 1.4355 (1.4772)  time: 1.218186  data: 0.000306  max mem: 61695
I20250104 12:50:10 92 dinov2 helpers.py:102] Training  [     70/2500000]  eta: 58 days, 4:59:15  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.6827 (inf)  dino_local_crops_loss: 5.3001 (5.2464)  dino_global_crops_loss: 0.6624 (0.6557)  koleo_loss: 0.3088 (inf)  ibot_loss: 1.4072 (1.4649)  time: 1.207826  data: 0.009667  max mem: 61695
I20250104 12:50:25 92 dinov2 helpers.py:102] Training  [     80/2500000]  eta: 56 days, 6:19:01  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5742 (inf)  dino_local_crops_loss: 5.3078 (5.2543)  dino_global_crops_loss: 0.6628 (0.6566)  koleo_loss: 0.2331 (inf)  ibot_loss: 1.3645 (1.4508)  time: 1.343278  data: 0.133212  max mem: 61695
I20250104 12:50:38 92 dinov2 helpers.py:102] Training  [     90/2500000]  eta: 54 days, 5:52:49  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4723 (inf)  dino_local_crops_loss: 5.3049 (5.2594)  dino_global_crops_loss: 0.6625 (0.6573)  koleo_loss: 0.1611 (inf)  ibot_loss: 1.3397 (1.4381)  time: 1.388627  data: 0.183987  max mem: 61695
I20250104 12:50:52 92 dinov2 helpers.py:102] Training  [    100/2500000]  eta: 52 days, 18:29:04  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3696 (inf)  dino_local_crops_loss: 5.2941 (5.2624)  dino_global_crops_loss: 0.6616 (0.6576)  koleo_loss: 0.0845 (inf)  ibot_loss: 1.3316 (1.4274)  time: 1.334942  data: 0.104081  max mem: 61695
I20250104 12:51:05 92 dinov2 helpers.py:102] Training  [    110/2500000]  eta: 51 days, 9:48:17  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3295 (inf)  dino_local_crops_loss: 5.2861 (5.2645)  dino_global_crops_loss: 0.6609 (0.6579)  koleo_loss: 0.0510 (inf)  ibot_loss: 1.3284 (1.4179)  time: 1.330774  data: 0.078324  max mem: 61695
I20250104 12:51:20 92 dinov2 helpers.py:102] Training  [    120/2500000]  eta: 50 days, 19:06:48  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3054 (inf)  dino_local_crops_loss: 5.2866 (5.2667)  dino_global_crops_loss: 0.6610 (0.6582)  koleo_loss: 0.0392 (inf)  ibot_loss: 1.3118 (1.4086)  time: 1.411183  data: 0.106631  max mem: 61695
I20250104 12:51:33 92 dinov2 helpers.py:102] Training  [    130/2500000]  eta: 49 days, 19:54:10  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3462 (inf)  dino_local_crops_loss: 5.3002 (5.2789)  dino_global_crops_loss: 0.6629 (0.6598)  koleo_loss: 0.0328 (inf)  ibot_loss: 1.2978 (1.3993)  time: 1.419342  data: 0.072200  max mem: 61695
I20250104 12:51:47 92 dinov2 helpers.py:102] Training  [    140/2500000]  eta: 49 days, 3:28:18  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5636 (inf)  dino_local_crops_loss: 5.5653 (5.3009)  dino_global_crops_loss: 0.6943 (0.6624)  koleo_loss: 0.0280 (inf)  ibot_loss: 1.2724 (1.3897)  time: 1.353243  data: 0.000371  max mem: 61695
I20250104 12:52:00 92 dinov2 helpers.py:102] Training  [    150/2500000]  eta: 48 days, 10:32:08  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5810 (inf)  dino_local_crops_loss: 5.6022 (5.3213)  dino_global_crops_loss: 0.6990 (0.6649)  koleo_loss: 0.0282 (inf)  ibot_loss: 1.2488 (1.3800)  time: 1.359504  data: 0.000533  max mem: 61695
I20250104 12:52:15 92 dinov2 helpers.py:102] Training  [    160/2500000]  eta: 48 days, 0:33:00  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5798 (inf)  dino_local_crops_loss: 5.6156 (5.3400)  dino_global_crops_loss: 0.7011 (0.6672)  koleo_loss: 0.0294 (inf)  ibot_loss: 1.2347 (1.3703)  time: 1.386547  data: 0.000506  max mem: 61695
I20250104 12:52:28 92 dinov2 helpers.py:102] Training  [    170/2500000]  eta: 47 days, 10:43:38  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5790 (inf)  dino_local_crops_loss: 5.6256 (5.3568)  dino_global_crops_loss: 0.7022 (0.6693)  koleo_loss: 0.0298 (inf)  ibot_loss: 1.2143 (1.3608)  time: 1.381116  data: 0.000455  max mem: 61695
I20250104 12:52:41 92 dinov2 helpers.py:102] Training  [    180/2500000]  eta: 46 days, 23:22:35  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5624 (inf)  dino_local_crops_loss: 5.6293 (5.3720)  dino_global_crops_loss: 0.7027 (0.6711)  koleo_loss: 0.0305 (inf)  ibot_loss: 1.1983 (1.3515)  time: 1.331812  data: 0.000411  max mem: 61695
I20250104 12:52:55 92 dinov2 helpers.py:102] Training  [    190/2500000]  eta: 46 days, 12:30:33  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5558 (inf)  dino_local_crops_loss: 5.6328 (5.3858)  dino_global_crops_loss: 0.7030 (0.6728)  koleo_loss: 0.0308 (inf)  ibot_loss: 1.1865 (1.3427)  time: 1.334437  data: 0.000337  max mem: 61695
I20250104 12:53:09 92 dinov2 helpers.py:102] Training  [    200/2500000]  eta: 46 days, 5:52:12  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5474 (inf)  dino_local_crops_loss: 5.6372 (5.3983)  dino_global_crops_loss: 0.7035 (0.6744)  koleo_loss: 0.0308 (inf)  ibot_loss: 1.1768 (1.3341)  time: 1.370298  data: 0.000302  max mem: 61695
I20250104 12:53:22 92 dinov2 helpers.py:102] Training  [    210/2500000]  eta: 45 days, 20:04:14  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5393 (inf)  dino_local_crops_loss: 5.6391 (5.4098)  dino_global_crops_loss: 0.7036 (0.6757)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1627 (1.3259)  time: 1.358276  data: 0.000423  max mem: 61695
I20250104 12:53:35 92 dinov2 helpers.py:102] Training  [    220/2500000]  eta: 45 days, 10:35:16  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5331 (inf)  dino_local_crops_loss: 5.6407 (5.4203)  dino_global_crops_loss: 0.7037 (0.6770)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1576 (1.3182)  time: 1.291635  data: 0.000498  max mem: 61695
I20250104 12:53:48 92 dinov2 helpers.py:102] Training  [    230/2500000]  eta: 45 days, 3:30:49  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5298 (inf)  dino_local_crops_loss: 5.6422 (5.4300)  dino_global_crops_loss: 0.7039 (0.6782)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1513 (1.3108)  time: 1.308980  data: 0.000374  max mem: 61695
I20250104 12:54:02 92 dinov2 helpers.py:102] Training  [    240/2500000]  eta: 45 days, 0:05:39  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5221 (inf)  dino_local_crops_loss: 5.6430 (5.4388)  dino_global_crops_loss: 0.7040 (0.6792)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1437 (1.3037)  time: 1.388635  data: 0.000503  max mem: 61695
I20250104 12:54:15 92 dinov2 helpers.py:102] Training  [    250/2500000]  eta: 44 days, 16:36:19  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5153 (inf)  dino_local_crops_loss: 5.6420 (5.4468)  dino_global_crops_loss: 0.7037 (0.6802)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.1360 (1.2970)  time: 1.363405  data: 0.000560  max mem: 61695
I20250104 12:54:28 92 dinov2 helpers.py:102] Training  [    260/2500000]  eta: 44 days, 9:52:24  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5027 (inf)  dino_local_crops_loss: 5.6365 (5.4539)  dino_global_crops_loss: 0.7031 (0.6811)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.1340 (1.2907)  time: 1.288382  data: 0.000440  max mem: 61695
I20250104 12:54:41 92 dinov2 helpers.py:102] Training  [    270/2500000]  eta: 44 days, 4:07:21  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4861 (inf)  dino_local_crops_loss: 5.6252 (5.4599)  dino_global_crops_loss: 0.7022 (0.6818)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.1288 (1.2846)  time: 1.301286  data: 0.000380  max mem: 61695
I20250104 12:54:56 92 dinov2 helpers.py:102] Training  [    280/2500000]  eta: 44 days, 1:48:45  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4649 (inf)  dino_local_crops_loss: 5.6021 (5.4644)  dino_global_crops_loss: 0.7001 (0.6824)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1250 (1.2789)  time: 1.372085  data: 0.000337  max mem: 61695
I20250104 12:55:09 92 dinov2 helpers.py:102] Training  [    290/2500000]  eta: 43 days, 21:23:26  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4324 (inf)  dino_local_crops_loss: 5.5600 (5.4665)  dino_global_crops_loss: 0.6959 (0.6828)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1226 (1.2735)  time: 1.385854  data: 0.000392  max mem: 61695
I20250104 12:55:23 92 dinov2 helpers.py:102] Training  [    300/2500000]  eta: 43 days, 19:33:25  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3170 (inf)  dino_local_crops_loss: 5.4743 (5.4644)  dino_global_crops_loss: 0.6866 (0.6826)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1174 (1.2683)  time: 1.388017  data: 0.000342  max mem: 61695
I20250104 12:55:37 92 dinov2 helpers.py:102] Training  [    310/2500000]  eta: 43 days, 16:03:54  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.1142 (inf)  dino_local_crops_loss: 5.2990 (5.4551)  dino_global_crops_loss: 0.6672 (0.6817)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1166 (1.2634)  time: 1.397980  data: 0.000467  max mem: 61695
I20250104 12:55:52 92 dinov2 helpers.py:102] Training  [    320/2500000]  eta: 43 days, 15:51:21  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 6.8622 (inf)  dino_local_crops_loss: 4.9936 (5.4367)  dino_global_crops_loss: 0.6334 (0.6797)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1125 (1.2587)  time: 1.429067  data: 0.000652  max mem: 61695
I20250104 12:56:06 92 dinov2 helpers.py:102] Training  [    330/2500000]  eta: 43 days, 13:16:05  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 6.5121 (inf)  dino_local_crops_loss: 4.7092 (5.4125)  dino_global_crops_loss: 0.6015 (0.6771)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1126 (1.2543)  time: 1.442938  data: 0.000441  max mem: 61695
I20250104 12:56:20 92 dinov2 helpers.py:102] Training  [    340/2500000]  eta: 43 days, 10:30:27  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 6.3390 (inf)  dino_local_crops_loss: 4.6126 (5.3885)  dino_global_crops_loss: 0.5894 (0.6745)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1121 (1.2501)  time: 1.377982  data: 0.000265  max mem: 61695
I20250104 12:56:33 92 dinov2 helpers.py:102] Training  [    350/2500000]  eta: 43 days, 7:10:52  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 6.3727 (inf)  dino_local_crops_loss: 4.6399 (5.3706)  dino_global_crops_loss: 0.5936 (0.6726)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1100 (1.2461)  time: 1.351746  data: 0.000308  max mem: 61695
I20250104 12:56:49 92 dinov2 helpers.py:102] Training  [    360/2500000]  eta: 43 days, 9:08:04  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 6.7405 (inf)  dino_local_crops_loss: 4.8965 (5.3631)  dino_global_crops_loss: 0.6216 (0.6718)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1080 (1.2422)  time: 1.465942  data: 0.000395  max mem: 61695
I20250104 12:57:03 92 dinov2 helpers.py:102] Training  [    370/2500000]  eta: 43 days, 7:22:19  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.0848 (inf)  dino_local_crops_loss: 5.2576 (5.3636)  dino_global_crops_loss: 0.6618 (0.6719)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.1076 (1.2386)  time: 1.501951  data: 0.000346  max mem: 61695
I20250104 12:57:17 92 dinov2 helpers.py:102] Training  [    380/2500000]  eta: 43 days, 4:44:37  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2864 (inf)  dino_local_crops_loss: 5.4641 (5.3680)  dino_global_crops_loss: 0.6833 (0.6724)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1061 (1.2351)  time: 1.379211  data: 0.000246  max mem: 61695
I20250104 12:57:30 92 dinov2 helpers.py:102] Training  [    390/2500000]  eta: 43 days, 2:32:41  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4062 (inf)  dino_local_crops_loss: 5.5733 (5.3742)  dino_global_crops_loss: 0.6946 (0.6730)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.1051 (1.2317)  time: 1.361236  data: 0.000334  max mem: 61695
I20250104 12:57:46 92 dinov2 helpers.py:102] Training  [    400/2500000]  eta: 43 days, 3:51:42  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4665 (inf)  dino_local_crops_loss: 5.6324 (5.3811)  dino_global_crops_loss: 0.6999 (0.6737)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1020 (1.2285)  time: 1.467908  data: 0.000446  max mem: 61695
I20250104 12:57:59 92 dinov2 helpers.py:102] Training  [    410/2500000]  eta: 43 days, 1:28:39  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5009 (inf)  dino_local_crops_loss: 5.6674 (5.3883)  dino_global_crops_loss: 0.7031 (0.6745)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1007 (1.2254)  time: 1.458628  data: 0.000350  max mem: 61695
I20250104 12:58:13 92 dinov2 helpers.py:102] Training  [    420/2500000]  eta: 42 days, 23:32:07  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5292 (inf)  dino_local_crops_loss: 5.6870 (5.3957)  dino_global_crops_loss: 0.7047 (0.6752)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1004 (1.2224)  time: 1.360968  data: 0.000398  max mem: 61695
I20250104 12:58:27 92 dinov2 helpers.py:102] Training  [    430/2500000]  eta: 42 days, 21:19:27  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5416 (inf)  dino_local_crops_loss: 5.7028 (5.4030)  dino_global_crops_loss: 0.7052 (0.6759)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1018 (1.2196)  time: 1.359808  data: 0.000442  max mem: 61695
I20250104 12:58:41 92 dinov2 helpers.py:102] Training  [    440/2500000]  eta: 42 days, 21:00:26  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5547 (inf)  dino_local_crops_loss: 5.7180 (5.4103)  dino_global_crops_loss: 0.7057 (0.6766)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.1002 (1.2169)  time: 1.405632  data: 0.000325  max mem: 61695
I20250104 12:58:55 92 dinov2 helpers.py:102] Training  [    450/2500000]  eta: 42 days, 19:19:22  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5643 (inf)  dino_local_crops_loss: 5.7273 (5.4173)  dino_global_crops_loss: 0.7063 (0.6773)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0980 (1.2143)  time: 1.417741  data: 0.000419  max mem: 61695
I20250104 12:59:08 92 dinov2 helpers.py:102] Training  [    460/2500000]  eta: 42 days, 17:17:45  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5691 (inf)  dino_local_crops_loss: 5.7319 (5.4242)  dino_global_crops_loss: 0.7062 (0.6779)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0979 (1.2118)  time: 1.359094  data: 0.000596  max mem: 61695
I20250104 12:59:22 92 dinov2 helpers.py:102] Training  [    470/2500000]  eta: 42 days, 16:03:56  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5736 (inf)  dino_local_crops_loss: 5.7357 (5.4309)  dino_global_crops_loss: 0.7061 (0.6785)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0977 (1.2093)  time: 1.369416  data: 0.000457  max mem: 61695
I20250104 12:59:37 92 dinov2 helpers.py:102] Training  [    480/2500000]  eta: 42 days, 16:30:25  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5745 (inf)  dino_local_crops_loss: 5.7409 (5.4374)  dino_global_crops_loss: 0.7059 (0.6791)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0953 (1.2070)  time: 1.449664  data: 0.000422  max mem: 61695
I20250104 12:59:51 92 dinov2 helpers.py:102] Training  [    490/2500000]  eta: 42 days, 14:30:58  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5751 (inf)  dino_local_crops_loss: 5.7419 (5.4436)  dino_global_crops_loss: 0.7056 (0.6796)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0954 (1.2047)  time: 1.420437  data: 0.000475  max mem: 61695
I20250104 13:00:04 92 dinov2 helpers.py:102] Training  [    500/2500000]  eta: 42 days, 12:54:49  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5734 (inf)  dino_local_crops_loss: 5.7399 (5.4496)  dino_global_crops_loss: 0.7050 (0.6801)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0958 (1.2025)  time: 1.346217  data: 0.000486  max mem: 61695
I20250104 13:00:18 92 dinov2 helpers.py:102] Training  [    510/2500000]  eta: 42 days, 11:10:38  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5729 (inf)  dino_local_crops_loss: 5.7394 (5.4552)  dino_global_crops_loss: 0.7046 (0.6806)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0958 (1.2005)  time: 1.350143  data: 0.000478  max mem: 61695
I20250104 13:00:33 92 dinov2 helpers.py:102] Training  [    520/2500000]  eta: 42 days, 11:30:49  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5719 (inf)  dino_local_crops_loss: 5.7385 (5.4607)  dino_global_crops_loss: 0.7043 (0.6810)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0958 (1.1984)  time: 1.418194  data: 0.000330  max mem: 61695
I20250104 13:00:46 92 dinov2 helpers.py:102] Training  [    530/2500000]  eta: 42 days, 9:54:33  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5719 (inf)  dino_local_crops_loss: 5.7389 (5.4659)  dino_global_crops_loss: 0.7038 (0.6815)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0951 (1.1965)  time: 1.419730  data: 0.000388  max mem: 61695
I20250104 13:01:00 92 dinov2 helpers.py:102] Training  [    540/2500000]  eta: 42 days, 8:47:22  lr: 0.0003 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5701 (inf)  dino_local_crops_loss: 5.7389 (5.4710)  dino_global_crops_loss: 0.7032 (0.6819)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0952 (1.1947)  time: 1.362588  data: 0.000429  max mem: 61695
I20250104 13:01:14 92 dinov2 helpers.py:102] Training  [    550/2500000]  eta: 42 days, 7:35:12  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5683 (inf)  dino_local_crops_loss: 5.7359 (5.4758)  dino_global_crops_loss: 0.7031 (0.6823)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0985 (1.1929)  time: 1.374269  data: 0.000414  max mem: 61695
I20250104 13:01:28 92 dinov2 helpers.py:102] Training  [    560/2500000]  eta: 42 days, 7:20:04  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5683 (inf)  dino_local_crops_loss: 5.7363 (5.4805)  dino_global_crops_loss: 0.7030 (0.6826)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0991 (1.1912)  time: 1.406038  data: 0.000545  max mem: 61695
I20250104 13:01:42 92 dinov2 helpers.py:102] Training  [    570/2500000]  eta: 42 days, 6:21:39  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5672 (inf)  dino_local_crops_loss: 5.7373 (5.4850)  dino_global_crops_loss: 0.7029 (0.6830)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0951 (1.1895)  time: 1.412688  data: 0.000609  max mem: 61695
I20250104 13:01:55 92 dinov2 helpers.py:102] Training  [    580/2500000]  eta: 42 days, 5:07:10  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5636 (inf)  dino_local_crops_loss: 5.7367 (5.4893)  dino_global_crops_loss: 0.7029 (0.6833)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0931 (1.1878)  time: 1.370074  data: 0.000395  max mem: 61695
I20250104 13:02:09 92 dinov2 helpers.py:102] Training  [    590/2500000]  eta: 42 days, 3:32:09  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5652 (inf)  dino_local_crops_loss: 5.7345 (5.4935)  dino_global_crops_loss: 0.7029 (0.6837)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0947 (1.1863)  time: 1.341128  data: 0.000414  max mem: 61695
I20250104 13:02:24 92 dinov2 helpers.py:102] Training  [    600/2500000]  eta: 42 days, 4:31:11  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5674 (inf)  dino_local_crops_loss: 5.7371 (5.4976)  dino_global_crops_loss: 0.7029 (0.6840)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0947 (1.1847)  time: 1.433617  data: 0.000468  max mem: 61695
I20250104 13:02:38 92 dinov2 helpers.py:102] Training  [    610/2500000]  eta: 42 days, 4:10:40  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5659 (inf)  dino_local_crops_loss: 5.7378 (5.5015)  dino_global_crops_loss: 0.7025 (0.6843)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0930 (1.1833)  time: 1.485555  data: 0.000300  max mem: 61695
I20250104 13:02:51 92 dinov2 helpers.py:102] Training  [    620/2500000]  eta: 42 days, 2:17:46  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5646 (inf)  dino_local_crops_loss: 5.7358 (5.5052)  dino_global_crops_loss: 0.7024 (0.6846)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0962 (1.1819)  time: 1.359291  data: 0.000246  max mem: 61695
I20250104 13:03:05 92 dinov2 helpers.py:102] Training  [    630/2500000]  eta: 42 days, 0:55:29  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5634 (inf)  dino_local_crops_loss: 5.7358 (5.5089)  dino_global_crops_loss: 0.7022 (0.6848)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0958 (1.1805)  time: 1.310429  data: 0.000411  max mem: 61695
I20250104 13:03:19 92 dinov2 helpers.py:102] Training  [    640/2500000]  eta: 42 days, 0:39:51  lr: 0.0003 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5639 (inf)  dino_local_crops_loss: 5.7382 (5.5125)  dino_global_crops_loss: 0.7024 (0.6851)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0937 (1.1791)  time: 1.380226  data: 0.000504  max mem: 61695
I20250104 13:03:33 92 dinov2 helpers.py:102] Training  [    650/2500000]  eta: 41 days, 23:48:19  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5663 (inf)  dino_local_crops_loss: 5.7372 (5.5159)  dino_global_crops_loss: 0.7029 (0.6854)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0953 (1.1778)  time: 1.401111  data: 0.000574  max mem: 61695
I20250104 13:03:47 92 dinov2 helpers.py:102] Training  [    660/2500000]  eta: 41 days, 23:39:56  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5663 (inf)  dino_local_crops_loss: 5.7385 (5.5193)  dino_global_crops_loss: 0.7029 (0.6857)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0932 (1.1766)  time: 1.405697  data: 0.000584  max mem: 61695
I20250104 13:04:01 92 dinov2 helpers.py:102] Training  [    670/2500000]  eta: 41 days, 22:59:12  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5632 (inf)  dino_local_crops_loss: 5.7375 (5.5225)  dino_global_crops_loss: 0.7024 (0.6859)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0916 (1.1753)  time: 1.412453  data: 0.000354  max mem: 61695
I20250104 13:04:16 92 dinov2 helpers.py:102] Training  [    680/2500000]  eta: 41 days, 23:13:22  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5637 (inf)  dino_local_crops_loss: 5.7382 (5.5257)  dino_global_crops_loss: 0.7023 (0.6861)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0919 (1.1741)  time: 1.430096  data: 0.000297  max mem: 61695
I20250104 13:04:29 92 dinov2 helpers.py:102] Training  [    690/2500000]  eta: 41 days, 21:42:51  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5647 (inf)  dino_local_crops_loss: 5.7378 (5.5287)  dino_global_crops_loss: 0.7024 (0.6864)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0948 (1.1730)  time: 1.387519  data: 0.000532  max mem: 61695
I20250104 13:04:42 92 dinov2 helpers.py:102] Training  [    700/2500000]  eta: 41 days, 20:14:55  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5667 (inf)  dino_local_crops_loss: 5.7381 (5.5318)  dino_global_crops_loss: 0.7026 (0.6866)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0938 (1.1718)  time: 1.301054  data: 0.000486  max mem: 61695
I20250104 13:04:54 92 dinov2 helpers.py:102] Training  [    710/2500000]  eta: 41 days, 18:28:20  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5656 (inf)  dino_local_crops_loss: 5.7395 (5.5347)  dino_global_crops_loss: 0.7025 (0.6868)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0928 (1.1707)  time: 1.283028  data: 0.000320  max mem: 61695
I20250104 13:05:09 92 dinov2 helpers.py:102] Training  [    720/2500000]  eta: 41 days, 18:58:07  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5656 (inf)  dino_local_crops_loss: 5.7398 (5.5375)  dino_global_crops_loss: 0.7026 (0.6871)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0914 (1.1696)  time: 1.380477  data: 0.000364  max mem: 61695
I20250104 13:05:22 92 dinov2 helpers.py:102] Training  [    730/2500000]  eta: 41 days, 17:36:20  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5662 (inf)  dino_local_crops_loss: 5.7400 (5.5403)  dino_global_crops_loss: 0.7030 (0.6873)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0917 (1.1686)  time: 1.398771  data: 0.000347  max mem: 61695
I20250104 13:05:35 92 dinov2 helpers.py:102] Training  [    740/2500000]  eta: 41 days, 16:13:06  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5662 (inf)  dino_local_crops_loss: 5.7371 (5.5429)  dino_global_crops_loss: 0.7030 (0.6875)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0957 (1.1676)  time: 1.298343  data: 0.000482  max mem: 61695
I20250104 13:05:48 92 dinov2 helpers.py:102] Training  [    750/2500000]  eta: 41 days, 14:53:26  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5668 (inf)  dino_local_crops_loss: 5.7364 (5.5456)  dino_global_crops_loss: 0.7026 (0.6877)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0969 (1.1666)  time: 1.296319  data: 0.000434  max mem: 61695
I20250104 13:06:02 92 dinov2 helpers.py:102] Training  [    760/2500000]  eta: 41 days, 14:17:08  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5684 (inf)  dino_local_crops_loss: 5.7383 (5.5481)  dino_global_crops_loss: 0.7029 (0.6879)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0953 (1.1657)  time: 1.335244  data: 0.000409  max mem: 61695
I20250104 13:06:15 92 dinov2 helpers.py:102] Training  [    770/2500000]  eta: 41 days, 13:04:38  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5698 (inf)  dino_local_crops_loss: 5.7424 (5.5507)  dino_global_crops_loss: 0.7032 (0.6881)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0937 (1.1648)  time: 1.338582  data: 0.000680  max mem: 61695
I20250104 13:06:28 92 dinov2 helpers.py:102] Training  [    780/2500000]  eta: 41 days, 11:41:03  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5708 (inf)  dino_local_crops_loss: 5.7428 (5.5531)  dino_global_crops_loss: 0.7034 (0.6883)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0936 (1.1638)  time: 1.292089  data: 0.000561  max mem: 61695
I20250104 13:06:41 92 dinov2 helpers.py:102] Training  [    790/2500000]  eta: 41 days, 10:37:26  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5731 (inf)  dino_local_crops_loss: 5.7428 (5.5555)  dino_global_crops_loss: 0.7030 (0.6885)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0950 (1.1630)  time: 1.296927  data: 0.000430  max mem: 61695
I20250104 13:06:55 92 dinov2 helpers.py:102] Training  [    800/2500000]  eta: 41 days, 9:58:21  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5725 (inf)  dino_local_crops_loss: 5.7428 (5.5579)  dino_global_crops_loss: 0.7033 (0.6887)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0942 (1.1621)  time: 1.335943  data: 0.000385  max mem: 61695
I20250104 13:07:08 92 dinov2 helpers.py:102] Training  [    810/2500000]  eta: 41 days, 9:13:47  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5705 (inf)  dino_local_crops_loss: 5.7423 (5.5601)  dino_global_crops_loss: 0.7032 (0.6888)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0930 (1.1612)  time: 1.351740  data: 0.000478  max mem: 61695
I20250104 13:07:23 92 dinov2 helpers.py:102] Training  [    820/2500000]  eta: 41 days, 9:34:56  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5692 (inf)  dino_local_crops_loss: 5.7414 (5.5624)  dino_global_crops_loss: 0.7027 (0.6890)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0911 (1.1604)  time: 1.409175  data: 0.000587  max mem: 61695
I20250104 13:07:37 92 dinov2 helpers.py:102] Training  [    830/2500000]  eta: 41 days, 9:24:33  lr: 0.0004 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5699 (inf)  dino_local_crops_loss: 5.7418 (5.5645)  dino_global_crops_loss: 0.7033 (0.6892)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0943 (1.1596)  time: 1.441920  data: 0.000543  max mem: 61695
I20250104 13:07:51 92 dinov2 helpers.py:102] Training  [    840/2500000]  eta: 41 days, 9:11:37  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5704 (inf)  dino_local_crops_loss: 5.7418 (5.5666)  dino_global_crops_loss: 0.7033 (0.6894)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0941 (1.1588)  time: 1.408169  data: 0.000484  max mem: 61695
I20250104 13:08:08 92 dinov2 helpers.py:102] Training  [    850/2500000]  eta: 41 days, 11:18:09  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5714 (inf)  dino_local_crops_loss: 5.7425 (5.5687)  dino_global_crops_loss: 0.7031 (0.6895)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0938 (1.1580)  time: 1.547529  data: 0.097135  max mem: 61695
I20250104 13:08:20 92 dinov2 helpers.py:102] Training  [    860/2500000]  eta: 41 days, 9:57:43  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5714 (inf)  dino_local_crops_loss: 5.7434 (5.5708)  dino_global_crops_loss: 0.7037 (0.6897)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0934 (1.1573)  time: 1.478813  data: 0.097187  max mem: 61695
I20250104 13:08:34 92 dinov2 helpers.py:102] Training  [    870/2500000]  eta: 41 days, 9:01:52  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5712 (inf)  dino_local_crops_loss: 5.7438 (5.5728)  dino_global_crops_loss: 0.7037 (0.6898)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0923 (1.1565)  time: 1.291728  data: 0.000439  max mem: 61695
I20250104 13:08:47 92 dinov2 helpers.py:102] Training  [    880/2500000]  eta: 41 days, 7:57:07  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5731 (inf)  dino_local_crops_loss: 5.7442 (5.5747)  dino_global_crops_loss: 0.7035 (0.6900)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0939 (1.1559)  time: 1.304768  data: 0.000438  max mem: 61695
I20250104 13:09:00 92 dinov2 helpers.py:102] Training  [    890/2500000]  eta: 41 days, 7:01:36  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5756 (inf)  dino_local_crops_loss: 5.7448 (5.5766)  dino_global_crops_loss: 0.7038 (0.6901)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0926 (1.1551)  time: 1.302334  data: 0.000441  max mem: 61695
I20250104 13:09:13 92 dinov2 helpers.py:102] Training  [    900/2500000]  eta: 41 days, 6:31:09  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5746 (inf)  dino_local_crops_loss: 5.7448 (5.5785)  dino_global_crops_loss: 0.7036 (0.6903)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0925 (1.1544)  time: 1.336447  data: 0.000450  max mem: 61695
I20250104 13:09:26 92 dinov2 helpers.py:102] Training  [    910/2500000]  eta: 41 days, 5:24:04  lr: 0.0005 (0.0002)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5730 (inf)  dino_local_crops_loss: 5.7450 (5.5804)  dino_global_crops_loss: 0.7035 (0.6904)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0910 (1.1537)  time: 1.321455  data: 0.000418  max mem: 61695
I20250104 13:09:39 92 dinov2 helpers.py:102] Training  [    920/2500000]  eta: 41 days, 4:16:40  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5729 (inf)  dino_local_crops_loss: 5.7445 (5.5821)  dino_global_crops_loss: 0.7036 (0.6906)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0909 (1.1531)  time: 1.278710  data: 0.000447  max mem: 61695
I20250104 13:09:52 92 dinov2 helpers.py:102] Training  [    930/2500000]  eta: 41 days, 3:27:18  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5764 (inf)  dino_local_crops_loss: 5.7454 (5.5839)  dino_global_crops_loss: 0.7036 (0.6907)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0952 (1.1525)  time: 1.295272  data: 0.000420  max mem: 61695
I20250104 13:10:06 92 dinov2 helpers.py:102] Training  [    940/2500000]  eta: 41 days, 3:10:41  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5762 (inf)  dino_local_crops_loss: 5.7478 (5.5856)  dino_global_crops_loss: 0.7034 (0.6909)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0939 (1.1518)  time: 1.349636  data: 0.001102  max mem: 61695
I20250104 13:10:19 92 dinov2 helpers.py:102] Training  [    950/2500000]  eta: 41 days, 2:16:41  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5762 (inf)  dino_local_crops_loss: 5.7475 (5.5873)  dino_global_crops_loss: 0.7034 (0.6910)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0941 (1.1512)  time: 1.342402  data: 0.001047  max mem: 61695
I20250104 13:10:32 92 dinov2 helpers.py:102] Training  [    960/2500000]  eta: 41 days, 1:14:29  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5762 (inf)  dino_local_crops_loss: 5.7474 (5.5890)  dino_global_crops_loss: 0.7036 (0.6911)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0939 (1.1506)  time: 1.288567  data: 0.000424  max mem: 61695
I20250104 13:10:45 92 dinov2 helpers.py:102] Training  [    970/2500000]  eta: 41 days, 0:30:40  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5762 (inf)  dino_local_crops_loss: 5.7470 (5.5906)  dino_global_crops_loss: 0.7038 (0.6913)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0945 (1.1501)  time: 1.297752  data: 0.000426  max mem: 61695
I20250104 13:10:58 92 dinov2 helpers.py:102] Training  [    980/2500000]  eta: 41 days, 0:05:17  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5782 (inf)  dino_local_crops_loss: 5.7461 (5.5923)  dino_global_crops_loss: 0.7041 (0.6914)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0945 (1.1495)  time: 1.338357  data: 0.000397  max mem: 61695
I20250104 13:11:12 92 dinov2 helpers.py:102] Training  [    990/2500000]  eta: 40 days, 23:21:34  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5779 (inf)  dino_local_crops_loss: 5.7481 (5.5938)  dino_global_crops_loss: 0.7042 (0.6915)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0945 (1.1490)  time: 1.336592  data: 0.000455  max mem: 61695
I20250104 13:11:24 92 dinov2 helpers.py:102] Training  [   1000/2500000]  eta: 40 days, 22:16:34  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5766 (inf)  dino_local_crops_loss: 5.7473 (5.5954)  dino_global_crops_loss: 0.7038 (0.6916)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0957 (1.1484)  time: 1.287562  data: 0.000446  max mem: 61695
I20250104 13:11:37 92 dinov2 helpers.py:102] Training  [   1010/2500000]  eta: 40 days, 21:41:25  lr: 0.0005 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5780 (inf)  dino_local_crops_loss: 5.7467 (5.5969)  dino_global_crops_loss: 0.7037 (0.6918)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0957 (1.1479)  time: 1.295620  data: 0.000557  max mem: 61695
I20250104 13:11:52 92 dinov2 helpers.py:102] Training  [   1020/2500000]  eta: 40 days, 21:45:36  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5787 (inf)  dino_local_crops_loss: 5.7483 (5.5984)  dino_global_crops_loss: 0.7036 (0.6919)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0944 (1.1474)  time: 1.377671  data: 0.000503  max mem: 61695
I20250104 13:12:05 92 dinov2 helpers.py:102] Training  [   1030/2500000]  eta: 40 days, 21:01:02  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5781 (inf)  dino_local_crops_loss: 5.7499 (5.5998)  dino_global_crops_loss: 0.7038 (0.6920)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0938 (1.1468)  time: 1.364823  data: 0.000352  max mem: 61695
I20250104 13:12:18 92 dinov2 helpers.py:102] Training  [   1040/2500000]  eta: 40 days, 20:17:19  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5763 (inf)  dino_local_crops_loss: 5.7491 (5.6012)  dino_global_crops_loss: 0.7039 (0.6921)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0946 (1.1463)  time: 1.304571  data: 0.000553  max mem: 61695
I20250104 13:12:31 92 dinov2 helpers.py:102] Training  [   1050/2500000]  eta: 40 days, 19:32:38  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5766 (inf)  dino_local_crops_loss: 5.7478 (5.6026)  dino_global_crops_loss: 0.7040 (0.6922)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0940 (1.1458)  time: 1.302301  data: 0.000543  max mem: 61695
I20250104 13:12:45 92 dinov2 helpers.py:102] Training  [   1060/2500000]  eta: 40 days, 19:40:07  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5783 (inf)  dino_local_crops_loss: 5.7473 (5.6040)  dino_global_crops_loss: 0.7041 (0.6923)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0934 (1.1453)  time: 1.365422  data: 0.000465  max mem: 61695
I20250104 13:12:58 92 dinov2 helpers.py:102] Training  [   1070/2500000]  eta: 40 days, 19:11:34  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5784 (inf)  dino_local_crops_loss: 5.7473 (5.6054)  dino_global_crops_loss: 0.7037 (0.6924)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0934 (1.1449)  time: 1.384650  data: 0.000599  max mem: 61695
I20250104 13:13:12 92 dinov2 helpers.py:102] Training  [   1080/2500000]  eta: 40 days, 18:32:04  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5780 (inf)  dino_local_crops_loss: 5.7469 (5.6067)  dino_global_crops_loss: 0.7036 (0.6925)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0959 (1.1444)  time: 1.323614  data: 0.000395  max mem: 61695
I20250104 13:13:24 92 dinov2 helpers.py:102] Training  [   1090/2500000]  eta: 40 days, 17:45:34  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5823 (inf)  dino_local_crops_loss: 5.7478 (5.6080)  dino_global_crops_loss: 0.7041 (0.6927)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0978 (1.1440)  time: 1.298622  data: 0.000468  max mem: 61695
I20250104 13:13:39 92 dinov2 helpers.py:102] Training  [   1100/2500000]  eta: 40 days, 17:54:12  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5817 (inf)  dino_local_crops_loss: 5.7492 (5.6093)  dino_global_crops_loss: 0.7044 (0.6928)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0946 (1.1435)  time: 1.360271  data: 0.000472  max mem: 61695
I20250104 13:13:52 92 dinov2 helpers.py:102] Training  [   1110/2500000]  eta: 40 days, 17:24:31  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5792 (inf)  dino_local_crops_loss: 5.7481 (5.6105)  dino_global_crops_loss: 0.7041 (0.6929)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0942 (1.1431)  time: 1.381135  data: 0.000417  max mem: 61695
I20250104 13:14:06 92 dinov2 helpers.py:102] Training  [   1120/2500000]  eta: 40 days, 17:11:53  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5821 (inf)  dino_local_crops_loss: 5.7507 (5.6118)  dino_global_crops_loss: 0.7042 (0.6930)  koleo_loss: 0.0312 (inf)  ibot_loss: 1.0946 (1.1427)  time: 1.352477  data: 0.000434  max mem: 61695
I20250104 13:14:21 92 dinov2 helpers.py:102] Training  [   1130/2500000]  eta: 40 days, 17:43:14  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5810 (inf)  dino_local_crops_loss: 5.7516 (5.6130)  dino_global_crops_loss: 0.7042 (0.6931)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0948 (1.1423)  time: 1.434136  data: 0.000286  max mem: 61695
I20250104 13:14:36 92 dinov2 helpers.py:102] Training  [   1140/2500000]  eta: 40 days, 18:09:37  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5808 (inf)  dino_local_crops_loss: 5.7505 (5.6142)  dino_global_crops_loss: 0.7041 (0.6932)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0949 (1.1419)  time: 1.487538  data: 0.000320  max mem: 61695
I20250104 13:14:49 92 dinov2 helpers.py:102] Training  [   1150/2500000]  eta: 40 days, 17:37:07  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5793 (inf)  dino_local_crops_loss: 5.7487 (5.6154)  dino_global_crops_loss: 0.7037 (0.6932)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0950 (1.1415)  time: 1.400768  data: 0.000356  max mem: 61695
I20250104 13:15:02 92 dinov2 helpers.py:102] Training  [   1160/2500000]  eta: 40 days, 17:11:29  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5793 (inf)  dino_local_crops_loss: 5.7493 (5.6165)  dino_global_crops_loss: 0.7042 (0.6933)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0950 (1.1411)  time: 1.328803  data: 0.000472  max mem: 61695
I20250104 13:15:15 92 dinov2 helpers.py:102] Training  [   1170/2500000]  eta: 40 days, 16:42:56  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5816 (inf)  dino_local_crops_loss: 5.7507 (5.6177)  dino_global_crops_loss: 0.7044 (0.6934)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0934 (1.1407)  time: 1.332891  data: 0.000583  max mem: 61695
I20250104 13:15:31 92 dinov2 helpers.py:102] Training  [   1180/2500000]  eta: 40 days, 17:33:34  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5828 (inf)  dino_local_crops_loss: 5.7518 (5.6188)  dino_global_crops_loss: 0.7044 (0.6935)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0929 (1.1403)  time: 1.439797  data: 0.000505  max mem: 61695
I20250104 13:15:44 92 dinov2 helpers.py:102] Training  [   1190/2500000]  eta: 40 days, 16:58:51  lr: 0.0006 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5807 (inf)  dino_local_crops_loss: 5.7498 (5.6199)  dino_global_crops_loss: 0.7039 (0.6936)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0959 (1.1399)  time: 1.430553  data: 0.000588  max mem: 61695
I20250104 13:15:58 92 dinov2 helpers.py:102] Training  [   1200/2500000]  eta: 40 days, 16:39:37  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5814 (inf)  dino_local_crops_loss: 5.7500 (5.6210)  dino_global_crops_loss: 0.7040 (0.6937)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0953 (1.1395)  time: 1.331226  data: 0.000528  max mem: 61695
I20250104 13:16:11 92 dinov2 helpers.py:102] Training  [   1210/2500000]  eta: 40 days, 16:10:34  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5814 (inf)  dino_local_crops_loss: 5.7514 (5.6221)  dino_global_crops_loss: 0.7044 (0.6938)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0944 (1.1392)  time: 1.338027  data: 0.000295  max mem: 61695
I20250104 13:16:25 92 dinov2 helpers.py:102] Training  [   1220/2500000]  eta: 40 days, 16:17:12  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5803 (inf)  dino_local_crops_loss: 5.7512 (5.6232)  dino_global_crops_loss: 0.7046 (0.6939)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0944 (1.1388)  time: 1.374903  data: 0.000346  max mem: 61695
I20250104 13:16:38 92 dinov2 helpers.py:102] Training  [   1230/2500000]  eta: 40 days, 15:52:26  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5802 (inf)  dino_local_crops_loss: 5.7527 (5.6242)  dino_global_crops_loss: 0.7043 (0.6940)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0924 (1.1384)  time: 1.380267  data: 0.000350  max mem: 61695
I20250104 13:16:52 92 dinov2 helpers.py:102] Training  [   1240/2500000]  eta: 40 days, 15:25:15  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5798 (inf)  dino_local_crops_loss: 5.7523 (5.6252)  dino_global_crops_loss: 0.7042 (0.6940)  koleo_loss: 0.0309 (inf)  ibot_loss: 1.0915 (1.1381)  time: 1.329826  data: 0.000345  max mem: 61695
I20250104 13:17:05 92 dinov2 helpers.py:102] Training  [   1250/2500000]  eta: 40 days, 14:57:38  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5859 (inf)  dino_local_crops_loss: 5.7496 (5.6262)  dino_global_crops_loss: 0.7039 (0.6941)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0972 (1.1378)  time: 1.324331  data: 0.000501  max mem: 61695
I20250104 13:17:19 92 dinov2 helpers.py:102] Training  [   1260/2500000]  eta: 40 days, 14:56:18  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0007 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5860 (inf)  dino_local_crops_loss: 5.7521 (5.6272)  dino_global_crops_loss: 0.7043 (0.6942)  koleo_loss: 0.0311 (inf)  ibot_loss: 1.0972 (1.1375)  time: 1.362183  data: 0.000479  max mem: 61695
I20250104 13:17:32 92 dinov2 helpers.py:102] Training  [   1270/2500000]  eta: 40 days, 14:38:55  lr: 0.0007 (0.0003)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0007 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5849 (inf)  dino_local_crops_loss: 5.7521 (5.6282)  dino_global_crops_loss: 0.7045 (0.6943)  koleo_loss: 0.0310 (inf)  ibot_loss: 1.0953 (1.1371)  time: 1.376792  data: 0.000470  max mem: 61695
I20250104 13:17:37 92 dinov2 train.py:278] NaN detected
I20250104 13:21:37 92 dinov2 config.py:67] git:
  sha: fda283ef182296a3187e17b7c6482658c6b64ee3, status: has uncommitted changes, branch: main

I20250104 13:21:37 92 dinov2 config.py:68] config_file: dinov2/configs/train/patch.yaml
eval: 
eval_only: False
local_rank: 2
no_resume: False
opts: ['train.dataset_path=TileDataset:split=TRAIN:root=/ruiyan/yuhao/data', 'train.output_dir=/ruiyan/yuhao/project/FMBC/dinov2/output']
output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
I20250104 13:21:37 92 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.006928203230275509
I20250104 13:21:37 92 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
dino:
  loss_weight: 1.0
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 380
  dataset_path: TileDataset:split=TRAIN:root=/ruiyan/yuhao/data
  output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
  saveckp_freq: 20
  seed: 0
  num_workers: 8
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 2000
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.006928203230275509
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20250104 13:21:37 92 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 13:21:38 92 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 768
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 384
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_base network.
I20250104 13:21:39 92 dinov2 ssl_meta_arch.py:396] DISTRIBUTED FSDP -- preparing model for distributed training
I20250104 13:21:40 92 dinov2 train.py:307] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
)
I20250104 13:21:40 92 dinov2 param_groups.py:54] chunked fsdp
I20250104 13:21:40 92 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.05083731656658002, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.05083731656658002, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.3.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.4.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.1.5.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.6.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.7.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.2.8.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.9.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.10.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] blocks.3.11.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 13:21:40 92 dinov2 param_groups.py:64] else code branch
I20250104 13:21:40 92 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:21:40 92 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 13:21:40 92 dinov2 train.py:106] Schedulers ready.
I20250104 13:21:40 92 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20250104 13:21:40 92 dinov2 augmentations.py:34] ###################################
I20250104 13:21:40 92 dinov2 augmentations.py:35] Using data augmentation parameters:
I20250104 13:21:40 92 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20250104 13:21:40 92 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20250104 13:21:40 92 dinov2 augmentations.py:38] local_crops_number: 8
I20250104 13:21:40 92 dinov2 augmentations.py:39] global_crops_size: 224
I20250104 13:21:40 92 dinov2 augmentations.py:40] local_crops_size: 96
I20250104 13:21:40 92 dinov2 augmentations.py:41] ###################################
I20250104 13:21:40 92 dinov2 loaders.py:87] using dataset: "TileDataset:split=TRAIN:root=/ruiyan/yuhao/data"
I20250104 13:24:21 92 dinov2 config.py:67] git:
  sha: fda283ef182296a3187e17b7c6482658c6b64ee3, status: has uncommitted changes, branch: main

I20250104 13:24:21 92 dinov2 config.py:68] config_file: dinov2/configs/train/patch.yaml
eval: 
eval_only: False
local_rank: 2
no_resume: False
opts: ['train.dataset_path=TileDataset:split=TRAIN:root=/ruiyan/yuhao/data', 'train.output_dir=/ruiyan/yuhao/project/FMBC/dinov2/output']
output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
I20250104 13:24:21 92 dinov2 config.py:26] sqrt scaling learning rate; base: 0.004, new: 0.006928203230275509
I20250104 13:24:21 92 dinov2 config.py:33] MODEL:
  WEIGHTS: ''
compute_precision:
  grad_scaler: true
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp16
dino:
  loss_weight: 1.0
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
  koleo_loss_weight: 0.1
ibot:
  loss_weight: 1.0
  mask_sample_probability: 0.5
  mask_ratio_min_max:
  - 0.1
  - 0.5
  separate_head: false
  head_n_prototypes: 384
  head_bottleneck_dim: 256
  head_nlayers: 3
  head_hidden_dim: 2048
train:
  batch_size_per_gpu: 380
  dataset_path: TileDataset:split=TRAIN:root=/ruiyan/yuhao/data
  output_dir: /ruiyan/yuhao/project/FMBC/dinov2/output
  saveckp_freq: 20
  seed: 0
  num_workers: 8
  OFFICIAL_EPOCH_LENGTH: 1250
  cache_dataset: true
  centering: centering
student:
  arch: vit_base
  patch_size: 16
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: true
  pretrained_weights: ''
  ffn_layer: mlp
  block_chunks: 4
  qkv_bias: true
  proj_bias: true
  ffn_bias: true
  num_register_tokens: 0
  interpolate_antialias: false
  interpolate_offset: 0.1
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30
optim:
  epochs: 2000
  weight_decay: 0.04
  weight_decay_end: 0.4
  base_lr: 0.004
  lr: 0.006928203230275509
  warmup_epochs: 10
  min_lr: 1.0e-06
  clip_grad: 3.0
  freeze_last_layer_epochs: 1
  scaling_rule: sqrt_wrt_1024
  patch_embed_lr_mult: 0.2
  layerwise_decay: 0.9
  adamw_beta1: 0.9
  adamw_beta2: 0.999
crops:
  global_crops_scale:
  - 0.32
  - 1.0
  local_crops_number: 8
  local_crops_scale:
  - 0.05
  - 0.32
  global_crops_size: 224
  local_crops_size: 96
evaluation:
  eval_period_iterations: 12500

I20250104 13:24:21 92 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 13:24:22 92 dinov2 vision_transformer.py:122] using MLP layer as FFN
I20250104 13:24:23 92 dinov2 ssl_meta_arch.py:43] OPTIONS -- architecture : embed_dim: 768
I20250104 13:24:23 92 dinov2 ssl_meta_arch.py:58] OPTIONS -- DINO
I20250104 13:24:23 92 dinov2 ssl_meta_arch.py:60] OPTIONS -- DINO -- loss_weight: 1.0
I20250104 13:24:23 92 dinov2 ssl_meta_arch.py:61] OPTIONS -- DINO -- head_n_prototypes: 384
I20250104 13:24:23 92 dinov2 ssl_meta_arch.py:62] OPTIONS -- DINO -- head_bottleneck_dim: 256
I20250104 13:24:23 92 dinov2 ssl_meta_arch.py:63] OPTIONS -- DINO -- head_hidden_dim: 2048
I20250104 13:24:23 92 dinov2 ssl_meta_arch.py:75] OPTIONS -- DINO -- applying KOLEO regularization
I20250104 13:24:24 92 dinov2 ssl_meta_arch.py:85] OPTIONS -- IBOT
I20250104 13:24:24 92 dinov2 ssl_meta_arch.py:86] OPTIONS -- IBOT -- loss_weight: 1.0
I20250104 13:24:24 92 dinov2 ssl_meta_arch.py:87] OPTIONS -- IBOT masking -- ibot_mask_ratio_tuple: [0.1, 0.5]
I20250104 13:24:24 92 dinov2 ssl_meta_arch.py:88] OPTIONS -- IBOT masking -- ibot_mask_sample_probability: 0.5
I20250104 13:24:24 92 dinov2 ssl_meta_arch.py:111] OPTIONS -- IBOT -- head shared with DINO
I20250104 13:24:24 92 dinov2 ssl_meta_arch.py:121] Student and Teacher are built: they are both vit_base network.
I20250104 13:24:24 92 dinov2 ssl_meta_arch.py:396] DISTRIBUTED FSDP -- preparing model for distributed training
I20250104 13:24:24 92 dinov2 train.py:307] Model:
SSLMetaArch(
  (dino_loss): DINOLoss()
  (koleo_loss): KoLeoLoss(
    (pdist): PairwiseDistance()
  )
  (ibot_patch_loss): iBOTPatchLoss()
  (student): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): DropPath()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): DropPath()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
  (teacher): ModuleDict(
    (backbone): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DinoVisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (blocks): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-2): 3 x Identity()
              (3-5): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-5): 6 x Identity()
              (6-8): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): BlockChunk(
              (0-8): 9 x Identity()
              (9-11): 3 x NestedTensorBlock(
                (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (attn): MemEffAttention(
                  (qkv): Linear(in_features=768, out_features=2304, bias=True)
                  (attn_drop): Dropout(p=0.0, inplace=False)
                  (proj): Linear(in_features=768, out_features=768, bias=True)
                  (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (ls1): LayerScale()
                (drop_path1): Identity()
                (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                  (fc1): Linear(in_features=768, out_features=3072, bias=True)
                  (act): GELU(approximate='none')
                  (fc2): Linear(in_features=3072, out_features=768, bias=True)
                  (drop): Dropout(p=0.0, inplace=False)
                )
                (ls2): LayerScale()
                (drop_path2): Identity()
              )
            )
          )
        )
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (dino_head): FullyShardedDataParallel(
      (_fsdp_wrapped_module): DINOHead(
        (mlp): Sequential(
          (0): Linear(in_features=768, out_features=2048, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=2048, out_features=2048, bias=True)
          (3): GELU(approximate='none')
          (4): Linear(in_features=2048, out_features=256, bias=True)
        )
        (last_layer): Linear(in_features=256, out_features=384, bias=False)
      )
    )
  )
)
I20250104 13:24:24 92 dinov2 param_groups.py:54] chunked fsdp
I20250104 13:24:24 92 dinov2 param_groups.py:87] cls_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] pos_embed: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] mask_token: lr_multiplier: 0.2541865828329001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] patch_embed.proj.weight: lr_multiplier: 0.05083731656658002, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] patch_embed.proj.bias: lr_multiplier: 0.05083731656658002, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.norm1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.norm1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.attn.qkv.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.attn.proj.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.attn.proj.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.ls1.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.norm2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.norm2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc1.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.weight: lr_multiplier: 0.2824295364810001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.mlp.fc2.bias: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.0.ls2.gamma: lr_multiplier: 0.2824295364810001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.norm1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.norm1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.attn.qkv.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.attn.proj.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.attn.proj.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.ls1.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.norm2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.norm2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc1.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.weight: lr_multiplier: 0.31381059609000006, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.mlp.fc2.bias: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.1.ls2.gamma: lr_multiplier: 0.31381059609000006, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.norm1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.norm1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.attn.qkv.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.attn.proj.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.attn.proj.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.ls1.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.norm2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.norm2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc1.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.weight: lr_multiplier: 0.3486784401000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.mlp.fc2.bias: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.0.2.ls2.gamma: lr_multiplier: 0.3486784401000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.norm1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.norm1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.attn.qkv.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.attn.proj.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.attn.proj.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.ls1.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.norm2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.norm2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc1.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.weight: lr_multiplier: 0.3874204890000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.mlp.fc2.bias: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.3.ls2.gamma: lr_multiplier: 0.3874204890000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.norm1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.norm1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.attn.qkv.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.attn.proj.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.attn.proj.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.ls1.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.norm2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.norm2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc1.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.weight: lr_multiplier: 0.4304672100000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.mlp.fc2.bias: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.4.ls2.gamma: lr_multiplier: 0.4304672100000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.norm1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.norm1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.attn.qkv.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.attn.proj.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.attn.proj.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.ls1.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.norm2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.norm2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc1.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.weight: lr_multiplier: 0.4782969000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.mlp.fc2.bias: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.1.5.ls2.gamma: lr_multiplier: 0.4782969000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.norm1.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.norm1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.attn.qkv.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.attn.proj.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.attn.proj.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.ls1.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.norm2.weight: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.norm2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc1.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.weight: lr_multiplier: 0.531441, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.mlp.fc2.bias: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.6.ls2.gamma: lr_multiplier: 0.531441, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.norm1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.norm1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.attn.qkv.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.attn.proj.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.attn.proj.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.ls1.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.norm2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.norm2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc1.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.weight: lr_multiplier: 0.5904900000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.mlp.fc2.bias: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.7.ls2.gamma: lr_multiplier: 0.5904900000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.norm1.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.norm1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.attn.qkv.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.attn.proj.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.attn.proj.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.ls1.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.norm2.weight: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.norm2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc1.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.weight: lr_multiplier: 0.6561, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.mlp.fc2.bias: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.2.8.ls2.gamma: lr_multiplier: 0.6561, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.norm1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.norm1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.attn.qkv.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.attn.proj.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.attn.proj.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.ls1.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.norm2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.norm2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc1.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.weight: lr_multiplier: 0.7290000000000001, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.mlp.fc2.bias: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.9.ls2.gamma: lr_multiplier: 0.7290000000000001, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.norm1.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.norm1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.attn.qkv.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.attn.proj.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.attn.proj.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.ls1.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.norm2.weight: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.norm2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc1.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.weight: lr_multiplier: 0.81, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.mlp.fc2.bias: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.10.ls2.gamma: lr_multiplier: 0.81, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.norm1.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.norm1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.attn.qkv.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.attn.proj.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.attn.proj.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.ls1.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.norm2.weight: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.norm2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc1.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.weight: lr_multiplier: 0.9, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.mlp.fc2.bias: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] blocks.3.11.ls2.gamma: lr_multiplier: 0.9, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] norm.weight: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] norm.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 13:24:24 92 dinov2 param_groups.py:64] else code branch
I20250104 13:24:24 92 dinov2 param_groups.py:87] mlp.0.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] mlp.0.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] mlp.2.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] mlp.2.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] mlp.4.weight: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] mlp.4.bias: lr_multiplier: 1.0, wd_multiplier: 0.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] last_layer.weight_g: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 param_groups.py:87] last_layer.weight_v: lr_multiplier: 1.0, wd_multiplier: 1.0
I20250104 13:24:24 92 dinov2 ssl_meta_arch.py:383] fusing param groups
I20250104 13:24:25 92 dinov2 train.py:106] Schedulers ready.
I20250104 13:24:25 92 fvcore.common.checkpoint checkpoint.py:148] No checkpoint found. Initializing model from scratch
I20250104 13:24:25 92 dinov2 augmentations.py:34] ###################################
I20250104 13:24:25 92 dinov2 augmentations.py:35] Using data augmentation parameters:
I20250104 13:24:25 92 dinov2 augmentations.py:36] global_crops_scale: [0.32, 1.0]
I20250104 13:24:25 92 dinov2 augmentations.py:37] local_crops_scale: [0.05, 0.32]
I20250104 13:24:25 92 dinov2 augmentations.py:38] local_crops_number: 8
I20250104 13:24:25 92 dinov2 augmentations.py:39] global_crops_size: 224
I20250104 13:24:25 92 dinov2 augmentations.py:40] local_crops_size: 96
I20250104 13:24:25 92 dinov2 augmentations.py:41] ###################################
I20250104 13:24:25 92 dinov2 loaders.py:87] using dataset: "TileDataset:split=TRAIN:root=/ruiyan/yuhao/data"
I20250104 13:33:10 92 dinov2 loaders.py:92] # of dataset samples: 92,020,608
I20250104 13:33:10 92 dinov2 loaders.py:125] sampler: sharded infinite
I20250104 13:33:10 92 dinov2 loaders.py:209] using PyTorch data loader
I20250104 13:33:10 92 dinov2 loaders.py:224] infinite data loader
I20250104 13:33:10 92 dinov2 train.py:221] Starting training from iteration 0
I20250104 13:35:43 92 dinov2 helpers.py:102] Training  [      0/2500000]  eta: 4425 days, 8:02:05  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4789 (7.4789)  dino_local_crops_loss: 4.6839 (4.6839)  dino_global_crops_loss: 0.5855 (0.5855)  koleo_loss: 0.7135 (0.7135)  ibot_loss: 1.4959 (1.4959)  time: 152.939575  data: 109.594353  max mem: 61574
I20250104 13:35:56 92 dinov2 helpers.py:102] Training  [     10/2500000]  eta: 436 days, 8:01:36  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.8011 (7.7703)  dino_local_crops_loss: 4.9622 (4.9458)  dino_global_crops_loss: 0.6203 (0.6182)  koleo_loss: 0.7116 (0.6996)  ibot_loss: 1.5083 (1.5067)  time: 15.079779  data: 9.963349  max mem: 61695
I20250104 13:36:14 92 dinov2 helpers.py:102] Training  [     20/2500000]  eta: 253 days, 5:55:37  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.9771 (7.8926)  dino_local_crops_loss: 5.1624 (5.1015)  dino_global_crops_loss: 0.6453 (0.6377)  koleo_loss: 0.6413 (0.6430)  ibot_loss: 1.5132 (1.5104)  time: 1.542921  data: 0.042196  max mem: 61695
I20250104 13:36:29 92 dinov2 helpers.py:102] Training  [     30/2500000]  eta: 185 days, 15:07:25  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 8.0052 (7.9154)  dino_local_crops_loss: 5.3185 (5.1734)  dino_global_crops_loss: 0.6648 (0.6467)  koleo_loss: 0.5033 (0.5842)  ibot_loss: 1.5140 (1.5112)  time: 1.650077  data: 0.042175  max mem: 61695
I20250104 13:36:46 92 dinov2 helpers.py:102] Training  [     40/2500000]  eta: 152 days, 8:41:36  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.9152 (7.9065)  dino_local_crops_loss: 5.3200 (5.2076)  dino_global_crops_loss: 0.6650 (0.6509)  koleo_loss: 0.4167 (0.5385)  ibot_loss: 1.5089 (1.5095)  time: 1.604833  data: 0.000321  max mem: 61695
I20250104 13:37:05 92 dinov2 helpers.py:102] Training  [     50/2500000]  eta: 133 days, 12:23:21  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.8197 (7.8883)  dino_local_crops_loss: 5.3065 (5.2262)  dino_global_crops_loss: 0.6633 (0.6533)  koleo_loss: 0.3584 (0.5027)  ibot_loss: 1.4980 (1.5061)  time: 1.822798  data: 0.000454  max mem: 61695
I20250104 13:37:20 92 dinov2 helpers.py:102] Training  [     60/2500000]  eta: 118 days, 17:29:42  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.7903 (7.8656)  dino_local_crops_loss: 5.2987 (5.2378)  dino_global_crops_loss: 0.6624 (0.6547)  koleo_loss: 0.3428 (0.4716)  ibot_loss: 1.4864 (1.5014)  time: 1.720551  data: 0.000527  max mem: 61695
I20250104 13:37:36 92 dinov2 helpers.py:102] Training  [     70/2500000]  eta: 108 days, 5:02:32  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.6966 (7.8348)  dino_local_crops_loss: 5.2965 (5.2462)  dino_global_crops_loss: 0.6622 (0.6558)  koleo_loss: 0.2700 (0.4376)  ibot_loss: 1.4690 (1.4953)  time: 1.509699  data: 0.000440  max mem: 61695
I20250104 13:37:51 92 dinov2 helpers.py:102] Training  [     80/2500000]  eta: 100 days, 5:11:50  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.5911 (7.7985)  dino_local_crops_loss: 5.3002 (5.2539)  dino_global_crops_loss: 0.6632 (0.6570)  koleo_loss: 0.1774 (0.3994)  ibot_loss: 1.4477 (1.4884)  time: 1.512277  data: 0.000305  max mem: 61695
I20250104 13:38:09 92 dinov2 helpers.py:102] Training  [     90/2500000]  eta: 94 days, 23:42:00  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4859 (7.7612)  dino_local_crops_loss: 5.3063 (5.2594)  dino_global_crops_loss: 0.6646 (0.6577)  koleo_loss: 0.0759 (0.3621)  ibot_loss: 1.4334 (1.4820)  time: 1.660682  data: 0.000341  max mem: 61695
I20250104 13:38:24 92 dinov2 helpers.py:102] Training  [    100/2500000]  eta: 90 days, 0:55:46  lr: 0.0000 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4343 (7.7273)  dino_local_crops_loss: 5.2952 (5.2625)  dino_global_crops_loss: 0.6624 (0.6581)  koleo_loss: 0.0458 (0.3302)  ibot_loss: 1.4282 (1.4764)  time: 1.687348  data: 0.000449  max mem: 61695
I20250104 13:38:40 92 dinov2 helpers.py:102] Training  [    110/2500000]  eta: 85 days, 23:56:08  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.4052 (7.6973)  dino_local_crops_loss: 5.2875 (5.2648)  dino_global_crops_loss: 0.6614 (0.6584)  koleo_loss: 0.0343 (0.3031)  ibot_loss: 1.4192 (1.4710)  time: 1.558567  data: 0.000457  max mem: 61695
I20250104 13:38:57 92 dinov2 helpers.py:102] Training  [    120/2500000]  eta: 83 days, 1:01:04  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3851 (7.6709)  dino_local_crops_loss: 5.2874 (5.2667)  dino_global_crops_loss: 0.6613 (0.6587)  koleo_loss: 0.0253 (0.2800)  ibot_loss: 1.4104 (1.4656)  time: 1.649096  data: 0.000352  max mem: 61695
I20250104 13:39:14 92 dinov2 helpers.py:102] Training  [    130/2500000]  eta: 80 days, 11:38:21  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3761 (7.6481)  dino_local_crops_loss: 5.2895 (5.2686)  dino_global_crops_loss: 0.6615 (0.6589)  koleo_loss: 0.0209 (0.2602)  ibot_loss: 1.4033 (1.4605)  time: 1.724470  data: 0.000273  max mem: 61695
I20250104 13:39:29 92 dinov2 helpers.py:102] Training  [    140/2500000]  eta: 77 days, 21:00:54  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3677 (7.6278)  dino_local_crops_loss: 5.2926 (5.2704)  dino_global_crops_loss: 0.6620 (0.6591)  koleo_loss: 0.0184 (0.2430)  ibot_loss: 1.3936 (1.4553)  time: 1.611298  data: 0.000262  max mem: 61695
I20250104 13:39:45 92 dinov2 helpers.py:102] Training  [    150/2500000]  eta: 75 days, 14:46:16  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3556 (7.6094)  dino_local_crops_loss: 5.2953 (5.2721)  dino_global_crops_loss: 0.6625 (0.6594)  koleo_loss: 0.0165 (0.2279)  ibot_loss: 1.3817 (1.4500)  time: 1.511206  data: 0.000317  max mem: 61695
I20250104 13:40:04 92 dinov2 helpers.py:102] Training  [    160/2500000]  eta: 74 days, 11:11:00  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3410 (7.5923)  dino_local_crops_loss: 5.2971 (5.2737)  dino_global_crops_loss: 0.6630 (0.6596)  koleo_loss: 0.0139 (0.2145)  ibot_loss: 1.3666 (1.4445)  time: 1.743054  data: 0.000315  max mem: 61695
I20250104 13:40:24 92 dinov2 helpers.py:102] Training  [    170/2500000]  eta: 73 days, 8:41:49  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3300 (7.5766)  dino_local_crops_loss: 5.2982 (5.2751)  dino_global_crops_loss: 0.6633 (0.6598)  koleo_loss: 0.0118 (0.2026)  ibot_loss: 1.3559 (1.4390)  time: 1.947799  data: 0.000334  max mem: 61695
I20250104 13:40:38 92 dinov2 helpers.py:102] Training  [    180/2500000]  eta: 71 days, 15:19:09  lr: 0.0001 (0.0000)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.3163 (7.5615)  dino_local_crops_loss: 5.2987 (5.2765)  dino_global_crops_loss: 0.6634 (0.6600)  koleo_loss: 0.0095 (0.1919)  ibot_loss: 1.3444 (1.4331)  time: 1.689424  data: 0.000334  max mem: 61695
I20250104 13:40:53 92 dinov2 helpers.py:102] Training  [    190/2500000]  eta: 70 days, 2:08:06  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2998 (7.5477)  dino_local_crops_loss: 5.2988 (5.2776)  dino_global_crops_loss: 0.6633 (0.6602)  koleo_loss: 0.0073 (0.1822)  ibot_loss: 1.3285 (1.4277)  time: 1.455305  data: 0.000294  max mem: 61695
I20250104 13:41:09 92 dinov2 helpers.py:102] Training  [    200/2500000]  eta: 68 days, 23:33:39  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2921 (7.5349)  dino_local_crops_loss: 5.2988 (5.2787)  dino_global_crops_loss: 0.6633 (0.6604)  koleo_loss: 0.0054 (0.1734)  ibot_loss: 1.3244 (1.4225)  time: 1.553414  data: 0.000344  max mem: 61695
I20250104 13:41:26 92 dinov2 helpers.py:102] Training  [    210/2500000]  eta: 68 days, 1:34:15  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2824 (7.5226)  dino_local_crops_loss: 5.2986 (5.2796)  dino_global_crops_loss: 0.6632 (0.6605)  koleo_loss: 0.0039 (0.1653)  ibot_loss: 1.3171 (1.4171)  time: 1.684815  data: 0.000303  max mem: 61695
I20250104 13:41:41 92 dinov2 helpers.py:102] Training  [    220/2500000]  eta: 66 days, 21:07:00  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2745 (7.5113)  dino_local_crops_loss: 5.2983 (5.2805)  dino_global_crops_loss: 0.6632 (0.6606)  koleo_loss: 0.0025 (0.1579)  ibot_loss: 1.3099 (1.4123)  time: 1.581670  data: 0.000311  max mem: 61695
I20250104 13:41:56 92 dinov2 helpers.py:102] Training  [    230/2500000]  eta: 65 days, 22:18:40  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2680 (7.5005)  dino_local_crops_loss: 5.2981 (5.2812)  dino_global_crops_loss: 0.6631 (0.6607)  koleo_loss: 0.0003 (0.1510)  ibot_loss: 1.3068 (1.4075)  time: 1.500128  data: 0.000388  max mem: 61695
I20250104 13:42:12 92 dinov2 helpers.py:102] Training  [    240/2500000]  eta: 65 days, 0:46:18  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2555 (7.4903)  dino_local_crops_loss: 5.2979 (5.2819)  dino_global_crops_loss: 0.6631 (0.6608)  koleo_loss: -0.0014 (0.1447)  ibot_loss: 1.2973 (1.4029)  time: 1.542246  data: 0.000303  max mem: 61695
I20250104 13:42:28 92 dinov2 helpers.py:102] Training  [    250/2500000]  eta: 64 days, 7:24:10  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2554 (7.4809)  dino_local_crops_loss: 5.2976 (5.2825)  dino_global_crops_loss: 0.6630 (0.6609)  koleo_loss: -0.0034 (0.1387)  ibot_loss: 1.2968 (1.3987)  time: 1.575742  data: 0.000328  max mem: 61695
I20250104 13:42:44 92 dinov2 helpers.py:102] Training  [    260/2500000]  eta: 63 days, 14:17:07  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2535 (7.4722)  dino_local_crops_loss: 5.2974 (5.2831)  dino_global_crops_loss: 0.6629 (0.6610)  koleo_loss: -0.0050 (0.1332)  ibot_loss: 1.2982 (1.3949)  time: 1.599825  data: 0.000412  max mem: 61695
I20250104 13:42:59 92 dinov2 helpers.py:102] Training  [    270/2500000]  eta: 62 days, 21:32:23  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2467 (7.4637)  dino_local_crops_loss: 5.2971 (5.2836)  dino_global_crops_loss: 0.6629 (0.6611)  koleo_loss: -0.0070 (0.1280)  ibot_loss: 1.2943 (1.3910)  time: 1.562161  data: 0.000312  max mem: 61695
I20250104 13:43:17 92 dinov2 helpers.py:102] Training  [    280/2500000]  eta: 62 days, 12:15:02  lr: 0.0001 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2404 (7.4557)  dino_local_crops_loss: 5.2968 (5.2841)  dino_global_crops_loss: 0.6629 (0.6611)  koleo_loss: -0.0086 (0.1231)  ibot_loss: 1.2890 (1.3874)  time: 1.671537  data: 0.000375  max mem: 61695
I20250104 13:43:33 92 dinov2 helpers.py:102] Training  [    290/2500000]  eta: 61 days, 22:38:13  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2384 (7.4482)  dino_local_crops_loss: 5.2965 (5.2845)  dino_global_crops_loss: 0.6629 (0.6612)  koleo_loss: -0.0097 (0.1185)  ibot_loss: 1.2880 (1.3840)  time: 1.694306  data: 0.000462  max mem: 61695
I20250104 13:43:52 92 dinov2 helpers.py:102] Training  [    300/2500000]  eta: 61 days, 15:58:26  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2347 (7.4410)  dino_local_crops_loss: 5.2962 (5.2849)  dino_global_crops_loss: 0.6627 (0.6612)  koleo_loss: -0.0111 (0.1142)  ibot_loss: 1.2872 (1.3807)  time: 1.721372  data: 0.000410  max mem: 61695
I20250104 13:44:06 92 dinov2 helpers.py:102] Training  [    310/2500000]  eta: 61 days, 0:34:24  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2267 (7.4341)  dino_local_crops_loss: 5.2958 (5.2852)  dino_global_crops_loss: 0.6627 (0.6613)  koleo_loss: -0.0123 (0.1101)  ibot_loss: 1.2819 (1.3775)  time: 1.647165  data: 0.000333  max mem: 61695
I20250104 13:44:21 92 dinov2 helpers.py:102] Training  [    320/2500000]  eta: 60 days, 10:43:05  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2296 (7.4279)  dino_local_crops_loss: 5.2951 (5.2855)  dino_global_crops_loss: 0.6627 (0.6613)  koleo_loss: -0.0133 (0.1063)  ibot_loss: 1.2847 (1.3748)  time: 1.455442  data: 0.000298  max mem: 61695
I20250104 13:44:35 92 dinov2 helpers.py:102] Training  [    330/2500000]  eta: 59 days, 21:40:47  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2309 (7.4218)  dino_local_crops_loss: 5.2943 (5.2858)  dino_global_crops_loss: 0.6626 (0.6614)  koleo_loss: -0.0143 (0.1026)  ibot_loss: 1.2897 (1.3721)  time: 1.468512  data: 0.000445  max mem: 61695
I20250104 13:44:54 92 dinov2 helpers.py:102] Training  [    340/2500000]  eta: 59 days, 17:42:20  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2224 (7.4159)  dino_local_crops_loss: 5.2932 (5.2860)  dino_global_crops_loss: 0.6625 (0.6614)  koleo_loss: -0.0151 (0.0991)  ibot_loss: 1.2813 (1.3694)  time: 1.671845  data: 0.000567  max mem: 61695
I20250104 13:45:11 92 dinov2 helpers.py:102] Training  [    350/2500000]  eta: 59 days, 9:25:01  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2225 (7.4105)  dino_local_crops_loss: 5.2917 (5.2861)  dino_global_crops_loss: 0.6624 (0.6614)  koleo_loss: -0.0159 (0.0959)  ibot_loss: 1.2832 (1.3671)  time: 1.760874  data: 0.000466  max mem: 61695
I20250104 13:45:25 92 dinov2 helpers.py:102] Training  [    360/2500000]  eta: 58 days, 21:52:08  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2277 (7.4055)  dino_local_crops_loss: 5.2905 (5.2862)  dino_global_crops_loss: 0.6623 (0.6614)  koleo_loss: -0.0167 (0.0927)  ibot_loss: 1.2923 (1.3651)  time: 1.549445  data: 0.000386  max mem: 61695
I20250104 13:45:42 92 dinov2 helpers.py:102] Training  [    370/2500000]  eta: 58 days, 15:10:19  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2249 (7.4005)  dino_local_crops_loss: 5.2897 (5.2863)  dino_global_crops_loss: 0.6622 (0.6615)  koleo_loss: -0.0173 (0.0898)  ibot_loss: 1.2903 (1.3630)  time: 1.565762  data: 0.000433  max mem: 61695
I20250104 13:45:57 92 dinov2 helpers.py:102] Training  [    380/2500000]  eta: 58 days, 6:40:00  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2202 (7.3958)  dino_local_crops_loss: 5.2893 (5.2864)  dino_global_crops_loss: 0.6621 (0.6615)  koleo_loss: -0.0178 (0.0869)  ibot_loss: 1.2863 (1.3610)  time: 1.619484  data: 0.000387  max mem: 61695
I20250104 13:46:12 92 dinov2 helpers.py:102] Training  [    390/2500000]  eta: 57 days, 21:21:49  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2232 (7.3914)  dino_local_crops_loss: 5.2890 (5.2864)  dino_global_crops_loss: 0.6621 (0.6615)  koleo_loss: -0.0181 (0.0842)  ibot_loss: 1.2895 (1.3592)  time: 1.525522  data: 0.000340  max mem: 61695
I20250104 13:46:27 92 dinov2 helpers.py:102] Training  [    400/2500000]  eta: 57 days, 12:09:34  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2217 (7.3871)  dino_local_crops_loss: 5.2885 (5.2865)  dino_global_crops_loss: 0.6620 (0.6615)  koleo_loss: -0.0183 (0.0817)  ibot_loss: 1.2901 (1.3574)  time: 1.480281  data: 0.000328  max mem: 61695
I20250104 13:46:44 92 dinov2 helpers.py:102] Training  [    410/2500000]  eta: 57 days, 7:25:48  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2215 (7.3831)  dino_local_crops_loss: 5.2869 (5.2865)  dino_global_crops_loss: 0.6619 (0.6615)  koleo_loss: -0.0192 (0.0792)  ibot_loss: 1.2901 (1.3559)  time: 1.588936  data: 0.000271  max mem: 61695
I20250104 13:47:00 92 dinov2 helpers.py:102] Training  [    420/2500000]  eta: 57 days, 0:09:50  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2187 (7.3792)  dino_local_crops_loss: 5.2857 (5.2864)  dino_global_crops_loss: 0.6617 (0.6615)  koleo_loss: -0.0197 (0.0769)  ibot_loss: 1.2914 (1.3544)  time: 1.624411  data: 0.000414  max mem: 61695
I20250104 13:47:14 92 dinov2 helpers.py:102] Training  [    430/2500000]  eta: 56 days, 15:32:24  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2191 (7.3755)  dino_local_crops_loss: 5.2843 (5.2864)  dino_global_crops_loss: 0.6616 (0.6615)  koleo_loss: -0.0200 (0.0746)  ibot_loss: 1.2933 (1.3530)  time: 1.488104  data: 0.000506  max mem: 61695
I20250104 13:47:30 92 dinov2 helpers.py:102] Training  [    440/2500000]  eta: 56 days, 9:59:46  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2191 (7.3718)  dino_local_crops_loss: 5.2824 (5.2862)  dino_global_crops_loss: 0.6614 (0.6615)  koleo_loss: -0.0204 (0.0725)  ibot_loss: 1.2933 (1.3516)  time: 1.520900  data: 0.000358  max mem: 61695
I20250104 13:47:47 92 dinov2 helpers.py:102] Training  [    450/2500000]  eta: 56 days, 6:06:55  lr: 0.0002 (0.0001)  wd: 0.0400 (0.0400)  mom: 0.9940 (0.9940)  last_layer_lr: 0.0000 (0.0000)  current_batch_size: 380.0000 (380.0000)  total_loss: 7.2085 (7.3682)  dino_local_crops_loss: 5.2788 (5.2861)  dino_global_crops_loss: 0.6610 (0.6615)  koleo_loss: -0.0206 (0.0704)  ibot_loss: 1.2885 (1.3502)  time: 1.652334  data: 0.000351  max mem: 61695
