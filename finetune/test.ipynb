{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cluster import KMeans\n",
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import ModuleList\n",
    "from functools import partial\n",
    "from typing import List, Tuple, Dict, Optional, Any\n",
    "from torchvision.transforms import Compose, RandomApply\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.transforms.transforms import _setup_angle, _check_sequence_input\n",
    "from torch import Tensor\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from PIL import ImageFilter, ImageOps, Image, ImageDraw\n",
    "\n",
    "import os\n",
    "# 设定实验结果的目录 (请修改为你的实际路径)\n",
    "result_dir = \"/home/yuhaowang/project/FMBC/finetune/outputs/BRACS_Fine\"\n",
    "\n",
    "# 需要展示的评估指标\n",
    "evaluation_metrics = ['val_bacc', 'val_weighted_f1', 'val_macro_auroc']\n",
    "\n",
    "# 你希望的模型顺序（从上到下）\n",
    "desired_order = [\n",
    "    \"UNI\", \"CONCH\", \"Virchow\",\"Gigapath_Tile\",'Gigapath',\"CHIEF_Tile\",\"TITAN\",\"FMBC\"  # 请修改为你的模型名称\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# 遍历目录中的所有模型文件夹\n",
    "for model_name in os.listdir(result_dir):\n",
    "    model_path = os.path.join(result_dir, model_name, \"summary.csv\")\n",
    "    \n",
    "    # 检查是否存在 summary.csv 文件\n",
    "    if os.path.isfile(model_path):\n",
    "        df = pd.read_csv(model_path)\n",
    "\n",
    "        # 计算均值和标准差\n",
    "        summary_stats = {\"Model\": model_name}\n",
    "        for metric in evaluation_metrics:\n",
    "            if metric in df.columns:\n",
    "                mean_val = np.mean(df[metric])\n",
    "                std_val = np.std(df[metric], ddof=1)  # 样本标准差\n",
    "                summary_stats[metric] = f\"{mean_val:.3f}±{std_val:.4f}\"\n",
    "\n",
    "        # 添加到列表\n",
    "        all_results.append(summary_stats)\n",
    "\n",
    "# 转换为 DataFrame\n",
    "final_result_df = pd.DataFrame(all_results)\n",
    "\n",
    "# 按照提供的模型顺序排序\n",
    "final_result_df['sort_order'] = final_result_df['Model'].apply(lambda x: desired_order.index(x) if x in desired_order else len(desired_order))\n",
    "#delete the model not in desired_order\n",
    "final_result_df = final_result_df[final_result_df['sort_order']!=len(desired_order)]\n",
    "final_result_df = final_result_df.sort_values(by='sort_order').drop(columns=['sort_order'])\n",
    "final_result_df.style.hide(axis=\"index\")\n",
    "# 在 Jupyter Notebook 中美观显示\n",
    "display(final_result_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = '/home/yuhaowang/data/processed_data'\n",
    "embedding_path ='/data1/embedding'\n",
    "for dataset in os.listdir(data_path):\n",
    "    print(dataset,'has',len(os.listdir(os.path.join(data_path, dataset, 'output'))),'slides')\n",
    "    dataset_embedding_path = os.path.join(embedding_path, dataset)\n",
    "    if not os.path.exists(dataset_embedding_path):\n",
    "        print(dataset, 'has no embedding')\n",
    "        print('---------------------------')\n",
    "\n",
    "        continue\n",
    "    for model in os.listdir(dataset_embedding_path):\n",
    "        print(model,'has',len(os.listdir(os.path.join(dataset_embedding_path, model))),'slides processed')\n",
    "    print('---------------------------')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  h5py\n",
    "def read_assets_from_h5( h5_path: str) -> tuple:\n",
    "    '''Read the assets from the h5 file'''\n",
    "    assets = {}\n",
    "    attrs = {}\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            assets[key] = f[key][:]\n",
    "            if f[key].attrs is not None:\n",
    "                attrs[key] = dict(f[key].attrs)\n",
    "    return assets, attrs\n",
    "\n",
    "data_dir = '/data1/embedding/TCGA-BRCA'\n",
    "for model in os.listdir(data_dir):\n",
    "    embedding_path=os.path.join(data_dir,model)\n",
    "    test_case = os.listdir(embedding_path)[0]\n",
    "    test_data,_ = read_assets_from_h5(os.path.join(embedding_path,test_case))\n",
    "    print(test_data.keys())\n",
    "    print(model)\n",
    "    print(test_data['features'].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 174\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Test C-index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_test_cindex\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[43mtrain_survival_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 144\u001b[0m, in \u001b[0;36mtrain_survival_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    143\u001b[0m model \u001b[38;5;241m=\u001b[39m SurvivalModel(input_dim, hidden_dim, num_bins)\n\u001b[0;32m--> 144\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m    146\u001b[0m criterion \u001b[38;5;241m=\u001b[39m NLLSurvLoss()\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/UNI/lib/python3.10/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lifelines.utils import concordance_index\n",
    "#CUDA error: device-side assert triggered\n",
    "import os\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# Loss Function (provided by you)\n",
    "class NLLSurvLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.0, eps=1e-7, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def __call__(self, x, y_bins, y_event):\n",
    "        y_event = y_event.type(torch.int64)\n",
    "        y_bins = y_bins.type(torch.int64)\n",
    "        y_censor = 1 - y_event\n",
    "        hazards = torch.sigmoid(x)\n",
    "        S = torch.cumprod(1 - hazards, dim=1)\n",
    "        S_padded = torch.cat([torch.ones_like(y_censor), S], 1)\n",
    "        s_prev = torch.gather(S_padded, dim=1, index=y_bins).clamp(min=self.eps)\n",
    "        h_this = torch.gather(hazards, dim=1, index=y_bins).clamp(min=self.eps)\n",
    "        s_this = torch.gather(S_padded, dim=1, index=y_bins + 1).clamp(min=self.eps)\n",
    "        uncensored_loss = -(1 - y_censor) * (torch.log(s_prev) + torch.log(h_this))\n",
    "        censored_loss = -y_censor * torch.log(s_this)\n",
    "        loss = uncensored_loss + self.alpha * censored_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid reduction type: {self.reduction}\")\n",
    "\n",
    "\n",
    "\n",
    "# Model Class\n",
    "class SurvivalModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)  # Output dim = number of time bins\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# C-index calculation\n",
    "def calculate_cindex(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_times = []\n",
    "    all_events = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            features = batch['features'].to(device)\n",
    "            events = batch['event'].numpy()\n",
    "            times = batch['time_bin'].numpy()\n",
    "            logits = model(features)\n",
    "            hazards = torch.sigmoid(logits)\n",
    "            survival = torch.cumprod(1 - hazards, dim=1)\n",
    "            risk_scores = -survival.sum(dim=1).cpu().numpy()  # Negative sum of survival as risk score\n",
    "            \n",
    "            all_preds.extend(risk_scores)\n",
    "            all_times.extend(times.flatten())\n",
    "            all_events.extend(events.flatten())\n",
    "    \n",
    "    cindex = concordance_index(all_times, all_preds, all_events)\n",
    "    return cindex\n",
    "\n",
    "# Main training loop\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lifelines.utils import concordance_index\n",
    "import os\n",
    "\n",
    "\n",
    "# SurvivalDataset 修改版\n",
    "class SurvivalDataset(Dataset):\n",
    "    def __init__(self, data_file, time_bins):\n",
    "        df = pd.read_csv(data_file)\n",
    "        df = df.dropna(subset=['OS_MONTHS'])\n",
    "        self.features = torch.tensor(df['Sex'].values, dtype=torch.float32).unsqueeze(1)\n",
    "        self.time_bins = time_bins\n",
    "        \n",
    "        event_np = df['OS_STATUS'].apply(lambda x: 1 if 'DECEASED' in x else 0).values\n",
    "        time_np = df['OS_MONTHS'].values\n",
    "        \n",
    "        binned_time = np.digitize(time_np, bins=time_bins[:-1]) - 1\n",
    "        num_bins = len(time_bins) - 1\n",
    "        binned_time = np.where(event_np == 1, \n",
    "                              np.clip(binned_time, 0, num_bins - 1), \n",
    "                              np.clip(binned_time, 0, num_bins))\n",
    "        self.binned_time = torch.tensor(binned_time, dtype=torch.int64).unsqueeze(1)\n",
    "        self.event = torch.tensor(event_np, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': self.features[idx],\n",
    "            'event': self.event[idx],\n",
    "            'time_bin': self.binned_time[idx]\n",
    "        }\n",
    "\n",
    "# 主训练函数修改版\n",
    "def train_survival_model():\n",
    "    input_dim = 1\n",
    "    hidden_dim = 32\n",
    "    num_bins = 10\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.001\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    data_file = './TCGA-BRCA-KM.csv'\n",
    "    df = pd.read_csv(data_file)\n",
    "    max_time = df['OS_MONTHS'].max() + 1\n",
    "    time_bins = np.linspace(0, max_time, num_bins + 1)\n",
    "\n",
    "    dataset = SurvivalDataset(data_file, time_bins)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = SurvivalModel(input_dim, hidden_dim, num_bins)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = NLLSurvLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            time_bins = batch['time_bin'].to(device)\n",
    "            events = batch['event'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, time_bins, events)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_cindex = calculate_cindex(model, train_loader, device)\n",
    "        test_cindex = calculate_cindex(model, test_loader, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, '\n",
    "              f'Train C-index: {train_cindex:.4f}, Test C-index: {test_cindex:.4f}')\n",
    "\n",
    "    final_test_cindex = calculate_cindex(model, test_loader, device)\n",
    "    print(f'Final Test C-index: {final_test_cindex:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_survival_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = '/home/yuhaowang/data/'\n",
    "save_dir = '/data4/embedding'\n",
    "def get_unprocessed_datasets(data_dir, processed_dir):\n",
    "    \"\"\"获取未处理的数据集\"\"\"\n",
    "    all_datasets = os.listdir(data_dir)\n",
    "    processed_datasets = os.listdir(processed_dir) if os.path.exists(processed_dir) else []\n",
    "    \n",
    "    #unprocessd_dataset= [d for d in all_datasets if len(os.listdir(os.path.join(data_dir, d,'output'))) != len(os.listdir(os.path.join(processed_dir, d, 'FMBC')))]\n",
    "    unprocessed_dataset = []\n",
    "    for d in all_datasets:\n",
    "        if len(os.listdir(os.path.join(data_dir, d, 'output'))) - len(os.listdir(os.path.join(processed_dir, d, 'FMBC')))>10:\n",
    "            unprocessed_dataset.append(d)\n",
    "    return unprocessed_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_unprocessed_datasets(data_dir, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
