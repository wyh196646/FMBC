{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "from training import train\n",
    "from params import get_finetune_params\n",
    "from task_configs.utils import load_task_config\n",
    "from finetune_utils import seed_torch, get_exp_code, get_splits, get_loader, save_obj\n",
    "from datasets.slide_datatset import SlideDataset\n",
    "sys.path.append(os.path.abspath('../dino_stage2'))\n",
    "from pprint import pprint\n",
    "\n",
    "from dino_stage2.wsi_dataset import WSIDataset\n",
    "from dino_stage2.eval_knn import extract_features\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_embedding_dir='/home/yuhaowang/WSI_Traing/data'\n",
    "tile_embedding_dataset=WSIDataset(tile_embedding_dir,stage='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(tile_embedding_dataset)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case='/home/yuhaowang/WSI_Traing/data/TCGA-05-4422-01Z-00-DX1.4802093f-71ea-43d5-bd92-b4bd7fc8c5bf.npy'\n",
    "ori_data=np.load(test_case,allow_pickle=True).item()\n",
    "ori_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "len(os.listdir('/home/yuhaowang/data/test/TCGA-LUAD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将我们的数据处理为匹配provgigapath的，而不是修改gigapath的Dataset类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imgs': tensor([[-0.0024,  0.0925, -0.1277,  ..., -0.0612, -0.0490,  0.0461],\n",
      "        [ 0.0216,  0.0510, -0.1759,  ..., -0.0195, -0.0025,  0.1390],\n",
      "        [ 0.0592,  0.1081, -0.1108,  ..., -0.0039,  0.0420,  0.1062],\n",
      "        ...,\n",
      "        [-0.0092, -0.0077,  0.0332,  ...,  0.0004, -0.0202, -0.0437],\n",
      "        [-0.0183,  0.0641, -0.1471,  ..., -0.0219, -0.0289,  0.0578],\n",
      "        [ 0.0275,  0.1116, -0.1241,  ..., -0.0008,  0.0439,  0.0720]]), 'img_lens': 7767, 'pad_mask': 0, 'coords': tensor([[ -168,   836],\n",
      "        [ -168, 90950],\n",
      "        [ -168, 91974],\n",
      "        ...,\n",
      "        [99162, 78662],\n",
      "        [99162, 90950],\n",
      "        [99162, 91974]])}\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "def shuffle_data(images: torch.Tensor, coords: torch.Tensor) -> tuple:\n",
    "    '''Shuffle the serialized images and coordinates'''\n",
    "    indices = torch.randperm(len(images))\n",
    "    images_ = images[indices]\n",
    "    coords_ = coords[indices]\n",
    "    return images_, coords_\n",
    "\n",
    "def read_assets_from_h5( h5_path: str) -> tuple:\n",
    "    '''Read the assets from the h5 file'''\n",
    "    assets = {}\n",
    "    attrs = {}\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            assets[key] = f[key][:]\n",
    "            if f[key].attrs is not None:\n",
    "                attrs[key] = dict(f[key].attrs)\n",
    "    return assets, attrs\n",
    "\n",
    "def get_images_from_path(img_path: str,shuffle_tiles=False,max_tiles=1000000) -> dict:\n",
    "    '''Get the images from the path'''\n",
    "    if '.pt' in img_path:\n",
    "        images = torch.load(img_path)\n",
    "        coords = 0\n",
    "    elif '.h5' in img_path:\n",
    "        assets, _ = read_assets_from_h5(img_path)\n",
    "        images = torch.from_numpy(assets['features'])\n",
    "        coords = torch.from_numpy(assets['coords'])\n",
    "\n",
    "        # if shuffle the data\n",
    "        if shuffle_tiles:\n",
    "            images, coords = shuffle_data(images, coords)\n",
    "\n",
    "        if images.size(0) > max_tiles:\n",
    "            images = images[:max_tiles, :]\n",
    "        if coords.size(0) > max_tiles:\n",
    "            coords = coords[:max_tiles, :]\n",
    "    \n",
    "    # set the input dict\n",
    "    data_dict = {'imgs': images,\n",
    "            'img_lens': images.size(0),\n",
    "            'pad_mask': 0,\n",
    "            'coords': coords}\n",
    "    return data_dict\n",
    "\n",
    "import os\n",
    "test_path='/home/yuhaowang/data/embedding/TCGA-TOY'\n",
    "file_list=os.listdir(test_path)\n",
    "\n",
    "file_case=os.path.join(test_path,file_list[0])\n",
    "data_dict=get_images_from_path(file_case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imgs': tensor([[-0.0024,  0.0925, -0.1277,  ..., -0.0612, -0.0490,  0.0461],\n",
       "         [ 0.0216,  0.0510, -0.1759,  ..., -0.0195, -0.0025,  0.1390],\n",
       "         [ 0.0592,  0.1081, -0.1108,  ..., -0.0039,  0.0420,  0.1062],\n",
       "         ...,\n",
       "         [-0.0092, -0.0077,  0.0332,  ...,  0.0004, -0.0202, -0.0437],\n",
       "         [-0.0183,  0.0641, -0.1471,  ..., -0.0219, -0.0289,  0.0578],\n",
       "         [ 0.0275,  0.1116, -0.1241,  ..., -0.0008,  0.0439,  0.0720]]),\n",
       " 'img_lens': 7767,\n",
       " 'pad_mask': 0,\n",
       " 'coords': tensor([[ -168,   836],\n",
       "         [ -168, 90950],\n",
       "         [ -168, 91974],\n",
       "         ...,\n",
       "         [99162, 78662],\n",
       "         [99162, 90950],\n",
       "         [99162, 91974]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2016.,  4520.],\n",
       "       [ 2272.,  4520.],\n",
       "       [ 1760.,  4776.],\n",
       "       [ 2016.,  4776.],\n",
       "       [ 2272.,  4776.],\n",
       "       [ 1760.,  5032.],\n",
       "       [ 2016.,  5032.],\n",
       "       [ 2272.,  5032.],\n",
       "       [ 1760.,  5288.],\n",
       "       [ 2016.,  5288.],\n",
       "       [ 2272.,  5288.],\n",
       "       [ 1760.,  5544.],\n",
       "       [ 2016.,  5544.],\n",
       "       [ 2272.,  5544.],\n",
       "       [ 1760.,  5800.],\n",
       "       [ 2016.,  5800.],\n",
       "       [ 2272.,  5800.],\n",
       "       [ 1760.,  6056.],\n",
       "       [ 2016.,  6056.],\n",
       "       [ 2272.,  6056.],\n",
       "       [ 2528.,  6056.],\n",
       "       [ 1760.,  6312.],\n",
       "       [ 2016.,  6312.],\n",
       "       [ 2272.,  6312.],\n",
       "       [ 2528.,  6312.],\n",
       "       [ 1504.,  6568.],\n",
       "       [ 1760.,  6568.],\n",
       "       [ 2016.,  6568.],\n",
       "       [ 2272.,  6568.],\n",
       "       [ 2528.,  6568.],\n",
       "       [ 2784.,  6568.],\n",
       "       [ 1504.,  6824.],\n",
       "       [ 1760.,  6824.],\n",
       "       [ 2016.,  6824.],\n",
       "       [ 2272.,  6824.],\n",
       "       [ 2528.,  6824.],\n",
       "       [ 2784.,  6824.],\n",
       "       [ 1504.,  7080.],\n",
       "       [ 1760.,  7080.],\n",
       "       [ 2016.,  7080.],\n",
       "       [ 2272.,  7080.],\n",
       "       [ 2528.,  7080.],\n",
       "       [ 2784.,  7080.],\n",
       "       [ 3040.,  7080.],\n",
       "       [ 1504.,  7336.],\n",
       "       [ 1760.,  7336.],\n",
       "       [ 2016.,  7336.],\n",
       "       [ 2272.,  7336.],\n",
       "       [ 2528.,  7336.],\n",
       "       [ 2784.,  7336.],\n",
       "       [ 3040.,  7336.],\n",
       "       [ 3296.,  7336.],\n",
       "       [ 1760.,  7592.],\n",
       "       [ 2016.,  7592.],\n",
       "       [ 2272.,  7592.],\n",
       "       [ 2528.,  7592.],\n",
       "       [ 2784.,  7592.],\n",
       "       [ 3040.,  7592.],\n",
       "       [ 3296.,  7592.],\n",
       "       [ 1760.,  7848.],\n",
       "       [ 2016.,  7848.],\n",
       "       [ 2272.,  7848.],\n",
       "       [ 2528.,  7848.],\n",
       "       [ 2784.,  7848.],\n",
       "       [ 3040.,  7848.],\n",
       "       [ 3296.,  7848.],\n",
       "       [ 3552.,  7848.],\n",
       "       [ 1760.,  8104.],\n",
       "       [ 2016.,  8104.],\n",
       "       [ 2272.,  8104.],\n",
       "       [ 2528.,  8104.],\n",
       "       [ 2784.,  8104.],\n",
       "       [ 3040.,  8104.],\n",
       "       [ 3296.,  8104.],\n",
       "       [ 3552.,  8104.],\n",
       "       [ 2016.,  8360.],\n",
       "       [ 2272.,  8360.],\n",
       "       [ 2528.,  8360.],\n",
       "       [ 2784.,  8360.],\n",
       "       [ 3040.,  8360.],\n",
       "       [ 3296.,  8360.],\n",
       "       [ 3552.,  8360.],\n",
       "       [ 2272.,  8616.],\n",
       "       [ 2528.,  8616.],\n",
       "       [ 2784.,  8616.],\n",
       "       [ 3040.,  8616.],\n",
       "       [ 3296.,  8616.],\n",
       "       [ 3552.,  8616.],\n",
       "       [ 3808.,  8616.],\n",
       "       [ 2272.,  8872.],\n",
       "       [ 2528.,  8872.],\n",
       "       [ 2784.,  8872.],\n",
       "       [ 3040.,  8872.],\n",
       "       [ 3296.,  8872.],\n",
       "       [ 3552.,  8872.],\n",
       "       [ 3808.,  8872.],\n",
       "       [ 4064.,  8872.],\n",
       "       [ 2272.,  9128.],\n",
       "       [ 2528.,  9128.],\n",
       "       [ 2784.,  9128.],\n",
       "       [ 3040.,  9128.],\n",
       "       [ 3296.,  9128.],\n",
       "       [ 3552.,  9128.],\n",
       "       [ 3808.,  9128.],\n",
       "       [ 4064.,  9128.],\n",
       "       [ 2528.,  9384.],\n",
       "       [ 2784.,  9384.],\n",
       "       [ 3040.,  9384.],\n",
       "       [ 3296.,  9384.],\n",
       "       [ 3552.,  9384.],\n",
       "       [ 3808.,  9384.],\n",
       "       [ 4064.,  9384.],\n",
       "       [ 4320.,  9384.],\n",
       "       [ 2528.,  9640.],\n",
       "       [ 2784.,  9640.],\n",
       "       [ 3040.,  9640.],\n",
       "       [ 3296.,  9640.],\n",
       "       [ 3552.,  9640.],\n",
       "       [ 3808.,  9640.],\n",
       "       [ 4064.,  9640.],\n",
       "       [ 4320.,  9640.],\n",
       "       [ 4576.,  9640.],\n",
       "       [ 3040.,  9896.],\n",
       "       [ 3296.,  9896.],\n",
       "       [ 3552.,  9896.],\n",
       "       [ 3808.,  9896.],\n",
       "       [ 4064.,  9896.],\n",
       "       [ 4320.,  9896.],\n",
       "       [ 4576.,  9896.],\n",
       "       [ 4832.,  9896.],\n",
       "       [ 3040., 10152.],\n",
       "       [ 3296., 10152.],\n",
       "       [ 3552., 10152.],\n",
       "       [ 3808., 10152.],\n",
       "       [ 4064., 10152.],\n",
       "       [ 4320., 10152.],\n",
       "       [ 4576., 10152.],\n",
       "       [ 3296., 10408.],\n",
       "       [ 3552., 10408.],\n",
       "       [ 3808., 10408.],\n",
       "       [ 4064., 10408.],\n",
       "       [ 4320., 10408.],\n",
       "       [ 4576., 10408.],\n",
       "       [ 3296., 10664.],\n",
       "       [ 3552., 10664.],\n",
       "       [ 3808., 10664.],\n",
       "       [ 4064., 10664.],\n",
       "       [ 4320., 10664.],\n",
       "       [ 4576., 10664.],\n",
       "       [ 4832., 10664.],\n",
       "       [ 3296., 10920.],\n",
       "       [ 3552., 10920.],\n",
       "       [ 3808., 10920.],\n",
       "       [ 4064., 10920.],\n",
       "       [ 4320., 10920.],\n",
       "       [ 4576., 10920.],\n",
       "       [ 3552., 11176.],\n",
       "       [ 3808., 11176.],\n",
       "       [ 4064., 11176.],\n",
       "       [ 4320., 11176.],\n",
       "       [ 4576., 11176.],\n",
       "       [ 4832., 11176.],\n",
       "       [ 3552., 11432.],\n",
       "       [ 3808., 11432.],\n",
       "       [ 4064., 11432.],\n",
       "       [ 4320., 11432.],\n",
       "       [ 4576., 11432.],\n",
       "       [ 4832., 11432.],\n",
       "       [ 3808., 11688.],\n",
       "       [ 4064., 11688.],\n",
       "       [ 4320., 11688.],\n",
       "       [ 4576., 11688.],\n",
       "       [ 4832., 11688.],\n",
       "       [ 3808., 11944.],\n",
       "       [ 4064., 11944.],\n",
       "       [ 4320., 11944.],\n",
       "       [ 4576., 11944.],\n",
       "       [ 4832., 11944.],\n",
       "       [ 3808., 12200.],\n",
       "       [ 4064., 12200.],\n",
       "       [ 4320., 12200.],\n",
       "       [ 4576., 12200.],\n",
       "       [ 4832., 12200.],\n",
       "       [ 5088., 12200.],\n",
       "       [ 3808., 12456.],\n",
       "       [ 4064., 12456.],\n",
       "       [ 4320., 12456.],\n",
       "       [ 4576., 12456.],\n",
       "       [ 4832., 12456.],\n",
       "       [ 5088., 12456.],\n",
       "       [ 3808., 12712.],\n",
       "       [ 4064., 12712.],\n",
       "       [ 4320., 12712.],\n",
       "       [ 4576., 12712.],\n",
       "       [ 4832., 12712.],\n",
       "       [ 5088., 12712.],\n",
       "       [ 3808., 12968.],\n",
       "       [ 4064., 12968.],\n",
       "       [ 4320., 12968.],\n",
       "       [ 4576., 12968.],\n",
       "       [ 4832., 12968.],\n",
       "       [ 5088., 12968.],\n",
       "       [ 4064., 13224.],\n",
       "       [ 4320., 13224.],\n",
       "       [ 4576., 13224.],\n",
       "       [ 4832., 13224.],\n",
       "       [ 5088., 13224.],\n",
       "       [ 5344., 13224.],\n",
       "       [ 4064., 13480.],\n",
       "       [ 4320., 13480.],\n",
       "       [ 4576., 13480.],\n",
       "       [ 4832., 13480.],\n",
       "       [ 5088., 13480.],\n",
       "       [ 5344., 13480.],\n",
       "       [ 5600., 13480.],\n",
       "       [ 5856., 13480.],\n",
       "       [ 4320., 13736.],\n",
       "       [ 4576., 13736.],\n",
       "       [ 4832., 13736.],\n",
       "       [ 5088., 13736.],\n",
       "       [ 5344., 13736.],\n",
       "       [ 5600., 13736.],\n",
       "       [ 5856., 13736.],\n",
       "       [ 6112., 13736.],\n",
       "       [ 4832., 13992.],\n",
       "       [ 5088., 13992.],\n",
       "       [ 5344., 13992.],\n",
       "       [ 5600., 13992.],\n",
       "       [ 5856., 13992.],\n",
       "       [ 4832., 14248.],\n",
       "       [ 5088., 14248.],\n",
       "       [ 5344., 14248.],\n",
       "       [ 5600., 14248.],\n",
       "       [ 5856., 14248.],\n",
       "       [ 6112., 14248.],\n",
       "       [ 4832., 14504.],\n",
       "       [ 5088., 14504.],\n",
       "       [ 5344., 14504.],\n",
       "       [ 5600., 14504.],\n",
       "       [ 5856., 14504.],\n",
       "       [ 6112., 14504.],\n",
       "       [ 5088., 14760.],\n",
       "       [ 5344., 14760.],\n",
       "       [ 5600., 14760.],\n",
       "       [ 5856., 14760.],\n",
       "       [ 6112., 14760.],\n",
       "       [ 5088., 15016.],\n",
       "       [ 5344., 15016.],\n",
       "       [ 5600., 15016.],\n",
       "       [ 5856., 15016.],\n",
       "       [ 6112., 15016.],\n",
       "       [ 5088., 15272.],\n",
       "       [ 5344., 15272.],\n",
       "       [ 5600., 15272.],\n",
       "       [ 5856., 15272.],\n",
       "       [ 6112., 15272.],\n",
       "       [ 5088., 15528.],\n",
       "       [ 5344., 15528.],\n",
       "       [ 5600., 15528.],\n",
       "       [ 5856., 15528.],\n",
       "       [ 6112., 15528.],\n",
       "       [ 6368., 15528.],\n",
       "       [ 5088., 15784.],\n",
       "       [ 5344., 15784.],\n",
       "       [ 5600., 15784.],\n",
       "       [ 5856., 15784.],\n",
       "       [ 6112., 15784.],\n",
       "       [ 6368., 15784.],\n",
       "       [ 6624., 15784.],\n",
       "       [ 5344., 16040.],\n",
       "       [ 5600., 16040.],\n",
       "       [ 5856., 16040.],\n",
       "       [ 6112., 16040.],\n",
       "       [ 6368., 16040.],\n",
       "       [ 6624., 16040.],\n",
       "       [ 5344., 16296.],\n",
       "       [ 5600., 16296.],\n",
       "       [ 5856., 16296.],\n",
       "       [ 6112., 16296.],\n",
       "       [ 6368., 16296.],\n",
       "       [ 6624., 16296.],\n",
       "       [ 6880., 16296.],\n",
       "       [ 5600., 16552.],\n",
       "       [ 5856., 16552.],\n",
       "       [ 6112., 16552.],\n",
       "       [ 6368., 16552.],\n",
       "       [ 6624., 16552.],\n",
       "       [ 6880., 16552.],\n",
       "       [ 5856., 16808.],\n",
       "       [ 6112., 16808.],\n",
       "       [ 6368., 16808.],\n",
       "       [ 6624., 16808.],\n",
       "       [ 6880., 16808.],\n",
       "       [ 5856., 17064.],\n",
       "       [ 6112., 17064.],\n",
       "       [ 6368., 17064.],\n",
       "       [ 6624., 17064.],\n",
       "       [ 6880., 17064.],\n",
       "       [ 7136., 17064.],\n",
       "       [ 6112., 17320.],\n",
       "       [ 6368., 17320.],\n",
       "       [ 6624., 17320.],\n",
       "       [ 6880., 17320.],\n",
       "       [ 7136., 17320.],\n",
       "       [ 7392., 17320.],\n",
       "       [ 6112., 17576.],\n",
       "       [ 6368., 17576.],\n",
       "       [ 6624., 17576.],\n",
       "       [ 6880., 17576.],\n",
       "       [ 7136., 17576.],\n",
       "       [ 7392., 17576.],\n",
       "       [ 6368., 17832.],\n",
       "       [ 6624., 17832.],\n",
       "       [ 6880., 17832.],\n",
       "       [ 7136., 17832.],\n",
       "       [ 7392., 17832.],\n",
       "       [ 6368., 18088.],\n",
       "       [ 6624., 18088.],\n",
       "       [ 6880., 18088.],\n",
       "       [ 7136., 18088.],\n",
       "       [ 7392., 18088.],\n",
       "       [ 6624., 18344.],\n",
       "       [ 6880., 18344.],\n",
       "       [ 7136., 18344.],\n",
       "       [ 7392., 18344.],\n",
       "       [ 7648., 18344.],\n",
       "       [ 6624., 18600.],\n",
       "       [ 6880., 18600.],\n",
       "       [ 7136., 18600.],\n",
       "       [ 7392., 18600.],\n",
       "       [ 7648., 18600.],\n",
       "       [ 7904., 18600.],\n",
       "       [ 8160., 18600.],\n",
       "       [ 6880., 18856.],\n",
       "       [ 7136., 18856.],\n",
       "       [ 7392., 18856.],\n",
       "       [ 7648., 18856.],\n",
       "       [ 7904., 18856.],\n",
       "       [ 8160., 18856.],\n",
       "       [ 6880., 19112.],\n",
       "       [ 7136., 19112.],\n",
       "       [ 7392., 19112.],\n",
       "       [ 7648., 19112.],\n",
       "       [ 7904., 19112.],\n",
       "       [ 8160., 19112.],\n",
       "       [ 8416., 19112.],\n",
       "       [ 8672., 19112.],\n",
       "       [ 6880., 19368.],\n",
       "       [ 7136., 19368.],\n",
       "       [ 7392., 19368.],\n",
       "       [ 7648., 19368.],\n",
       "       [ 7904., 19368.],\n",
       "       [ 8160., 19368.],\n",
       "       [ 8416., 19368.],\n",
       "       [ 8672., 19368.],\n",
       "       [ 6880., 19624.],\n",
       "       [ 7136., 19624.],\n",
       "       [ 7392., 19624.],\n",
       "       [ 7648., 19624.],\n",
       "       [ 7904., 19624.],\n",
       "       [ 8160., 19624.],\n",
       "       [ 8416., 19624.],\n",
       "       [ 8672., 19624.],\n",
       "       [ 8928., 19624.],\n",
       "       [ 7136., 19880.],\n",
       "       [ 7904., 19880.],\n",
       "       [ 8160., 19880.],\n",
       "       [ 8416., 19880.],\n",
       "       [ 8672., 19880.],\n",
       "       [ 8928., 19880.],\n",
       "       [ 9184., 19880.],\n",
       "       [ 8160., 20136.],\n",
       "       [ 8416., 20136.],\n",
       "       [ 8672., 20136.],\n",
       "       [ 8928., 20136.],\n",
       "       [ 9184., 20136.],\n",
       "       [ 8160., 20392.],\n",
       "       [ 8416., 20392.],\n",
       "       [ 8672., 20392.],\n",
       "       [ 8928., 20392.],\n",
       "       [ 9184., 20392.],\n",
       "       [ 8416., 20648.],\n",
       "       [ 8672., 20648.],\n",
       "       [ 8928., 20648.],\n",
       "       [ 9184., 20648.],\n",
       "       [ 9440., 20648.],\n",
       "       [ 8416., 20904.],\n",
       "       [ 8672., 20904.],\n",
       "       [ 8928., 20904.],\n",
       "       [ 9184., 20904.],\n",
       "       [ 9440., 20904.],\n",
       "       [ 8416., 21160.],\n",
       "       [ 8672., 21160.],\n",
       "       [ 8928., 21160.],\n",
       "       [ 9184., 21160.],\n",
       "       [ 8928., 21416.],\n",
       "       [ 9184., 21416.],\n",
       "       [ 9440., 21416.],\n",
       "       [ 9184., 21672.],\n",
       "       [ 9440., 21672.],\n",
       "       [ 8928., 21928.],\n",
       "       [ 9184., 21928.],\n",
       "       [ 9440., 21928.],\n",
       "       [ 9696., 21928.],\n",
       "       [ 8672., 22184.],\n",
       "       [ 8928., 22184.],\n",
       "       [ 9184., 22184.],\n",
       "       [ 9440., 22184.],\n",
       "       [ 9696., 22184.],\n",
       "       [ 8672., 22440.],\n",
       "       [ 8928., 22440.],\n",
       "       [ 9184., 22440.],\n",
       "       [ 9440., 22440.],\n",
       "       [ 9696., 22440.],\n",
       "       [ 8672., 22696.],\n",
       "       [ 8928., 22696.],\n",
       "       [ 9184., 22696.],\n",
       "       [ 9440., 22696.],\n",
       "       [ 9696., 22696.],\n",
       "       [ 9952., 22696.],\n",
       "       [ 8928., 22952.],\n",
       "       [ 9184., 22952.],\n",
       "       [ 9440., 22952.],\n",
       "       [ 9696., 22952.],\n",
       "       [ 9952., 22952.],\n",
       "       [ 8672., 23208.],\n",
       "       [ 8928., 23208.],\n",
       "       [ 9184., 23208.],\n",
       "       [ 9440., 23208.],\n",
       "       [ 9696., 23208.],\n",
       "       [ 8928., 23464.],\n",
       "       [ 9184., 23464.],\n",
       "       [ 9440., 23464.],\n",
       "       [ 9696., 23464.],\n",
       "       [ 9952., 23464.],\n",
       "       [ 8672., 23720.],\n",
       "       [ 8928., 23720.],\n",
       "       [ 9184., 23720.],\n",
       "       [ 9440., 23720.],\n",
       "       [ 9696., 23720.],\n",
       "       [ 8672., 23976.],\n",
       "       [ 8928., 23976.],\n",
       "       [ 9184., 23976.],\n",
       "       [ 9440., 23976.],\n",
       "       [ 9696., 23976.],\n",
       "       [ 8672., 24232.],\n",
       "       [ 8928., 24232.],\n",
       "       [ 9184., 24232.],\n",
       "       [ 9440., 24232.],\n",
       "       [ 8160., 24488.],\n",
       "       [ 8416., 24488.],\n",
       "       [ 8672., 24488.],\n",
       "       [ 8928., 24488.],\n",
       "       [ 9184., 24488.],\n",
       "       [ 9440., 24488.],\n",
       "       [ 8416., 24744.],\n",
       "       [ 8672., 24744.],\n",
       "       [ 8928., 24744.],\n",
       "       [ 9184., 24744.],\n",
       "       [ 9440., 24744.],\n",
       "       [ 9696., 24744.],\n",
       "       [ 7648., 25000.],\n",
       "       [ 8672., 25000.],\n",
       "       [ 8928., 25000.],\n",
       "       [ 9184., 25000.],\n",
       "       [ 9440., 25000.],\n",
       "       [ 9696., 25000.],\n",
       "       [ 7392., 25256.],\n",
       "       [ 7648., 25256.],\n",
       "       [ 7904., 25256.],\n",
       "       [ 7648., 25512.],\n",
       "       [ 7904., 25512.],\n",
       "       [ 8160., 25512.],\n",
       "       [ 8416., 25512.],\n",
       "       [ 8672., 25512.],\n",
       "       [ 8416., 25768.],\n",
       "       [ 8672., 25768.],\n",
       "       [ 8160., 26024.],\n",
       "       [ 8416., 26024.],\n",
       "       [ 8672., 26024.],\n",
       "       [ 8928., 26024.],\n",
       "       [ 9184., 26024.],\n",
       "       [ 9440., 26024.],\n",
       "       [ 8416., 26280.],\n",
       "       [ 8672., 26280.],\n",
       "       [ 8928., 26280.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assets, _ = read_assets_from_h5('/home/yuhaowang/data/embedding/TCGA-TOY')\n",
    "assets['coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "538"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "len(os.listdir('/home/yuhaowang/data/embedding/TCGA-LUAD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "len(os.listdir('/home/yuhaowang/data/processed_data/TCGA-LUAD/output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[b'07784', b'28596'],\n",
       "       [b'07784', b'29620'],\n",
       "       [b'07784', b'30644'],\n",
       "       ...,\n",
       "       [b'99945', b'70580'],\n",
       "       [b'99945', b'71604'],\n",
       "       [b'99945', b'72628']], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list\n",
    "assets, _ = read_assets_from_h5(os.path.join(test_path,file_list[0]))\n",
    "assets['coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'07784'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assets['coords'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=torch.rand(678,224)\n",
    "img_len=[678]\n",
    "pad_mask=[0]\n",
    "coords=torch.rand((678,2))\n",
    "data_dict={'features':img,'img_lens':img_len,'pad_mask':pad_mask,'coords':coords}\n",
    "#save h5 file\n",
    "with h5py.File('/home/yuhaowang/data/temp/GigaPath_PANDA_embeddings/h5_files/test.h5', 'w') as f:\n",
    "    for key,value in data_dict.items():\n",
    "        f.create_dataset(key, data=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,_=read_assets_from_h5('/home/yuhaowang/data/test/TCGA-LUAD-AllFeature/TCGA-75-5147-01Z-00-DX1.3156FA5C-9737-4EAA-989C-FD578FBF15A1.h5')\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict, deque\n",
    "def element_indices(lst):\n",
    "    index_dict = defaultdict(list)  \n",
    "    for i, value in enumerate(lst):\n",
    "        index_dict[value].append(i)  \n",
    "    return dict(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['features'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['features'][0].sum()\n",
    "for i in data['features']:\n",
    "    print(i.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir('/home/yuhaowang/data/test/TCGA-LUAD-AllFeature'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align the name of feature dir\n",
    "\n",
    "import  os\n",
    "data_dir='/home/yuhaowang/data/test/TCGA-LUAD-AllFeature'\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith('.h5'):\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        #rename the file\n",
    "        #TCGA-97-8552-01A-01-TS1.24ca90a7-5762-4ab5-a404-67dfcd416f79.h5 -> TCGA-97-8552-01A-01-TS1.h5\n",
    "        new_file_name = file.split('.')[0] + '.h5'\n",
    "        new_file_path = os.path.join(data_dir, new_file_name)\n",
    "        os.rename(file_path, new_file_path)\n",
    "        print(f'Renamed file: {file_path} to {new_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as pth_transforms\n",
    "from torchvision import models as torchvision_models \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "import collections\n",
    "from collections import Counter\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "sys.path.append('/home/yuhaowang/project/FMBC/dino_stage2')\n",
    "import vision_transformer as vits\n",
    "embed_dim=384\n",
    "checkpoint_path='/home/yuhaowang/project/FMBC/dino_stage2/output/checkpoint0180.pth'\n",
    "model = vits.__dict__['vit_small'](embed_dim=embed_dim)\n",
    "len(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmodel\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "len(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take key teacher in provided checkpoint dict\n",
      "Pretrained weights found at /home/yuhaowang/project/FMBC/dino_stage2/output/checkpoint0180.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_pretrained_weights(model, pretrained_weights, checkpoint_key):\n",
    "    if os.path.isfile(pretrained_weights):\n",
    "        state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "        if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "            print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n",
    "            state_dict = state_dict[checkpoint_key]\n",
    "        # remove `module.` prefix\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        # remove `backbone.` prefix induced by multicrop wrapper\n",
    "        state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "        msg = model.load_state_dict(state_dict, strict=False)\n",
    "        print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))\n",
    "        model.eval()\n",
    "load_pretrained_weights(model, checkpoint_path, 'teacher')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmodel\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "len(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuhaowang/anaconda3/envs/gigapath/lib/python3.9/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1678402412426/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Mask shape should match input. mask: [1, 17770] input: [1, 6, 17771, 17771]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m17770\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     output\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/gigapath/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project/FMBC/dino_stage2/vision_transformer.py:152\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    150\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_tokens(x)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 152\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x[:,\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/gigapath/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project/FMBC/dino_stage2/vision_transformer.py:95\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, mask, return_attention)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x,mask, return_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 95\u001b[0m     y, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_attention:\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
      "File \u001b[0;32m~/anaconda3/envs/gigapath/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/project/FMBC/dino_stage2/vision_transformer.py:45\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x,mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     B, N, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 45\u001b[0m     value, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value, attn\n",
      "File \u001b[0;32m~/anaconda3/envs/gigapath/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/gigapath/lib/python3.9/site-packages/torch/nn/modules/activation.py:1144\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m why_not_fast_path:\n\u001b[1;32m   1142\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_masks(attn_mask, key_padding_mask, query)\n\u001b[0;32m-> 1144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_native_multi_head_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmerged_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m            \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m            \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1159\u001b[0m any_nested \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mor\u001b[39;00m key\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mor\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_nested\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_nested, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiheadAttention does not support NestedTensor outside of its fast path. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   1161\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe fast path was not hit because \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhy_not_fast_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mask shape should match input. mask: [1, 17770] input: [1, 6, 17771, 17771]"
     ]
    }
   ],
   "source": [
    "test_tensor=torch.rand((1,17770,384))\n",
    "mask=torch.ones((1,17770))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output=model((test_tensor.cuda(),mask.cuda()))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "a=5\n",
    "def change(a):\n",
    "    a=6\n",
    "change(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1007196212.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    str(Path(__file__).resolve().parents[1]\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "str(Path(__file__).resolve().parents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " = os.path.join(args.split_dir,args.task_code) if not args.pre_split_dir else args.pre_split_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp =3 if not 1<4 else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('/home/yuhaowang/project/FMBC/finetune/dataset_csv/mutation/BRCA-6-gene_TCGA.csv')\n",
    "#delete rows with nan\n",
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  EGFR: 0\n",
    "#   FAT1: 1\n",
    "#   KRAS: 2\n",
    "#   LRP1B: 3\n",
    "#   TP53: 4\n",
    "#   HRD: 5\n",
    "df=data[['EGFR','FAT1','KRAS','LRP1B','TP53','HRD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EGFR</th>\n",
       "      <th>FAT1</th>\n",
       "      <th>KRAS</th>\n",
       "      <th>LRP1B</th>\n",
       "      <th>TP53</th>\n",
       "      <th>HRD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "      <td>932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>923</td>\n",
       "      <td>904</td>\n",
       "      <td>926</td>\n",
       "      <td>906</td>\n",
       "      <td>602</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         EGFR   FAT1   KRAS  LRP1B   TP53    HRD\n",
       "count     932    932    932    932    932    932\n",
       "unique      2      2      2      2      2      2\n",
       "top     False  False  False  False  False  False\n",
       "freq      923    904    926    906    602    481"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['HRP'],inplace=True)\n",
    "data.to_csv('/home/yuhaowang/project/FMBC/finetune/dataset_csv/mutation/BRCA-6-gene_TCGA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TCGA-AN-A0FL-01Z-00-DX1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['file_name'][0].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['slide_id']=data['file_name'].apply(lambda x:x.split('.')[0])\n",
    "data.to_csv('/home/yuhaowang/project/FMBC/finetune/dataset_csv/mutation/BRCA-6-gene_TCGA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conver HDR_label column to new columns with true and false\n",
    "df=pd.get_dummies(data['HDR_label'])\n",
    "data=pd.concat([data,df],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['HDR_label'],inplace=True)\n",
    "data.to_csv('/home/yuhaowang/project/FMBC/finetune/dataset_csv/mutation/BRCA-5-gene_TCGA.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/home/yuhaowang/data/embedding/TCGA-BRCA'\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith('.svs.h5'):\n",
    "        new_file_name=file.split('.')[0]+'.h5'\n",
    "        shutil.move(os.path.join(data_dir,file),os.path.join(data_dir,new_file_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = np.array([0, 0, 0, 0])\n",
    "y_scores = np.array([1, 0, 0, 0])\n",
    "try:\n",
    "    roc_auc_score(y_true, y_scores) ## y_true=ground_truth\n",
    "except ValueError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_scores\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gigapath/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/gigapath/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:640\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    638\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[1;32m    639\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    649\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    650\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    654\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/gigapath/lib/python3.9/site-packages/sklearn/metrics/_base.py:76\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m     79\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[0;32m~/anaconda3/envs/gigapath/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:382\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    387\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "538"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "len(os.listdir('/home/yuhaowang/data/embedding/TCGA-LUAD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=torch.ones((1,0))\n",
    "mask = torch.cat([torch.zeros_like(mask[:, :1]), mask], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gigapath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
